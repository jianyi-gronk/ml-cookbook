## 1. 题目介绍

- [题目](https://www.kaggle.com/competitions/playground-series-s5e3)：测每日降雨的概率
- 数据集：由 75w 条数据，10 个特征组成，包含
  - Podcast_Name：播客名称
  - Episode_Title：播客剧集的标题
  - Episode_Length_minutes：剧集的长度（ 分钟 ）
  - Genre：播客剧集的类型
  - Host_Popularity：表示主持人受欢迎程度的分数
  - Publication_Day：剧集发布的星期几
  - Publication_Time：剧集发布的时间
  - Guest_Popularity：表示嘉宾受欢迎程度的分数
  - Number_of_Ads：剧集中的广告数量
  - Episode_Sentiment：剧集内容的情绪
- 目标变量：Listening_Time_minutes 预测收听持续时间（ 分钟 ）
- 评估：通过 RMSE 评估
- 参考 [思路](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/575840)

## 2 数据预处理

- 参考 [notebook](https://www.kaggle.com/code/greysky/podcast-dataset-generator)

#### 2.1 分析数据

- 首先观察原始数据，是由 5.25 w 条数据数据增强得到的 75 w 条数据，所以它本质仍然只有 5.25 w 条数据的信息，但是相对于 11 条特征，数据量还是非常大的，需要构建更多的特征，那么特征工程就是非常重要的一个环节
- 比如 [notebook](https://www.kaggle.com/code/greysky/podcast-model-training/notebook) ，通过 10 个特征，生成了 1552 个特征，最终只通过一个 lightgbm 就拿到了第二名的成绩

#### 2.2 合并原始数据

- 由于数据是由 5.25 w 条数据通过数据增强得到，可以找到原始数据集，将它合并到当前数据集中，用来丰富数据集

#### 2.3 特征工程

- 将文本型分类特征转换为数值型

  ```python
  podc_dict = {...}  # 播客名称
  genr_dict = {...}  # 播客剧集的类型
  # ... 其他特征类似处理

  df['Podcast_Name'] = df['Podcast_Name'].replace(podc_dict)
  df['Genre'] = df['Genre'].replace(genr_dict)
  # ... 其他特征类似处理
  ```

- 转换为 category 类型节省内存

  ```python
  df['Genre'] = df['Genre'].astype('category')
  df['Podcast_Name'] = df['Podcast_Name'].astype('category')
  # ... 其他特征类似处理
  ```

- 从标题中提取集数

  ```python
  df['Episode_Title'] = df['Episode_Title'].str[8:].astype('category')
  ```

- 创建交互特征

  ```python
  # 会将剧集长度给四舍五入取整，理解是为了减少内存消耗
  df['Mul_Hpp_Elm'] = df['Host_Popularity_percentage'] * df['Episode_Length_minutes'].round()
  df['Mul_Gpp_Elm'] = df['Guest_Popularity_percentage'] * df['Episode_Length_minutes'].round()
  ```

- 将数值特征进行步长为 2 的分箱

  ```python
  # 增强泛化，将相似值分组（ 比如 44 和 45 分钟视为同一类 ），防止模型过拟合细微差异
  df['Round_1'] = df['Episode_Length_minutes'].round() // 2
  df['Round_2'] = df['Host_Popularity_percentage'].round() // 2
  df['Round_3'] = df['Guest_Popularity_percentage'].round() // 2
  ```

#### 2.4 目标编码

- 具体步骤
  1. 生成所有可能的 1-6 维特征组合，但是避免 'Round_X' 和原始特征出现在同一组合中
  2. 创建新的特征，将需要拼接的特征组合的特征值进行拼接 X_Y_Z...
  3. 在训练集中，对新特征进行目标编码，使用的是 5 折交叉验证，即第 i 折的目标编码结果基于其他 4 折的数据，防止目标信息泄露
  4. 由于测试集中没有目标变量，无法进行目标编码，因此需要使用训练集的目标编码结果来编码测试集
- 源码

  ```python
  def target_encode(train, test, columns, pair_size, stat='mean', split_method='random', suffix='', n_cv=5, smooth=0.0, random_state=42):
    # 初始化目标编码器
    encoder = TargetEncoder(n_folds=n_cv, smooth=smooth, seed=random_state, stat=stat, split_method=split_method)

    # 遍历所有可能的特征组合（ pair_size 指定组合大小 ）
    for cols in tqdm(list(combinations(columns, pair_size))):
      # 过滤条件：避免原始特征与其离散化版本同时出现在组合中
      if (('Round_1' not in cols) | ('Episode_Length_minutes' not in cols)) & \
        (('Round_2' not in cols) | ('Host_Popularity_percentage' not in cols)) & \
        (('Round_3' not in cols) | ('Guest_Popularity_percentage' not in cols)):

        # 创建新特征名称：特征组合 + 后缀
        new_col_name = '_'.join(cols) + f'_{suffix}'

        # 训练集：拼接特征值
        train[new_col_name] = train[cols[0]].astype(str)
        for col in cols[1:]:
          train[new_col_name] = train[new_col_name] + '_' + train[col].astype(str)
        # ... 测试集同样拼接特征值

        # 训练集：应用目标编码（ 使用 5 折交叉验证 ）
        train[new_col_name] = encoder.fit_transform(train[new_col_name], train['Listening_Time_minutes'])
        train[new_col_name] = train[new_col_name].astype('float32')  # 转换为32位浮点数节省内存

        # 测试集：应用目标编码（ 使用训练集学到的映射 ）
        test[new_col_name] = encoder.transform(test[new_col_name])
        test[new_col_name] = test[new_col_name].astype('float32')

    return train, test

  encode_columns = [
    'Podcast_Name', 'Episode_Length_minutes', 'Episode_Num', 'Episode_Sentiment',
    'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads',
    'Publication_Day', 'Publication_Time', 'Round_1', 'Round_2', 'Round_3',
  ]

  for r in [1, 2, 3, 4, 5, 6]:
    df_train, df_test = target_encode(
      train=df_train, test=df_test,
      columns=encode_columns,
      pair_size=r,
      stat='mean',
      split_method='random',
      suffix=f'mean_{r}',
      smooth=0.0,
      random_state=seed,
    )
  ```

#### 2.5 特征聚合

- 拿到所有的编码特征列 encoded_columns，然后对每一行中的各个特征进行聚合，聚合包括 平均值，标准差，最小值，最大值（ 比如所有特征的平均值较高，说明该剧集在各种组合中都表现良好，提供全局表现指标 ）
- 对所有编码特征进行聚合
  ```python
  agg_list = ['mean', 'std', 'min', 'max']
  for agg in agg_list:
    df_train[f'mean_{agg}'] = df_train[encoded_columns].agg(agg, axis=1)
    df_test[f'mean_{agg}'] = df_test[encoded_columns].agg(agg, axis=1)
  ```
- 按组合维度聚合，对之前 1-6 维组合的特征集，进行分别聚合
  ```python
  agg_list = ['mean', 'std', 'min', 'max']
  r_list = [1, 2, 3, 4, 5, 6]
  for r in tqdm(r_list):
    select_columns = encoded_columns[encoded_columns.str.contains(f'_{r}')]
    for agg in agg_list:
      df_train[f'mean_{agg}_{r}'] = df_train[select_columns].agg(agg, axis=1)
      df_test[f'mean_{agg}_{r}'] = df_test[select_columns].agg(agg, axis=1)
  ```
- 按原始特征聚合，按照 1-6 维组合的特征集中，是否包含该原始特征，进行分别聚合
  ```python
  agg_list = ['mean', 'std', 'min', 'max']
  column_names = ['Podcast_Name', 'Episode_Length_minutes', ...] # 12个原始特征
  for column_name in tqdm(column_names):
    select_columns = encoded_columns[encoded_columns.str.contains(column_name)]
    for agg in agg_list:
      df_train[f'mean_{agg}_{column_name}'] = df_train[select_columns].agg(agg, axis=1)
  ```

## 3. 模型训练

- 参考 [思路](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/575784)
- 认为由于数据具有复杂的交互和深层模式，传统的 单一模型 无法获得第一名，因此提出了一个三层的 RAPIDS cuML stack（ 三层堆叠 ）方案
  ![image](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/refs/heads/main/Apr-2025/stack.png)
