## 1. 题目介绍

- 题目：[预测最佳肥料](https://www.kaggle.com/competitions/playground-series-s5e6)
- 数据集：由 75w 条数据，8 个特征组成，包含
  - Temperature：温度
  - Humidity：湿度
  - Moisture：水分
  - Soil Type：土壤类型
  - Crop Type：作物类型
  - Nitrogen：氮
  - Potassium：钾
  - Phosphorous：磷
- 目标变量
  - Fertilizer Name 针对不同条件选取最佳的肥料（ 名字 ）
- 评估：通过 MAP@3 评估
- 参考 [思路](https://www.kaggle.com/competitions/playground-series-s5e6/discussion/587393)

## 2. cuDF 特征工程

#### 2.1 核心步骤

- 高维组合特征
  - 对 8 个原始分类特征，生成所有可能的二元组合（ 28 列 ）、三元组合（ 56 列 ）、四元组合（ 70 列 ），共 162 个新特征列
- 大规模目标编码
  - 使用 cuML Target Encoder 对上述 162 个组合特征，分别针对 7 个二元目标（ y == 0，y == 1，...，y == 6 ）进行编码，最终生成 162 \* 7 = 1134 个新列
  - 再次使用 原始数据集 对同样的 162 个组合特征进行目标编码（ 基于原始数据的统计 ），生成另外 1134 个新列
  - 最终得到 2268 个目标编码特征列，用这些列训练 XGBoost

#### 2.2 原始数据操作细节

- 方法一（ 主流 ）：将原始数据作为新行添加到训练数据中
- 方法二（ 创新点 ）：将原始数据用于生成新特征（ 列 ），具体做法
  - 选择一个分类列（ 或组合列 ）CAT_COL
  - 在原始数据上计算 CAT_COL 的每个类别对应的目标变量 TARGET 的均值 TE
  - 将这个计算好的 TE\_{CAT_COL}\_orig 通过 left join 合并到训练集和测试集上

## 3. 堆叠

- 目的：校准概率以提升 MAP@3
- 阶段 1
  - 利用 RAPIDS cuML 快速训练多样化的模型集合（ XGBoost，CatBoost，NN，线性回归 ），预测每个二元目标（ y==k ）的袋外（ OOF ）概率
- 阶段 2
  - NN：使用阶段 1 模型的所有 OOF 预测作为输入特征，训练一个神经网络（ NN ）作为元模型，使用分类交叉熵损失预测多类概率，NN 擅长校准概率
  - GBDT：同样使用阶段 1 的输出，训练一个 XGBoost 模型（ objective='multi:softprob' ）作为元模型
- 集成
  - 将 NN 阶段 2 模型和 GBDT 阶段 2 模型的预测概率进行平均，效果优于单个阶段 2 模型
  - 使用爬山法（ 或其他优化方法 ）为集合中的不同模型找到最优权重进行加权平均

## 4. 重复 KFold

- 原因：MAP@3 对模型预测概率的随机性非常敏感
- 方法
  - 对同一个模型架构，使用不同的随机种子进行多次训练
  - 例如：NN 训练 100 次，XGBoost 训练几十次
- 集成
  - 对多次训练产生的预测概率进行平均，然后再选取 Top 3，这显著提升了 MAP@3
  - 例如：单个 5 折 XGB 平均 0.376，100 次平均后达 0.380

## 5. GBoost 技巧（ 在残差上提升 ）

- 先训练一个 cuML 线性回归 模型
- 将线性回归模型的预测值（ logits ）设置为 XGBoost 训练数据（ dtrain ）的 base_margin
- XGBoost 会在这个线性模型预测的基础上进行梯度提升，相当于从一个更好的起点开始学习

## 6. 伪标签

- 伪标签：在比赛后期，使用当前最佳集成模型预测测试集标签（ 软概率 ），将这些预测作为伪标签加入到训练数据中
- 方式一（ 行 ）
  - 将带伪标签的测试数据作为新行加入训练集
- 方式二（ 列 ）
  - 将伪标签预测值作为新特征列加入训练集和测试集
- 模型适配
  - NN 天然支持软标签训练
  - XGBoost 需要自定义目标函数处理软标签

## 7. 100% 数据重训练

- 100% 数据重训练：在比赛后期，使用全部训练数据（ 包括可能的伪标签数据 ）重新训练集成中最重要的模型
- 调整模型参数
  - XGBoost：迭代次数设为交叉验证平均最优次数的 125%
  - NN：根据交叉验证的平均最优学习率调整计划，设定固定的学习率衰减步骤

## 8. 最终提交方案

- 最终提交应选择在交叉验证上表现最好且与排行榜分数一致的模型/集成
- 最终提交一个由 9 大类模型组成的强大集成，每类模型都用不同种子训练多次（ 总计约 300 次训练 ），并通过爬山法分配权重：
  - cuDF 特征工程（ 原始数据为列 ） - XGBoost（ 深度 4 ） - 10 个模型
  - cuDF 特征工程（ 原始数据为列 ） - XGBoost（ 深度 10 ） - 10 个模型
  - 堆叠 NN（ 多个阶段 1 模型 ） - 25 个模型
  - 重复分层 K 折 XGBoost（ 原始数据为行 ）- 参考 @bizen250 公开笔记本 - 50 个模型
  - XGBoost 堆叠在 LGBM 上 - 参考 @ayushchandramaurya 公开笔记本 - 多版本
  - XGBoost（ 原始数据为行, 深度 18 ） - 参考 @elainedazzio 公开笔记本 - 多版本
  - NN - 参考 @ricopue 公开笔记本 - 100 个模型
  - 带伪标签特征列的堆叠 XGBoost（ 阶段 1 包含 KNN 特征 ）
  - 带伪标签行的 NN - 25 个模型
