## 1. 分词（ Tokenization ）

- 分词属于预处理流程，将原始输入的连续文本数据，分割成一系列离散的 token（ 词、子词、字符 等 ），并将每个 token 映射为词汇表（ Vocabulary ）中的索引 ID
- 核心步骤
  - 预处理
    - 去除噪声（ 如 标点、特殊字符 ），转换为小写（ 可选 ），或规范化文本
  - 分割
    - 根据规则或算法将输入分割成 token
  - 映射
    - 每个 token 查找词汇表中的索引 ID
    - 如果 token 不在词汇表中，使用特殊处理，如 子词拆分 或 [UNK] 标记
  - 添加特殊 token
    - [CLS]
      - 分类标记，通常放在输入序列的开头，用于表示整个序列的语义表示（ 常用于分类任务 ）
    - [SEP]
      - 分隔标记，用于分隔两个句子（如在下一句预测任务中）或标记序列的结束
    - [PAD]
      - 填充标记，用于将输入序列填充到固定长度，以支持批处理
    - [UNK]
      - 未知词标记，当输入的 token 不在词汇表中时，模型会将其替换为 [UNK]
    - [MASK]
      - 用于掩码语言模型（ MLM ）预训练任务，BERT 会随机掩盖 15% 的 token，用 [MASK] 替换，模型需预测原始 token
      - 例如，输入 “I love learning” 可能变为 “I love [MASK]”，模型预测 [MASK] 为 “learning”

## 2. 常见分词方法

- 主要分为 词级别，字符级别，子词级别

#### 2.1 词级别分词

- 原理
  - 按空格或标点直接分割成完整单词
  - 如 "I love learning" → ["I", "love", "learning"]
- 优点
  - 简单，直观
- 缺点
  - 词汇表需覆盖所有可能单词，易达数百万
  - OOV（ 未知词，Out-of-Vocabulary ）问题严重，未知词一般用 [UNK] 表示
- 适用
  - 早期基于规则的 NLP 系统，例如 词性标注、简单分类
  - 词汇量有限的领域，如医疗术语处理

#### 2.2 字符级别分词

- 原理
  - 将文本拆成单个字符
  - 如 "learning" → ["l", "e", "a", "r", "n", "i", "n", "g"]
- 优点
  - 词汇表极小，通常只有 ASCII 中的 256，无 OOV 情况
- 缺点
  - 序列长度很长，计算开销大，例如句子 100 词时，可能会生成 500+ token
- 适用
  - 低资源语言处理（ 词汇表小，适应性强 ）
  - 语音识别（ 字符序列更灵活 ）
  - 拼写纠错任务

#### 2.3 子词级别分词

- 原理
  - 将单词拆分成更小的子单元（ 如常见词根或字节对 ），词汇表大小控制在 3-5 万
- 优点
  - 词汇表大小适中（ 通常 30k-50k ），能避免大部分 OOV 情况（ 拆分成子词 ）
  - 分词后的序列长度适中
- 缺点
  - 需要学习拆分模式
- 适用
  - 机器翻译，处理多语言词汇
  - 问答系统，处理长序列
  - 生成任务，平衡序列长度与词汇覆盖

## 3. 子词级别分词算法

#### 3.1 BPE（ Byte Pair Encoding ）

- 步骤
  - 初始化：将语料中所有单词拆成字符序列，添加词尾标记 \</w>（表示单词边界），如 "learning" → "l e a r n i n g \</w>"
  - 统计：计算相邻符号对的频率，选择最频繁的 pair（ 如 "l e" → "le" ），合并并替换语料中所有匹配
  - 迭代：重复合并过程，直到词汇表达到预设大小（ 比如 50k ）
  - 分词：对新文本，贪婪匹配最长子词；无法匹配的部分拆成字符
- 示例
  - 句子 "unhappiness"，BPE 可能学到 ["un", "happi", "ness"]

#### 3.2 WordPiece

- 原理
  - 类似于 BPE，通过最大化语料的似然概率选择合并对，计算公式为 $P(w) = ∏P(t_i | t_{i-1})$，其中 $t_i$ 为子词单元，选择合并能最大化语料概率的 pair，支持 Unicode
- 示例
  - "unhappiness" → ["un", "##happi", "ness"]（ "##" 表示子词非词首 ）

#### 3.3 SentencePiece

- 原理
  - 无监督子词模型，直接在原始文本上训练（ 不依赖空格 ），支持多语言，适合处理无空格语言（ 如中文、日文 ）
  - 通常结合 BPE 或 unigram 语言模型
- 示例
  - “我爱学习” → ["我", "爱", "学习"]
  - “日本語を勉強する” → ["日本", "語", "を", "勉強", "する"]

## 4. 分词步骤示例（ BPE ）

- 输入句子："I love learning unhappiness"
- 分词后
  - "I" → ["I"]
  - "love" → ["love"]（ 完整词 ）
  - "learning" → ["learn", "ing"]（ 假设学到拆分 ）
  - "unhappiness" → ["un", "hap", "pi", "ness"]（ OOV 拆分 ）
  - 最终得到 ["I", "love", "learn", "ing", "un", "hap", "pi", "ness"]
- 添加特殊 token
  - ["[CLS]", "I", "love", ..., "[SEP]", "[PAD]"]
- 映射到索引
  - 假设词汇表 ID，例 [101, 102, 200, ..., 103, 0]
