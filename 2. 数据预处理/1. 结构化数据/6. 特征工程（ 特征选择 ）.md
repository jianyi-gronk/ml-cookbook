## 1. 冗余特征

- 现实应用中属性维数经常成千上万，还有的一个特征分成了几十维度，看似特征多，但真正有用的信息少
- 过多维数会带来以下问题
  - 在高维情形下容易出现数据 **样本稀疏**，降维后在这个子空间中样本密度大幅提高
  - 同时 **会让计算更加复杂**，比如许多学习方法都涉及距离计算，但当维数很高时甚至连计算内积都不再容易
  - 并且降维能在一定程度上达到 **降噪** 的效果（ 每个特征对应的数据更多了，噪声数据的比例就下降了 ）
- 减少冗余特征 的主要方法是 **特征选择** 和 **数据降维**
  - 特征选择：核心是 **从原始特征中选择出最具代表性和区分性的特征子集**，以提高模型的性能、减少计算成本和增强模型的可解释性
  - 数据降维：核心是 **将该维度映射到其他维度**，从而减少维度数量，且在过程中 **尽可能保持数据的特性**

## 2. 特征选择

#### 2.1 Filter（ 过滤法 ）

- 原理：
  - **根据特征自身的统计特性对特征进行评分和排序**，然后选择得分较高的特征子集
  - 它主要基于 **特征与目标变量之间的相关性、特征的方差等统计指标** 来评估特征的重要性
- 常用方法
  - 方差选择法
    - 计算每个特征的方差，**方差反映了特征的离散程度，方差较小的特征可能包含的信息较少**，因此可以选择方差大于某个阈值的特征
    - 例如，在图像特征中，如果某个像素位置的灰度值几乎不变，其方差接近 0，这样的特征对于分类任务可能没有太大帮助，可以将其剔除
  - 特征相关性法
    - 可以删除与其他特征高度相关的特征，减少特征的冗余，通常使用 皮尔逊相关系数 来计算两个特征之间的线性相关程度
    - 例如，假设现在有 A，B，C，D 四个特征，阈值设为 0.95
      - 计算绝对相关系数矩阵
        ```
            A      B      C      D
        A  1.00   0.97   0.20   0.15
        B  0.97   1.00   0.25   0.18
        C  0.20   0.25   1.00   0.72
        D  0.15   0.18   0.72   1.00
        ```
      - 提取上三角矩阵（ 不包含对角线 ）
        ```
            A      B      C      D
        A   NaN   0.97   0.20   0.15
        B   NaN    NaN   0.25   0.18
        C   NaN    NaN    NaN   0.72
        D   NaN    NaN    NaN    NaN
        ```
      - 识别并移除高度相关特征
        - 列 A：最大值 0.97 > 0.95 → 标记删除
        - 列 B：最大值 0.25 < 0.95 → 保留
        - 列 C：最大值 0.72 < 0.95 → 保留
        - 列 D：全 NaN → 保留
  - 相关系数法
    - 通过 **计算特征与目标变量之间的相关系数**（ 如斯皮尔曼秩相关系数 ）来评估特征的重要性，相关系数的绝对值越大，说明特征与目标变量之间的线性关系越强
    - 例如，在预测房价时，房屋面积与房价之间可能存在较强的正相关关系，面积这个特征就可能是一个重要特征
  - 卡方检验
    - 主要用于分类问题，**通过计算特征与目标变量之间的卡方统计量**，来判断它们之间是否存在显著的相关性。卡方值越大，说明特征与目标变量之间的相关性越强
    - 例如，在疾病诊断中，某些症状特征与疾病类别之间的卡方检验可以帮助筛选出与疾病相关的重要症状
- 优点：
  - 计算速度快，不依赖于具体的模型，可作为特征选择的初步筛选方法
- 缺点：
  - **没有考虑特征之间的组合效应**，可能会忽略一些对模型有重要影响的特征组合

#### 2.2 Wrapper（ 包装法 ）

- 原理：
  - **将特征选择过程与具体的机器学习模型相结合**，通过不断地尝试不同的特征子集，使用模型的性能指标（ 如准确率、召回率等 ）来评估每个特征子集的优劣，最终选择性能最优的特征子集
- 常用方法
  - 前向选择：
    - 从空特征集开始，**每次选择一个使模型性能提升最大的特征加入到特征子集中**，直到达到预设的特征数量或模型性能不再提升为止
    - 例如，在一个文本分类任务中，初始特征集为空，然后依次选择能使分类准确率提升最大的词语作为特征加入到特征子集中
  - 后向选择：
    - 从包含所有特征的集合开始，**每次移除一个使模型性能下降最小的特征**，直到达到预设的特征数量或模型性能开始明显下降为止
  - 双向搜索：
    - **结合了前向选择和后向选择** 的思路，在每一步既可以选择一个特征加入到特征子集中，也可以移除一个特征，根据模型性能的变化来决定具体的操作
- 优点：
  - 考虑了特征之间的组合效应，能够选择出对模型性能提升最大的特征子集
- 缺点：
  - 计算复杂度高，尤其是在特征数量较多时，**需要进行大量的模型训练和评估，时间和计算成本较大**

#### 2.3 Embedded（ 嵌入法 ）

- 原理：
  - **将特征选择过程嵌入到模型的训练过程中**，在模型训练的同时进行特征选择
  - 通过在模型的目标函数中加入正则化项，使得模型在训练过程中自动对特征进行筛选，保留重要的特征，抑制不重要的特征
- 常用方法
  - Lasso 回归：
    - 在线性回归的基础上，加入 L1 正则化项，因为 L1 正则化项会使得部分特征的系数变为 0，从而实现特征选择
    - 例如，在预测学生成绩时，Lasso 回归可以自动筛选出对成绩影响较大的科目作为重要特征
  - Ridge 回归：
    - 加入 L2 正则化项，虽然不会使特征系数严格变为 0，但可以通过调整正则化参数，使得一些不重要特征的系数变得很小，从而降低这些特征的影响
  - 决策树及其集成方法（ 如随机森林、梯度提升树 ）：
    - 决策树在构建过程中会自动选择对分类或回归最有帮助的特征进行节点划分。基于决策树的集成方法可以通过特征在多个决策树中的重要性得分来进行特征选择
    - 例如，随机森林会根据每个特征在决策树中的分裂次数和信息增益等指标来评估特征的重要性
- 优点：
  - 计算效率相对较高，能够在模型训练的同时完成特征选择，且考虑了特征与模型的相互作用
- 缺点：
  - **不同的模型可能会选择出不同的特征子集，对模型的依赖性较强**

## 3. 数据降维

#### 3.1 哪些降维方法

- 可以从不同的角度对它们进行分类
- 按照是否有使用样本的标签值，可以将降维算法分为 **有监督降维** 和 **无监督降维**
  - 无监督降维算法不使用样本标签值，其典型代表是 PCA
  - 有监督的降维算法使用了样本标签值，其典型代表是 LDA
- 按照降维算法使用的映射函数，可以将降维算法分为 **线性降维** 与 **非线性降维**
  - 线性降维算法，根据样本集构造出线性函数完成向低维空间的映射。一般通过对向量 x 进行线性变换即左乘一个投影矩阵 W 而得到结果向量 y，即 y = Wx，PCA 和 LDA 都是线性降维算法，对于线性降维，主要使用 **投影** 的方法
  - 非线性降维算法则构造一个非线性映射完成数据的降维。很多时候数据是非线性的，因此需要使用非线性降维算法以取得更好的效果，LLE 就是一种非线性降维算法，对于非线性降维，主要使用 **流形学习** 的方法

#### 3.2 投影举例

- 比如存在三维的数据集如下（ 著名的瑞士卷数据集 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ed6c0265-2bb5-4c67-bb59-05f5f572b314)
- 可以简单地将数据集投射到一个平面上，这就是投影，结果如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b30495ca-05a1-4eae-8781-17ac6b6b118d)
- 但是这样会将瑞士卷的不同层叠在一起，真正想要的是 “展开” 瑞士卷，这时候就需要使用 流形学习

#### 3.3 PCA（ Principle Component Analysis，即主成分分析法 ）

- PCA 是特征降维的最常用手段，也是最基础的无监督降维算法
- PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并 **期望在所投影的维度上数据的方差最大**，以此使用较少的数据维度，同时保留住较多的原数据点的特性
- 通俗的理解，如果把所有的点都映射到一起，那么几乎所有的信息（ 如点和点之间的距离关系 ）都丢失了，而如果映射后方差尽可能的大，那么 **数据点则会分散开来，以此来保留更多的信息**
- PCA 具有两个性质
  - **最近重构性**：样本点到超平面的距离足够近，即尽可能在超平面附近，期望使均差最小
  - **最大可分性**：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性，期望使方差最大
- 如图，把二维投影到一维上
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/997ccdbf-d6a5-4c31-abee-0e4912a848ba)
- 投影到实线上保留了最大方差。选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这两种做法对应了 PCA 的两种性质，而且得到了相同的结果

#### 3.4 LDA（ Linear Discriminant Analysis，即线性判别分析 ）

- 是有监督的线性降维算法，其基本思想是，将训练样本投影到一个低维空间上（ 比如一条直线 ），**使得同类的样例尽可能近，不同类的样例尽可能远**
- 如图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2011561f-a656-44eb-b707-058d7e9b4e44)
- LDA 和 PCA 的降维思路相似，**都是通过矩阵乘法进行线性降维**，但两者原理有所不同，**PCA 是希望所有样本在每一维上尽可能分开，而 LDA 希望同类近，不同类远**
- 对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近

#### 3.5 LLE（ Locally Linear Embedding，即局部线性嵌入 ）

- 是非线性降维算法，它能够使降维后的数据较好地保持原有流形结构，也是流形学习方法最经典的算法
- 使用 LLE 将三维数据 b 映射到二维 c 之后，**映射后的数据仍能保持原有的数据流形**，即红色的点互相接近，蓝色的也互相接近，说明 LLE 有效地保持了数据原有的流行结构
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/33eac3f0-86cd-4668-9548-d90d1c9690f8)
- 但是 LLE 在有些情况下也并不适用，如果数据分布在整个封闭的球面上，LLE 则不能将它映射到二维空间，且不能保持原有的数据流形。那么我们在处理数据中，首先假设数据不是分布在闭合的球面或者椭球面上
- LLE 算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到，算法的主要步骤分为三步：
  - 寻找每个样本点的 k 个近邻点
  - 由每个样本点的近邻点计算出该样本点的局部重建权值矩阵
  - 由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值
