## 1. 冗余特征

- 现实应用中特征数经常成千上万，还有的特征又分成了几十维度，看似特征多，但真正有用的信息少
- 过多特征会带来以下问题
  - 容易出现数据 **样本稀疏**，去除冗余特征后在这个子空间中样本密度大幅提高
  - 同时 **会让计算更加复杂**，比如许多学习方法都涉及距离计算，但当特征很高时甚至连计算内积都不再容易
  - 能在一定程度上达到 **降噪** 的效果，冗余特征本身可能包含噪声或随机波动
- 减少冗余特征 的主要方法是 **特征选择** 和 **数据降维**
  - 特征选择
    - 核心是 **从原始特征中选择出最具代表性和区分性的特征子集**，以提高模型的性能、减少计算成本和增强模型的可解释性
    - **包括 Filter，Wrapper，Embedded**
  - 数据降维
    - 核心是 **将该维度映射到其他维度**，从而减少维度数量，且在过程中 **尽可能保持数据的特性**

## 2. Filter（ 过滤法 ）

#### 2.1 基础介绍

- 核心
  - **根据计算特征的统计指标对特征进行评估**，然后根据阈值进行筛选，**阈值的选取在不同情况下不同**
- 优缺点
  - 优点：计算速度快，不依赖于具体的模型，可作为特征选择的初步筛选方法
  - 缺点：**没有考虑特征之间的组合效应**，可能会忽略一些对模型有重要影响的特征组合
- 通常实践的时候，按照下面顺序进行筛选
  - 特征自身评估 -> 特征和目标变量评估 -> 特征和其他特征评估
  - 目的是在 特征和其他特征评估 过程中，当两个相似特征时，考虑删除某一个时，将与目标变量较相关的特征删除

#### 2.2 自身评估

- 核心是 **评估特征包含的信息量**，可以大致分为 3 类，通用评估，离散型特征评估，连续型特征评估
- 通用评估
  - 缺失值比例
    - $missing_{ratio} = \frac{n_{missing}}{n_{samples}}$
    - 计算特征中缺失值的比例，如果缺失值比例过大，那么即使用均值等填充，该特征包含的信息量也非常少
    - 通常 高于 50% 可以直接删除，20%-50% 根据情况评估，低于 20% 通常可以合理填充
  - 最大类别占比
    - $max_{class\_ratio} = \max(p_i)$
    - 评估主要取值的占比，接近 1 代表这接近是常量特征
    - 通常删除 大于 0.95 的特征
- 离散型特征评估
  - 信息熵
    - $H(X) = -\sum_{i=1}^{k} p_i \log_2(p_i)$
      - $p_i$：第 i 个类别出现的概率
      - $k$：类别总数
    - 衡量特征取值的不确定性，接近 0 代表所有样本同类别（ 无信息量 ），较大代表各类别均匀分布（ 信息量最大 ）
    - 通常删除 小于 0.5 的特征
  - 唯一值比例
    - $unique_{ratio} = \frac{n_{unique}}{n_{samples}}$
    - 计算特征中不同取值的比例，即 取值类型 / 总数量，接近 0 则 特征几乎恒定（ 常量特征，信息量极低 ），接近 1 则类似 ID 特征
    - 通常保留 0.1\~0.5 之间
- 连续型特征评估
  - 方差选择法
    - $\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^2$
    - 评估特征的离散程度，方差较小的特征可能包含的信息较少，因此可以选择方差大于某个阈值的特征
    - 通常删除 小于 0.01 的特征

#### 2.3 和目标变量评估

- 核心是 **删除与目标变量不相关的特征**，有多种评估方法评估特征和目标变量之间的关系程度
  - 皮尔逊相关系数
    - $\rho_{X,Y} = \frac{\text{cov}(X,Y)}{\sigma_X \sigma_Y}$
    - 适用于捕捉 **两个连续变量间的线性关系**，但 **对异常值敏感**
  - 斯皮尔曼相关系数
    - $\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}$
    - 适用于捕捉 **任意特征类型的有序关系**，且 **对异常值不敏感**
  - 互信息
    - $I(X;Y) = \sum*{y \in Y} \sum*{x \in X} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right)$
    - 适用于捕捉 **任意特征类型的关系（ 线性、非线性、非单调 ）**
  - 卡方检验
    - $\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$
    - 适用于捕捉 **两个分类变量间的独立性检验**
  - 克莱姆 V 系数
    - $V = \sqrt{\frac{\chi^2}{n \times \min(r-1, c-1)}}$
    - 是 卡方检验 的归一化版本
  - 方差分析
    - $F = \frac{\text{组间方差}}{\text{组内方差}}$
    - 适用于捕捉 **连续变量与离散变量的关系**
  - 点二列相关系数
    - $r_{pb} = \frac{M_1 - M_0}{s} \sqrt{\frac{n_1 n_0}{n(n-1)}}$
    - 适用于捕捉 **连续变量与二元分类变量的相关性**

#### 2.4 和其他特征评估

- 核心是 **删除与其他特征高度相关的特征**，与和目标变量评估所用到的评估指标相同，只不过需要计算所有特征之间的相关系数
- 例如，假设现在有 A，B，C，D 四个特征，阈值设为 0.95
  - 计算绝对相关系数矩阵
    ```
        A      B      C      D
    A  1.00   0.97   0.20   0.15
    B  0.97   1.00   0.25   0.18
    C  0.20   0.25   1.00   0.72
    D  0.15   0.18   0.72   1.00
    ```
  - 提取上三角矩阵（ 不包含对角线 ）
    ```
        A      B      C      D
    A   NaN   0.97   0.20   0.15
    B   NaN    NaN   0.25   0.18
    C   NaN    NaN    NaN   0.72
    D   NaN    NaN    NaN    NaN
    ```
  - 识别并移除高度相关特征
    - 列 A：最大值 0.97 > 0.95 → 标记删除
    - 列 B：最大值 0.25 < 0.95 → 保留
    - 列 C：最大值 0.72 < 0.95 → 保留
    - 列 D：全 NaN → 保留

## 3. Wrapper（ 包装法 ）

#### 3.1 基础介绍

- 核心
  - **将特征选择过程与具体的机器学习模型相结合**，通过不断地尝试不同的特征子集，使用模型的性能指标（ 如准确率、召回率等 ）来评估每个特征子集的优劣，最终选择性能最优的特征子集
- 优缺点
  - 优点：考虑了特征之间的组合效应，能够选择出对模型性能提升最大的特征子集
  - 缺点：计算复杂度高，尤其是在特征数量较多时，**需要进行大量的模型训练和评估，时间和计算成本较大**

#### 3.2 常用方法

- **前向选择**
  - 从空特征集开始，**每次选择一个使模型性能提升最大的特征加入到特征子集中**，直到达到预设的特征数量或模型性能不再提升为止
  - 例如，在一个文本分类任务中，初始特征集为空，然后依次选择能使分类准确率提升最大的词语作为特征加入到特征子集中
- **后向选择**
  - 从包含所有特征的集合开始，**每次移除一个使模型性能下降最小的特征**，直到达到预设的特征数量或模型性能开始明显下降为止
- **双向搜索**
  - **结合了前向选择和后向选择** 的思路，在每一步既可以选择一个特征加入到特征子集中，也可以移除一个特征，根据模型性能的变化来决定具体的操作

## 4. Embedded（ 嵌入法 ）

#### 4.1 基础介绍

- 原理
  - **将特征选择过程嵌入到模型的训练过程中**，在模型训练的同时进行特征选择
  - 通过在模型的目标函数中加入正则化项，使得模型在训练过程中自动对特征进行筛选，保留重要的特征，抑制不重要的特征
- 优缺点
  - 优点：计算效率相对较高，能够在模型训练的同时完成特征选择，且考虑了特征与模型的相互作用
  - 缺点：**不同的模型可能会选择出不同的特征子集，对模型的依赖性较强**

#### 4.2 常用方法

- Lasso 回归
  - 在线性回归的基础上，加入 L1 正则化项，因为 L1 正则化项会使得部分特征的系数变为 0，从而实现特征选择
  - 例如，在预测学生成绩时，Lasso 回归可以自动筛选出对成绩影响较大的科目作为重要特征
- Ridge 回归
  - 加入 L2 正则化项，虽然不会使特征系数严格变为 0，但可以通过调整正则化参数，使得一些不重要特征的系数变得很小，从而降低这些特征的影响
- 决策树及其集成方法（ 如随机森林、梯度提升树 ）
  - 决策树在构建过程中会自动选择对分类或回归最有帮助的特征进行节点划分。基于决策树的集成方法可以通过特征在多个决策树中的重要性得分来进行特征选择
  - 例如，随机森林会根据每个特征在决策树中的分裂次数和信息增益等指标来评估特征的重要性

## 5. 数据降维

#### 5.1 哪些降维方法

- 可以从不同的角度对它们进行分类
- 按照是否有使用样本的标签值，可以将降维算法分为 **有监督降维** 和 **无监督降维**
  - 无监督降维算法不使用样本标签值，其典型代表是 PCA
  - 有监督的降维算法使用了样本标签值，其典型代表是 LDA
- 按照降维算法使用的映射函数，可以将降维算法分为 **线性降维** 与 **非线性降维**
  - 线性降维算法，根据样本集构造出线性函数完成向低维空间的映射。一般通过对向量 x 进行线性变换即左乘一个投影矩阵 W 而得到结果向量 y，即 y = Wx，PCA 和 LDA 都是线性降维算法，对于线性降维，主要使用 **投影** 的方法
  - 非线性降维算法则构造一个非线性映射完成数据的降维。很多时候数据是非线性的，因此需要使用非线性降维算法以取得更好的效果，LLE 就是一种非线性降维算法，对于非线性降维，主要使用 **流形学习** 的方法

#### 5.2 投影举例

- 比如存在三维的数据集如下（ 著名的瑞士卷数据集 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ed6c0265-2bb5-4c67-bb59-05f5f572b314)
- 可以简单地将数据集投射到一个平面上，这就是投影，结果如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b30495ca-05a1-4eae-8781-17ac6b6b118d)
- 但是这样会将瑞士卷的不同层叠在一起，真正想要的是 “展开” 瑞士卷，这时候就需要使用 流形学习

#### 5.3 PCA（ Principle Component Analysis，即主成分分析法 ）

- PCA 是特征降维的最常用手段，也是最基础的无监督降维算法
- PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并 **期望在所投影的维度上数据的方差最大**，以此使用较少的数据维度，同时保留住较多的原数据点的特性
- 通俗的理解，如果把所有的点都映射到一起，那么几乎所有的信息（ 如点和点之间的距离关系 ）都丢失了，而如果映射后方差尽可能的大，那么 **数据点则会分散开来，以此来保留更多的信息**
- PCA 具有两个性质
  - **最近重构性**：样本点到超平面的距离足够近，即尽可能在超平面附近，期望使均差最小
  - **最大可分性**：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性，期望使方差最大
- 如图，把二维投影到一维上
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/997ccdbf-d6a5-4c31-abee-0e4912a848ba)
- 投影到实线上保留了最大方差。选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这两种做法对应了 PCA 的两种性质，而且得到了相同的结果

#### 5.4 LDA（ Linear Discriminant Analysis，即线性判别分析 ）

- 是有监督的线性降维算法，其基本思想是，将训练样本投影到一个低维空间上（ 比如一条直线 ），**使得同类的样例尽可能近，不同类的样例尽可能远**
- 如图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2011561f-a656-44eb-b707-058d7e9b4e44)
- LDA 和 PCA 的降维思路相似，**都是通过矩阵乘法进行线性降维**，但两者原理有所不同，**PCA 是希望所有样本在每一维上尽可能分开，而 LDA 希望同类近，不同类远**
- 对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近

#### 5.5 LLE（ Locally Linear Embedding，即局部线性嵌入 ）

- 是非线性降维算法，它能够使降维后的数据较好地保持原有流形结构，也是流形学习方法最经典的算法
- 使用 LLE 将三维数据 b 映射到二维 c 之后，**映射后的数据仍能保持原有的数据流形**，即红色的点互相接近，蓝色的也互相接近，说明 LLE 有效地保持了数据原有的流行结构
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/33eac3f0-86cd-4668-9548-d90d1c9690f8)
- 但是 LLE 在有些情况下也并不适用，如果数据分布在整个封闭的球面上，LLE 则不能将它映射到二维空间，且不能保持原有的数据流形。那么我们在处理数据中，首先假设数据不是分布在闭合的球面或者椭球面上
- LLE 算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到，算法的主要步骤分为三步：
  - 寻找每个样本点的 k 个近邻点
  - 由每个样本点的近邻点计算出该样本点的局部重建权值矩阵
  - 由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值
