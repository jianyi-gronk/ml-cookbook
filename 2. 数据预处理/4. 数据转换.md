- **核心是将数据转换成更适合模型输入的类型**

## 1. 数值特征处理

- 核心思路有两种
  - **一种是统一不同特征的取值范围**
  - **一种是减少数据复杂性，即从无限的连续值转换成有限个区间（ 类似于转换成分类特征 ）**

#### 1.1 归一化 / 中心化 / 标准化

- 归一化，中心化，标准化 概念类似，平常可以 **统一称为归一化**，细节差异为
  - 归一化：将数据处理后限制在某个固定范围内，通常是 [0, 1] 或者 [-1, 1]，当然也可类似 [-2, 0.5]
  - 中心化：每个数据减去其均值，使得处理后的数据特征的均值为零
  - 标准化：意思是对数据处理后，使数据具有特定的统计特性，通常是将数据转换为具有零均值和单位方差的形式
- 核心思想
  - **使不同特征具有相同范围区间，增强数据可比性，避免某些特征因数值过大或过小而对模型训练产生过大或过小的影响，且可以让梯度下降运行更快**
- 常用方法
  - 线性归一化（ 最小最大值归一化 ）
    - 通常 **将数据范围线性转换到 [0, 1] 区间**，公式为 $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$，如果想要转换到其他区间 [a, b]，则公式为 $x' = a + \frac{x - x_{min}}{x_{max} - x_{min}} \times (b - a)$
    - 适用场景：**适用于数据分布较为均匀，且不存在极端值的情况**
  - Z-score 归一化（ 标准差归一化 ）
    - **将数据的均值变为 0，标准差变为 1**，公式为 $x' = \frac{x - μ}{σ}$，其中 μ 是数据的均值，σ 是数据的标准差
    - 适用场景：**需要数据分布近似服从正态分布的情况，可以将数据转换为标准正态分布**。在一些基于梯度下降的机器学习算法中，如神经网络，Z-score 通常能提高模型的训练效果
  - 均值归一化
    - **将数据的均值变为 0，数据范围变成 [-1, 1]**，公式为 $x' = \frac{x - μ}{x_{max} - x_{min}}$，其中 μ 是数据的均值
    - 适用场景：**在数据的最大值和最小值已知，且需要在一定程度上保留数据的相对大小关系**
  - 小数定标归一化
    - 本质是 **通过移动属性值的小数点缩放数据**，公式为 $x' = \frac{x}{10^j}$，其中 j 是绝对值最大的数值的位数，比如如果绝对值最大的数是 567.8，则 j 是 3，然后将数据集中所有数除以 1000
    - 适用场景：**主要用于数据取值范围较大，且希望通过简单的小数点移动来实现归一化的情况**

#### 1.2 离散化

- 核心思想
  - **将连续的数值范围划分为有限个区间或类别，减少数据的复杂度，增加对异常数据的鲁棒性，也方便适配决策树等模型**
- 常用方法
  - 等距分箱
    - **将数值特征的取值范围划分为若干个等间距的区间**。例如，对于年龄特征，若取值范围是 0 ～ 100 岁，可以设定每个区间为 10 岁，那么就会得到 [0, 10)、[10, 20)、[20, 30) 等这样的区间
    - 适用场景：**依赖数据分布均匀，否则可能导致某些区间包含大量数据，而某些区间数据很少**
  - 等频分箱
    - **将数据按照频率划分为若干个区间，使得每个区间内包含的样本数量相等**。例如，有 100 个样本的年龄数据，要划分为 5 个区间，那么每个区间应包含 20 个样本
    - 适用场景：**依赖数据是有序的**（ 大量数据的排序是很耗时的 ），但是这样区间的边界不太直观
  - 聚类分箱
    - 利用聚类算法（ 如 K-means 算法 ）**将数值特征的数据点划分为不同的簇**，每个簇对应一个离散的类别。这样能够根据数据的内在结构进行离散化，更灵活地适应不同的数据分布
    - 适用场景：对数据分布适应性强，但计算复杂度较高，并且分箱的结果很不稳定
  - 决策树分箱
    - **利用决策树能够自动寻找数据中最佳分割点的特性来实现数据分箱**，本质就是将连续的特征值按照决策树的划分规则，划分到不同的箱（ 区间 ）中，这样可以考虑特征与目标的关系去考虑特征与目标的关系
    - 适用场景：对数据分布适应性强，但计算复杂度较高

## 2. 分类特征处理

- 大多数机器学习算法，如线性回归、逻辑回归、支持向量机、神经网络等，都是基于数值计算的。**这些算法要求输入的特征是数值型数据，以便进行数学运算和模型训练，因此分类特征经常需要转换成数值特征**
- 核心思路有两种
  - **一种是通过自身特征转换，例如 独热编码，标签编码，哈希编码**
  - **一种是通过目标特征和自身特征关联，例如 均值编码，WOE 编码，Helmert 编码**

#### 2.1 独热编码

- **独热编码将分类特征转换为二进制向量**，对于具有 n 个不同类别的分类特征，独热编码会将其转换为 n 维的二进制向量，每个向量中只有一个元素为 1，其余元素均为 0，且不同类别的向量中 1 所在的位置不同
- 假设表示颜色的分类变量，其取值为 ['红色', '蓝色', '绿色']，使用独热编码后，颜色特征会变成三个特征，[是否红色, 是否蓝色, 是否绿色]
  - 红色：[1, 0, 0]
  - 蓝色：[0, 1, 0]
  - 绿色：[0, 0, 1]

#### 2.2 标签编码

- **标签编码是将分类变量的每个类别映射为整数的编码方式**，它会为每个不同的类别分配一个唯一的整数，从 0 开始递增
- 假设表示颜色的分类变量，其取值为 ['红色', '蓝色', '绿色']，使用标签编码后，每个颜色会被转换为如下的整数：
  - 红色：0
  - 蓝色：1
  - 绿色：2

#### 2.3 哈希编码

- **通过哈希函数将分类特征的类别值映射到一个固定范围的整数**。例如，使用一个简单的哈希函数将 “红色”“蓝色”“绿色” 等颜色类别映射到 0 到 99 之间的整数
- 可以处理大量的类别，并且不需要事先知道所有的类别值，对于未知的新类别也能进行编码。但可能会存在不同的类别被映射到相同的哈希值，这可能会导致信息丢失或模型性能下降。而且哈希编码后的特征值通常没有明确的语义含义，不利于对模型进行解释

#### 2.4 均值编码

- **用每个类别对应的目标变量的均值来替换该类别**
- 例如，在一个预测客户是否会购买产品（ 目标变量为 0 或 1 ）的问题中，对于 “职业” 这个分类特征，计算每个职业类别中购买产品的客户比例，用这个比例值作为该职业类别的编码值
- 能够利用目标变量的信息，将分类特征 **转换为具有实际意义的数值**，可能比独热编码和标签编码更能反映特征与目标之间的关系，且不会像独热编码那样大幅增加特征维度

#### 2.5 WOE 编码

- 只能用于目标变量是二分类的问题，**计算每个类别中目标变量为正例和负例的比例，然后通过对数变换得到 WOE 值**。公式为
  ![image](https://github.com/user-attachments/assets/31bfb9fd-c009-444a-ba98-13c92c1ae4db)
- 假设现在有个性别特征，目标变量是是否购买，男性中的购买比率是 0.6，女性是 0.3，则男性的 WOE 是 ln(0.6/0.3)，女性是 len(0.3/0.6)
- 考虑了类别中在目标变量的分布，还通过对数变换增强了不同类别之间的差异，使得模型更容易捕捉到特征与目标之间的关系，但是只能解决二分类问题

#### 2.6 Helmert 编码

- **将分类特征转换为一组对比变量**。与独热编码类似，也是创建多个新的二进制特征，但不同的是，**Helmert 编码的每一列是对某一类别与其他类别均值的对比**，对于有 n 个类别的分类特征，Helmert 编码会生成 n-1 个新的特征
- 假设有一个分类特征 “水果类型”，它有三个类别：苹果、香蕉、橙，同时目标变量是水果价格
  - 计算第一列编码
    ![image](https://github.com/user-attachments/assets/87e69543-5b9b-4f9c-b625-a355d0486c8a)
  - 计算第二列编码
    ![image](https://github.com/user-attachments/assets/eaa8bb95-89db-46a9-8655-03c218349027)
- 适用于需要研究类别之间相对关系的场景。但解释性相对独热编码来说稍差，需要对编码的原理和意义有较深入的理解才能正确解读模型结果
