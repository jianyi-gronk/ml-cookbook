## 1. 网格搜索（ Grid Search ）

#### 1.1 核心思路

- 就是 **穷举搜索，在所有候选参数选择中，通过循环遍历，找到表现最好的参数组合**，可以保证在指定的参数范围内找到精度最高的参数
- 缺点是耗时久，当超参数存在 3 个以上时，组合种数会是笛卡尔积

#### 1.2 使用

- scikit-learn 提供了 sklearn.model_selection.GridSearchCV
- 参数
  - estimator：是要进行参数调优的机器学习模型对象（ 如分类器、回归器等 ），它必须实现 scikit-learn 规定的接口（ 如 fit、predict 等方法 ）
  - param_grid：字典或字典列表，指定要搜索的参数名称和取值组合
  - scoring：评估模型性能的策略，可使用字符串、可调用对象、列表、元组或字典表示
  - n_jobs：并行运行的作业数，None 表示 1，-1 表示使用所有处理器
  - refit：是否使用最佳参数在整个数据集上重新拟合估计器，也可传入字符串或可调用对象
  - cv：交叉验证的分割策略，可取值 None（ 默认 5 折交叉验证 ）、整数（ 指定折数 ）、交叉验证生成器或可迭代对象
  - verbose：控制日志信息的详细程度，值越大输出信息越多
  - pre_dispatch：控制并行执行时调度的作业数，可取值 None、整数或字符串表达式
  - error_score：拟合估计器出错时的得分设置，可取值 'raise' 或数值
  - return_train_score：是否在 cv_results 属性中包含训练分数，默认为 False

```python
class sklearn.model_selection.GridSearchCV(
  estimator,
  param_grid,
  scoring=None,
  n_jobs=None,
  refit=True,
  cv=None,
  verbose=0,
  pre_dispatch='2*n_jobs',
  error_score=nan,
  return_train_score=False
)
```

## 2. 随机搜索（ Random Search ）

#### 2.1 核心思路

- **即随机采样点（ 参数组合 ），虽然每个点的被选取概率相同，但是可设置点的分布类型，让点的分布密度不同**
- 数据的分布类型可以选择，类似 scipy.stats.distributions 值，例如
  - uniform：均匀分布，会生成连续型随机数，在指定区间内每个值的概率相等
  - randint：均匀整数分布，生成 离散型整数，在指定区间内每个整数的概率相等
  - loguniform：对数均匀分布，先在 [log(a), log(b)] 区间内均匀采样，再通过指数运算得到实际值
  - normal：正态分布，生成围绕均值（ μ ）集中分布的连续值，适用于参数在某个中心点附近波动的场景

#### 2.2 使用

- scikit-learn 提供了 sklearn.model_selection.RandomizedSearchCV
- 参数
  - param_distributions：和 GridSearchCV 相似，指定超参数的分布或取值列表。字典的键是超参数名，值可以是分布对象（ 如 scipy.stats.distributions 中的分布 ），用于随机采样；也可以是普通列表，此时会均匀采样。如果是列表形式，所有参数组合会被无放回采样；若至少有一个参数是分布形式，则采用有放回采样
  - n_iter：控制采样的参数设置数量，默认值为 10。该值越大，搜索越全面，但计算成本越高
  - random_state：对数均匀分布，用于设置随机数种子，保证结果可复现，可以是整数、RandomState 实例或 None

```python
class sklearn.model_selection.RandomizedSearchCV(
  estimator,
  param_distributions,
  n_iter=10,
  scoring=None,
  n_jobs=None,
  refit=True,
  cv=None,
  verbose=0,
  pre_dispatch='2*n_jobs',
  random_state=None,
  error_score=nan,
  return_train_score=False
)
```

## 3. 贝叶斯优化（ Bayesian Optimization ）

- 比网格搜索或随机搜索更高效，尤其在计算资源有限时

#### 3.1 优化步骤

- 用 **替代模型** 拟合已知数据（ x 是超参数组合，y 是模型得分 ），然后预测未采样的超参数组合的模型得分，用于替代高成本的目标函数（ 即不再需要去跑每一组超参数去获得模型得分 ）
- 然后 **采集函数** 使用替代模型的预测结果，来决定下一个要采样的超参数组合，然后再丢给替代模型去重新拟合

#### 3.2 替代模型

- 一般使用高斯过程（ Gaussian Process，GP ）作为替代模型
- **高斯过程 根据 已知数据（ x 是超参数组合，y 是模型得分 ）建立函数，函数的输入还是超参数组合，但是输出 y 是模型得分概率的 高斯分布，因为是概率，所以是不确定值**
- 高斯过程的具体输出
  - **预测均值 $\mu(x)$**：可以让我们衡量该超参数组合的目标得分
  - **预测方差 $\sigma^2(x)$**：即不确定性，可以让我们判断哪些区域还有比较大的探索空间

#### 3.3 采集函数

- **结合预测的预测均值和预测方差，平衡探索与利用**，选择下一个最有潜力的超参数点
  - 利用：选择均值高的区域，直接寻找当前最优解
  - 探索：选择方差高的区域，探索未知但可能存在更优解的区域
- 采集函数的设计：
  - 高斯过程的预测均值和方差是采集函数的输入
  - 期望改进（ EI ）：计算新采样点能改进当前最优值的概率期望
  - 置信上限（ UCB ）：直接选择 $\mu(x) + k\sigma(x)$ 最大的点（ k 控制探索强度 ）

#### 3.4 使用

```python
class bayes_opt.BayesianOptimization(
  f: collections.abc.Callable[..., float] | None,
  pbounds: Mapping[str, tuple[float, float]],
  acquisition_function: AcquisitionFunction | None = None,
  constraint: NonlinearConstraint | None = None,
  random_state: int | RandomState | None = None,
  verbose: int = 2,
  bounds_transformer: DomainTransformer | None = None,
  allow_duplicate_points: bool = False
)
```

- 主要参数是前两个
  - 第二个参数是字典，键是需要优化的超参数，值是最大值和最小值的元组
  - 第一个参数是函数，参数是第二个参数字典的键，返回值是该超参数组合的模型得分

```python
from bayes_opt import BayesianOptimization
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score

# 省略 train_X，train_y 定义

def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
  val = cross_val_score(
    LGBMRegressor(
      objective='regression_11',
      num_leaves=int(num_leaves) ,
      max_depth=int(max_depth),
      subsample=subsample,
      min_child_samples=int(min_child_samples),
    ),
    X=train_X,
    y=train_y,
    verbose=0,
    cv = 5,
    scoring=make_scorer(mean_absolute_errror)
  ).mean()

  return 1 - val

# 实例化 bayes 优化对象
rf_bo = BayesianOptimization(
  rf_cv,
  {
    'num_leaves': (2, 100),
    'max_depth': (2, 100),
    'subsample': (0.1, 1),
    'min_child_samples': (2, 100)
  }
)

# 运行 bayes 优化
rf_bo.maximize()

print(f"最佳参数: {rf_bo.res['max']}")
```

## 4. TPE（ Tree-structured Parzen Estimator ）

#### 4.1 优化步骤

- 用 **两个高斯混合模型（ Gaussian Mixture Model，GMM ）** 分别拟合表现 较好 和 较差 的超参数的分布（ 控制如何分割，是 TPE 的超参数，通常以 20% 作为分割点 ），分别得到 **建议分布** 和 **损失分布**
- 最终通过最大化 建议分布 与 损失分布 的 **比值** 来选择下一个采样点

#### 4.2 高斯混合模型

- 输入
  - 高斯过程 的输入是整个 超参数组合 和 对应模型得分 的数据集
  - 高斯混合模型 的输入是 较好 或者 较差 的超参数组合，**对应模型得分只是用来区分超参数组合的 好 和 坏，并不作为输入**
- 输出
  - 高斯过程 的输出是一个函数
  - 每个 高斯混合模型 中存在多个高斯分布，**每个高斯分布都会去拟合所有输入的超参数组合，输出的是输入超参数组合在该分布下的概率密度值**（ 如果拟合的是较好数据集，则较高的概率密度值意味着该超参数组合更接近 “好” 的超参数分布模式 ），最后的混合模型的输出是将其中所有高斯分布的输出进行加权累加，得到最终的 建议 或 损失 分布

#### 4.3 使用

- 首先创建采样器

  ```python
  class optuna.samplers.TPESampler(
    consider_prior=True,
    prior_weight=1.0,
    consider_magic_clip=True,
    consider_endpoints=False,
    n_startup_trials=10,
    n_ei_candidates=24,
    gamma=<function default_gamma>,
    weights=<function default_weights>,
    seed=None,
    multivariate=False,
    warn_independent_sampling=True
  )
  ```

- 然后将采样器作为参数，创建学习器

  ```python
  optuna.create_study(
    storage=None,
    sampler=None,
    pruner=None,
    study_name=None,
    direction=None,
    load_if_exists=False,
    directions=None
  )
  ```

- 最后运行学习期，找到最优的超参数组合

  ```python
  study.optimize(
    func,
    n_trials=None,
    timeout=None,
    n_jobs=1,
    catch=(),
    callbacks=None,
    gc_after_trial=False,
    show_progress_bar=False
  )
  ```

- 实战

  ```python
  import optuna
  from lightgbm import LGBMRegressor
  from sklearn.model_selection import cross_val_score

  # 省略 train_X，train_y 定义

  def objective(trial):
    # 定义超参数空间
    params = {
      'objective': 'regression',
      'metric': 'mse',
      'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
      'max_depth': trial.suggest_int('max_depth', 3, 10),
      'num_leaves': trial.suggest_int('num_leaves', 20, 200),
      'subsample': trial.suggest_float('subsample', 0.5, 1.0),
      'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
      'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
      'verbose': -1
    }

    # 创建模型并交叉验证
    model = LGBMRegressor(**params)
    mse = cross_val_score(
      model,
      X=train_X,
      y=train_y,
      cv=5,
      scoring='neg_mean_squared_error'
    ).mean()

    # Optuna 最小化目标，所以返回正的 MSE
    return -mse

  # 初始化 TPE 采样器
  sampler = optuna.samplers.TPESampler(
    n_startup_trials=5,  # 前5次随机采样
    multivariate=True    # 启用多元 TPE
  )

  # 创建学习器
  study = optuna.create_study(sampler=sampler, direction='minimize')

  # 运行优化
  study.optimize(objective, n_trials=30)

  # 输出结果
  print(f"最佳均方误差: {study.best_value:.4f}")
  print("最佳参数:")
  for key, value in study.best_params.items():
    print(f"{key}: {value}")
  ```

## 5. Hyperband

#### 5.1 优化步骤

- 核心是 **动态资源分配** 和 **早期淘汰** 的协同作用
  - 动态资源分配：**给表现好的超参数配置更多计算资源**（ 如训练轮数 ），让它们充分验证潜力
  - 早期淘汰：**提前终止表现差的配置**，避免浪费资源
- 具体步骤为，初始阶段以少量资源（ 如训练轮数 ）评估大量超参数配置，只保留表现较好的 1/η（ η 可设置 ），剩余配置进入下一轮并获得更多资源
- 例如，假设初始有 8 个超参数组合，且 η 为 2，第一轮每个超参数组合训练 1 个 epoch，淘汰 4 个后，剩下的 4 个超参数组合训练 2 个 epoch，依此类推，直到只剩 1 个超参数组合

#### 5.2 使用

```python
class optuna.pruners.HyperbandPruner(
  min_resource=1,
  max_resource='auto',
  reduction_factor=3,
  bootstrap_count=0
)
```

- 使用方法和 TPE 的例子完全一样

  ```python
  import optuna
  from lightgbm import LGBMRegressor

  def objective(trial):
    # 定义超参数空间
    params = {
      'objective': 'regression',
      'metric': 'mse',
      'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
      'max_depth': trial.suggest_int('max_depth', 3, 10),
      'num_leaves': trial.suggest_int('num_leaves', 20, 200),
      'subsample': trial.suggest_float('subsample', 0.5, 1.0),
      'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
      'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
      'verbose': -1
    }

    # 初始化模型
    model = LGBMRegressor(**params)

    # 动态训练并报告中间结果（ 每 5 轮报告一次 ）
    for step in range(1, 100):
      model.fit(
        train_X,
        train_y,
        eval_set=[(X_valid, y_valid)],
        eval_metric='mse',
        verbose=False,
        early_stopping_rounds=10,
        callbacks=[
          lgb.reset_parameter(learning_rate=params['learning_rate'])
        ]
      )

      # 报告当前 MSE
      current_mse = model.best_score_['valid_0']['mse']
      trial.report(current_mse, step)  # 向 Optuna 报告中间结果

      # 检查是否需要剪枝
      if trial.should_prune():
        raise optuna.TrialPruned()

    return current_mse  # 返回最终 MSE

  # 配置 Hyperband 剪枝器
  pruner = optuna.pruners.HyperbandPruner(
    min_resource=10,    # 最小训练步数（ 轮次 ）
    max_resource=100,   # 最大训练步数（ 轮次 ）
    reduction_factor=3  # 每轮保留 1/3 的试验
  )

  # 创建学习器
  study = optuna.create_study(
    direction='minimize',
    pruner=pruner
  )

  # 运行优化（ 总试验次数 = 20 ）
  study.optimize(objective, n_trials=20)

  # 输出结果
  print(f"最佳均方误差: {study.best_value:.4f}")
  print("最佳参数:")
  for key, value in study.best_params.items():
    print(f"{key}: {value}")
  ```

## 6. BOHB（ Bayesian Optimization and HyperBand ）

#### 6.1 优化步骤

- 看名字就能知道，BOHB 算法混合使用了 贝叶斯优化 和 Hyperband 两种算法
- 核心是 **贝叶斯优化通过代理模型和采集函数指导搜索方向，而 HyperBand 通过动态资源分配快速淘汰低性能配置，两者交替协作**
- 具体步骤
  - 初始随机采样，通过随机采样生成一批初始候选超参数组合（ 如 8 个 ）
  - HyperBand 多轮评估，候选组合按资源递增策略评估（ 如 1 epoch → 2 epochs → 4 epochs ），每轮淘汰表现最差的部分配置（ 如保留 1/2 ）
  - 贝叶斯优化拟合所有历史数据（ 包括各轮资源下的表现 ），通过采集函数生成新候选组合（ 如 4 个 ）
  - 循环优化，重复步骤 2 和 3，直至资源耗尽（ 生成的候选数递减为 8 → 4 → 2 → 1 ）

#### 6.2 使用

- 虽然 optuna 没有直接实现 BOHB 算法，但可以通过组合其现有的采样器 TPESampler 和剪枝器 HyperbandPruner 来近似实现 BOHB 的功能

```python
import optuna
from lightgbm import LGBMRegressor

# 省略 train_X，train_y，valid_X，valid_y 定义

def objective(trial):
  # 定义超参数空间
  params = {
    'objective': 'regression',
    'metric': 'mse',
    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-6, 1e-1),
    'max_depth': trial.suggest_int('max_depth', 3, 10),
    'num_leaves': trial.suggest_int('num_leaves', 20, 200),
    'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
    'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),
    'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),
    'verbose': -1
  }

  model = LGBMRegressor(**params)

  # 动态训练并报告中间结果
  for step in range(1, 100):
    model.fit(
      train_X,
      train_y,
      eval_set=[(valid_X, valid_y)],
      eval_metric='mse', verbose=False,
      early_stopping_rounds=10
    )

    current_mse = model.best_score_['valid_0']['mse']
    trial.report(current_mse, step)

    if trial.should_prune():
      raise optuna.TrialPruned()

  return current_mse

# 创建学习器
study = optuna.create_study(
  direction='minimize',
  sampler=optuna.samplers.TPESampler(),
  pruner=optuna.pruners.HyperbandPruner()
)

# 运行优化
study.optimize(objective, n_trials=20)

# 输出结果
print(f"Best MSE: {study.best_value:.4f}")
print("Best parameters:")
for key, value in study.best_params.items():
  print(f"{key}: {value}")
```

## 7. PBT（ Population Based Training ）

- 核心是通过维护一个超参数配置种群，在训练过程中动态调整参数，平衡探索与利用
- 具体步骤
  - 初始化种群：随机生成 N 个超参数组合（ 如学习率，batch_size 等 ），初始范围需合理
  - 并行异步训练：各配置独立训练，允许进度差异（ 如 A 训练 50 轮，B 训练 100 轮 ），**定期同步评估**
  - 评估与筛选：按性能排序（ 如准确率 ），保留前 k% 的配置（ 如前 25% ）
  - **参数 迁移 与 变异**：迁移，即将优秀配置的参数复制给较差配置（ 全量或部分复制 ）；变异，即对迁移后的参数施加随机扰动（ 如学习率 ±50% ），探索新空间
  - 循环优化：重复步骤 2，3，4，直至性能停滞或资源耗尽

## 8. CMA-ES（ Covariance Matrix Adaptation Evolution Strategy ）

#### 8.1 优化步骤

- 核心是 **动态调整搜索分布（ 均值和协方差矩阵 ），在连续空间中高效探索最优解**
- 具体步骤
  - 初始化：确定初始均值向量和协方差矩阵。均值向量可基于先验知识或随机选取，协方差矩阵常初始化为单位矩阵
  - 种群生成：基于当前均值和协方差矩阵，随机采样生成候选解
  - 评估候选解：计算每个候选解对应的目标函数值
  - 选择优秀解：根据目标函数值对候选解进行排序，选择排名靠前的一部分解作为优秀解
  - 更新参数：根据优秀解的分布，更新协方差矩阵（ 控制搜索方向 / 范围 ）和步长（ σ ），向更优区域收缩或扩展。
  - 迭代优化：保留表现最佳的个体，用更新后的均值和协方差矩阵进行下一轮迭代
  - 终止判断：检查是否满足终止条件（ 如达到最大迭代次数、目标函数值收敛等 ），若满足则输出最优解，否则返回步骤 2 继续迭代

#### 8.2 使用

- 需要使用 pycma 框架

```python
import cma
import lightgbm as lgb
from sklearn.metrics import mean_squared_error

# 省略 train_X，train_y，valid_X，valid_y 定义

# 定义目标函数，用于计算给定超参数下模型的均方误差
def objective(params):
  # 超参数范围映射
  learning_rate = 10 ** params[0]
  num_leaves = int(10 ** params[1])
  max_depth = int(params[2])
  reg_alpha = 10 ** params[3]
  reg_lambda = 10 ** params[4]

  # 创建 LGBMRegressor 模型
  model = lgb.LGBMRegressor(
    learning_rate=learning_rate,
    num_leaves=num_leaves,
    max_depth=max_depth,
    reg_alpha=reg_alpha,
    reg_lambda=reg_lambda,
    random_state=42
  )

  # 训练模型
  model.fit(train_X, train_y)

  # 预测并计算均方误差
  y_pred = model.predict(valid_X)
  mse = mean_squared_error(valid_y, y_pred)
  return mse


# 初始猜测值
initial_guess = [-2, 1, 5, -1, -1]
# 初始标准差
sigma = 0.5

# 运行 CMA-ES 优化
es = cma.CMAEvolutionStrategy(initial_guess, sigma)
while not es.stop():
  solutions = es.ask()
  es.tell(solutions, [objective(x) for x in solutions])
  es.logger.add()
  es.disp()

# 获取最优解
best_solution = es.result.xbest

# 解析最优超参数
best_learning_rate = 10 ** best_solution[0]
best_num_leaves = int(10 ** best_solution[1])
best_max_depth = int(best_solution[2])
best_reg_alpha = 10 ** best_solution[3]
best_reg_lambda = 10 ** best_solution[4]

print(f"最优学习率: {best_learning_rate}")
print(f"最优叶子节点数: {best_num_leaves}")
print(f"最优最大深度: {best_max_depth}")
print(f"最优 L1 正则化系数: {best_reg_alpha}")
print(f"最优 L2 正则化系数: {best_reg_lambda}")
```

## 9. 集成优化

- 还有种特殊情况，当想要将多个模型的预测结果加权累加成最终结果时，可能需要找到每个模型的最佳的权重
- 接下来的举例以 rf，extra，xgb，hgb，ctb，lgb 六个模型的拼接为例，争取找到最佳的权重组合

```python
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor

# 省略 train_X，train_y，valid_X，valid_y 定义

# 训练六个基础模型
models = [
  RandomForestRegressor(random_state=42),
  ExtraTreesRegressor(random_state=42),
  XGBRegressor(random_state=42),
  HistGradientBoostingRegressor(random_state=42),
  CatBoostRegressor(random_state=42, verbose=0),
  LGBMRegressor(random_state=42)
]

for model in models:
  model.fit(train_X, train_y)

```

- 主要思路有两个方面，一个是

#### 9.1 通过优化算法

- 核心是通过多次尝试加权组合，通过梯度不断调节，最终找到最佳组合

```python
from scipy.optimize import minimize
from sklearn.metrics import mean_squared_error

# 定义目标函数，用于优化权重
def objective(weights):
    weighted_preds = np.sum([w * model.predict(valid_X) for w, model in zip(weights, models)], axis=0)
    mse = mean_squared_error(valid_y, weighted_preds)
    return mse

# 初始权重猜测
initial_weights = np.ones(len(models)) / len(models)

# 约束条件：权重之和为 1 且非负
constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
bounds = [(0, 1) for _ in range(len(models))]

# 使用优化算法求解最优权重
result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)
optimal_weights = result.x

print("最优权重:", optimal_weights)
```

#### 9.2 通过线性回归

- 核心是通过用六个模型的预测作为输入，训练各个模型的权重
- 这个方法相比于 通过优化算法，对 **资源要求不高**，不需要多次迭代尝试，但是仅支持单个模型权重和拼接后模型得分是线性关系的情况，所以 **当存在非线性关系时，可能预测不准**

```python
from sklearn.linear_model import LinearRegression

# 获取六个模型在验证集上的预测结果
val_preds = np.array([model.predict(valid_X) for model in models]).T

# 训练线性回归模型来学习权重
linear_model = LinearRegression(positive=True)
linear_model.fit(val_preds, valid_y)

# 获取权重
weights = linear_model.coef_
weights = weights / np.sum(weights)  # 归一化权重

print("最优权重:", weights)
```

#### 9.3 通过 MLP 神经网络

- 核心和线性回归相似，只不过把模型替换成了神经网络
- 这个方法相比于 通过线性回归，**支持存在非线性关系的情况**，但是复杂度较高

```python
from sklearn.neural_network import MLPRegressor

# 获取六个模型在验证集上的预测结果
val_preds = np.array([model.predict(valid_X) for model in models]).T

# 训练神经网络来学习权重
meta_model = MLPRegressor(random_state=42)
meta_model.fit(val_preds, y_train)

# 最终预测
# final_preds = meta_model.predict(test_preds)
```
