## 1. 训练集，验证集，测试集

#### 1.1 概念

- 数据通常被分为三个部分：
  - 训练集（ Training Set，相当于上课学知识 ）：训练集用来训练模型，即确定模型的权重 w 和偏置 b 这些参数，通常称这些参数为学习参数
  - 验证集（ Validation Set，相当于课后的的练习题，用来纠正和强化学到的知识 ）：验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程。验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率、多项式回归的系数、正则化的 lambda 值等这些都叫超参数。比如在 KNN 算法中，k 值就是一个超参数。所以可以使用验证集来求出误差率最小的 k
  - 测试集（ Test Set，相当于期末考试，用来最终评估学习效果 ）：**测试集只使用一次**，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评估
- 验证集不像训练集和测试集，它是非必需的。如果多项式系数过高，容易过拟合，如果过低，容易欠拟合，可以通过验证集去比较，然后调整系数大小
- 数据划分常见规则
  - 对于小规模样本集（ 几万量级 ），常用的分配比例是 60% 训练集、20% 验证集、20% 测试集
  - 对于大规模样本集（ 百万级以上 ），只要验证集和测试集的数量足够即可，例如有 100w 条数据，那么留 1w 验证集，1w 测试集即可。1000w 的数据，同样留 1w 验证集和 1w 测试集

#### 1.2 实战

- 通常可以通过 sklearn.model_selection.train_test_split API 实现
  ```python
  sklearn.model_selection.train_test_split(
    *arrays,
    test_size=None,
    train_size=None,
    random_state=None,
    shuffle=True,
    stratify=None
  )
  ```
- 功能：将数据集一次性随机划分为训练集和测试集，如果还需要验证集，则可以对训练集进行再一次划分
- 参数：
  test_size：测试集所占比例，默认值为 0.25
  train_size：训练集大小，默认值为 None
  shuffle：是否打乱数据，默认是 True
  stratify：用于指定分层的标签，适用于分类任务，默认值为 None
  random_state：随机种子
- 适用场景：适用于快速验证模型，操作简单，但无法评估模型的稳定性

## 2. 分割器

- 基本上能在 [sklearn.model_selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) 中找到对应的评估方法 API
- API 通常格式如下
  ```python
  class sklearn.model_selection.XXX(
    n_splits=5,
    shuffle=False,
    random_state=None
  )
  ```
  - n_splits：表示要划分的折数，通常默认值为 5
  - shuffle：是一个布尔值，用于决定是否在划分前对数据进行打乱，通常默认值为 False
  - random_state：主要用于控制随机数生成，以便确保结果的可重复性

#### 2.1 KFold

- 功能：
  - 实现 K 折交叉验证，把数据集均分成 K 个互不重叠的子集，每次选取其中一个子集作为测试集，其余 K - 1 个子集作为训练集，这样能对模型进行 K 次训练和评估
- 适用场景：
  - 适用于数据分布比较均匀的普通数据集
  - 不过，它没有考虑数据的类别分布情况，所以在处理分类问题时可能不太合适

#### 2.2 StratifiedKFold

- 功能：
  - 这是一种分层 K 折交叉验证方法，**它能够保证每个折中的类别比例和原始数据的类别比例一致**，有效解决了类别不平衡问题
- 适用场景：
  - 特别适合处理分类任务，尤其是当数据存在类别不平衡的情况时，能确保每个折中各类别样本的比例与原始数据相同

#### 2.3 LeaveOneOut

- 功能：
  - 采用留一法交叉验证，每次将一个样本作为测试集，其余所有样本作为训练集
  - 这种方法会对模型进行 N 次训练和评估，其中 N 是样本的总数
- 适用场景：
  - 适用于小数据集。因为每次训练都要排除一个样本，所以计算成本非常高

#### 2.4 LeavePOut

- 功能：属于留 P 法交叉验证，每次会留下 P 个样本作为测试集，剩下的样本作为训练集。
- 特殊参数：
  - p：表示要保留的测试样本数量，默认值为 1，这个时候等同 LeaveOneOut
- 适用场景：当需要更多的测试样本时可以使用，但计算成本会随着 P 的增大而增加

#### 2.5 ShuffleSplit

- 功能：
  - 是一种随机划分交叉验证方法，按照指定的次数随机生成训练集和测试集
- 特殊参数：
  - n_splits：默认是 10
  - shuffle：默认是 True
  - test_size：可以是浮点数（ 表示测试集所占比例 ）或整数（ 表示测试集样本数量 ），默认值为 0.1
  - train_size：用于指定训练集的大小，默认值为 None
- 适用场景：
  - 适用于探索性分析

#### 2.6 StratifiedShuffleSplit

- 功能：结合了分层和随机划分的特点，每次划分时都能保证类别比例不变，并且具有随机性
- 特殊参数：
  - 和 ShuffleSplit 一致
- 适用场景：
  - 适用于分类任务，特别是在数据类别分布不均衡的情况下，能确保每个折中各类别样本的比例与原始数据一致

#### 2.7 TimeSeriesSplit

- 功能：
  - 专门用于时间序列数据的交叉验证，按照时间顺序进行划分，确保训练集在测试集之前
- 参数：
  - max_train_size：用于限制训练集的最大大小，默认值为 None
- 适用场景：
  - 适用于时间序列数据，能保证模型训练时不会使用到未来的数据

## 3. 交叉验证

- 在上面的分割器之上，封装了现成的交叉验证 API，也是通过 sklearn.model_selection 引入

#### 3.1 cross_val_score

```python
sklearn.model_selection.cross_val_score(
  estimator,
  X,
  y=None,
  groups=None,
  scoring=None,
  cv=None,
  n_jobs=None,
  verbose=0,
  params=None,
  pre_dispatch='2*n_jobs',
  error_score=nan
)
```

- 功能：
  - 自动化执行交叉验证流程，返回模型在每个折上的评估分数（ 如准确率、F1 值等 ），用于评估模型性能
- 参数
  - estimator：要评估的模型对象（ 如 SVC()、LinearRegression() ）
  - X：输入特征矩阵（ 形状：[n_samples, n_features] ）
  - y：目标变量（ 分类标签或回归值，形状：[n_samples] ）
  - cv：交叉验证策略
    - 整数：折数（ 如 5 ）
    - KFold/StratifiedKFold 对象
    - 自定义迭代器
  - scoring：评估指标（ 如 'accuracy'、'f1'、'roc_auc' ），支持自定义
  - n_jobs：并行任务数（ -1 表示使用所有 CPU ）
  - verbose：控制输出详细程度（ 值越大，信息越多 ）
  - fit_params：传递给模型 fit() 方法的参数（ 如 {'class_weight': 'balanced'} ）
  - pre_dispatch：控制并行任务的调度（ 如 '2\*n_jobs' ）
  - error_score：当验证出错时返回的默认值（ 如 'raise'、0.5' ）
- 适用场景：
  - 当需要快速评估模型的泛化能力，或比较不同 模型/参数 的表现时

#### 3.2 cross_val_predict

```python
sklearn.model_selection.cross_val_predict(
  estimator,
  X,
  y=None,
  groups=None,
  cv=None,
  n_jobs=None,
  verbose=0,
  params=None,
  pre_dispatch='2*n_jobs',
  method='predict'
)
```

- 功能：返回模型对每个样本生成其在交叉验证中的预测结果
- 参数
  - 和 cross_val_score 基本一致，不过少了 error_score，多了 method
  - method：预测方法
    - 'predict'：返回标签（ 默认 ）
    - 'predict_proba'：返回概率
- 适用场景：当需要基于交叉验证的预测结果进行后续分析（ 如混淆矩阵、误差分析 ）
- 简单解释下 **误差分析**
  - 假如模型想分类垃圾邮件和非垃圾邮件，在 500 个测试集中错误了 100 个，我们可以手动分析这 100 个错误分类数据的共通点
  - 比如可能 100 个错误分类中，有很多都是药品相关，可以在训练集中添加更多的药品相关训练数据
  - 有或者很多分类错误的测试数据中带有 url，则可以添加新的训练特征
  - 并且可以通过计数，排出不同种类数据出错频率，去判断优先解决什么问题
