## 1. 强化学习（ Reinforcement learning，RL ）

[参考资料](https://imzhanghao.com/2022/02/10/reinforcement-learning/#deep-q-network)

- 强化学习是除了监督学习和非监督学习之外的另一种基本的机器学习方法
- 强化学习讨论的问题是一个智能体（ agent ）怎么在一个复杂不确定的环境（ environment ）里面去极大化它能获得的奖励。通过感知所处环境的状态（ state ）对动作（ action ）的奖励（ reward ），来指导更好的动作，从而获得最大的收益（ return ），这被称为在交互中学习，这样的学习方法就被称作强化学习
- 比如说像控制直升机和其他机器人，很难获得一个理想动作的 x 和 y 的数据集，所以监督学习的效果在这场景效果不佳
- 强化学习特别的点在于，你需要告诉它该做什么而不是如何去做，并且指定奖励函数而不是最佳行为，具体到直升机上，只要它飞得好，那么可以给它飞得好的每秒都给加 1 的奖励，如果飞行不佳，就给他负奖励，如果它崩溃了，就给它非常大的负奖励
- 强化学习主要有以下几个特性：
  - 试错学习：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略（ Policy ）
  - 延迟回报：强化学习的指导信息很少，而且往往是在事后（ 最后一个状态 ）才给出的。比如 围棋中只有到了最后才能知道胜负
- 经典算法有 Q-Learning（ QL ），Deep Q Network（ DQN ） 等

## 2. 举例场景

- 比如火星探测器场景
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/74962ae7-e32f-447d-ba09-6d20ce9f9d30)
  - 不同的位置代表不同的 state 值，可以用 (s, a, R(s), s') 代表每次行为，分别表示，当前状态，选择的行为，操作后得到的奖励，操作后新的状态
- 强化学习的的收益是系统获得的奖励的总和，由折扣因子（ discount factor ）加权得到（ 需要更多操作才能到达的状态的奖励，会乘上更高幂的折扣因子 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b16b1d07-801f-46d9-933c-c83f098db3e7)
  - 折扣因子通常接近 1，比如 0.9，0.99，0.999 等
- 算法需要找到一个公式 π(s) = a，对于当前状态 s，找到适合的行为 a

## 3. Q Learing（ QL ）

- Q-Learning 是一种基于值的强化学习算法，它使用动作价值函数 Q(s, a) 来估计在给定状态 s 下采取动作 a 的期望累计回报。Q-Learning 使用**贪婪策略**进行更新，即在更新过程中总是选择最大的 Q 值
- 假设机器人必须越过迷宫并到达终点，机器人一次只能移动一个地砖。机器人必须在尽可能短的时间内到达终点。得分/奖励系统如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/015fea07-0a51-430d-86dc-b5434e5c870c)
  - 机器人在每一步都失去 1 点（ 这样做是为了使机器人采用最短路径并尽可能快地到达目标 ）
  - 如果机器人踩到地雷，则点损失为 100 并且游戏结束
  - 如果机器人获得动力 ⚡️，它会获得 1 点
  - 如果机器人达到最终目标，则机器人获得 100 分

#### 3.1 Q 值表

- 在 Q-learning 中，我们维护一张 Q 值表，表的维数为：状态数 S \* 动作数 A，表中每个数代表在当前状态 S 下可以采用动作 A 可以获得的未来收益的折现和。我们不断的迭代我们的 Q 值表使其最终收敛，然后根据 Q 值表我们就可以在每个状态下选取一个最优策略
- Q 值表（ Q-Table ）是一个简单查找表的名称，我们计算每个状态的最大预期未来奖励。基本上，这张表将指导我们在每个状态采取最佳行动
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/deebf677-9fe9-451b-afad-47f85340c64b)

#### 3.2 Q 函数

- 可以通过贝克曼方程去得到 Q 函数，本质就是动态规划，选取下一个状态的最大 Q 值乘以折扣因子再加上当前状态奖励
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/149c7202-f331-463f-810a-4ee95ca3de90)
- 可以把 Q 函数视为一个在 Q-Table 上滚动的读取器，用于寻找与当前状态关联的行以及与动作关联的列。它会从相匹配的单元格中返回 Q 值。这就是未来奖励的期望

#### 3.3 算法流程

- 初始化 Q 值表
  - 我们将首先构建一个 Q 值表。有 n 列，其中 n = 操作数，有 m 行，其中 m = 状态数。我们将值初始化为 0
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/7199ae77-00b0-4291-8911-7f286f25fe02)
- 选择并执行操作
  - 这些步骤的组合在不确定的时间内完成。这意味着此步骤一直运行，直到我们停止训练，或者训练循环停止
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/69308d6a-72d8-47f3-874b-006c9b8184a8)
  - 如果每个 Q 值都等于零，我们就需要权衡探索/利用（ exploration/exploitation ）的程度了，思路就是，在一开始，我们将使用 epsilon 贪婪策略：
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/01a8b322-9640-4023-a2ff-87f253424f2e)
    - 指定一个探索速率 epsilon，一开始将它设定为 1。这个就是我们将随机采用的步长。在一开始，这个速率应该处于最大值，因为我们不知道 Q-table 中任何的值。这意味着，我们需要通过随机选择动作进行大量的探索
    - 生成一个随机数。如果这个数大于 epsilon，那么我们将会进行“利用”（ 这意味着在每一步利用已经知道的信息选择动作 ）。否则，我们将继续进行探索
    - 在刚开始训练 Q 函数时，我们必须有一个大的 epsilon。随着智能体对估算出的 Q 值更有把握，我们将逐渐减小 epsilon
- 评估
  - 采取了行动并观察了结果和奖励。我们需要更新功能 Q(s，a)
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/63f5c098-8298-4199-9755-0c5e57716152)
  - 最后生成的 Q 表：
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/757874c0-4e3c-4ad0-b031-39562c38e3ad)

## 4，Deep Q Network（ DQN ）

- 在普通的 Q-learning 中，当状态和动作空间是离散且维数不高时可使用 Q-Table 储存每个状态动作对的 Q 值，而当状态和动作空间是高维连续时，使用 Q-Table 不现实，我们无法构建可以存储超大状态空间的 Q-Table
- 不过，在机器学习中，神经网络对这种事情很在行，可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样就没必要在表格中记录 Q 值，而是直接使用神经网络预测 Q 值
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/03364d9d-413f-4347-9535-c1bda4b1742c)
- 经验回放
  - DQN 利用 Qlearning 特点，目标策略与动作策略分离，学习时利用经验池储存的经验取 batch 更新 Q。同时提高了样本的利用率，也打乱了样本状态相关性使其符合神经网络的使用特点
- 固定 Q 目标
  - 神经网络一般学习的是固定的目标，而 Qlearning 中 Q 同样为学习的变化量，变动太大不利于学习。所以 DQN 使 Q 在一段时间内保持不变，使神经网络更易于学习
- 算法流程
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/005d1a7d-9922-444b-aadc-d00101205704)
- 主要问题
  - 在估计值函数的时候一个任意小的变化可能导致对应动作被选择或者不被选择，这种不连续的变化是致使基于值函数的方法无法得到收敛保证的重要因素
  - 选择最大的 Q 值这样一个搜索过程在高纬度或者连续空间是非常困难的
  - 无法学习到随机策略，有些情况下随机策略往往是最优策略
