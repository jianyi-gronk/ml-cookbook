## 1. Encoder-Decoder

- Encoder-Decoder 模型主要是 NLP 领域里的概念。它并不特值某种具体的算法，只要是用一个编码结构一个解码结构的模型都是 Encoder-Decoder 架构，在这个框架下可以使用不同的算法来解决不同的任务
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/8792e4a9-852f-42e7-b784-c3935ddda8b8)
  - 不论输入和输出的长度是什么，中间的向量 c 长度都是固定的（ 这也是它的缺陷 ）
  - 根据不同的任务可以选择不同的编码器和解码器（ 可以是 RNN ，但通常是其变种 LSTM 或者 GRU ）
  - 只要是符合上面的框架，都可以统称为 Encoder-Decoder 模型

## 2. Seq2seq

- Seq2seq（ 是 Sequence-to-sequence 的缩写 ），就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/6c283b53-2018-4d72-b7b2-b8626538d9ce)
- Encoder 将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列，每一个绿色或者紫色方块都可以用 RNN，GRU，LSTM 等结构
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/84170574-92c0-4ace-9b22-0c5a3d91cc5f)
- 最主要的用途是机器翻译
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2f2c43a4-7512-499e-a8de-8dc8ff3b3982)

## 3. Attention（ 注意力机制 ）

- 在 Encoder-Decoder 结构中，Encoder 把所有的输入序列都编码成了一个统一的语义特征 c 再输入到 Decoder 中，那么 c 中就必须包含原始序列中的所有信息，因此它的长度就成了限制模型性能的瓶颈。c 的长度是固定的，所以当翻译语句过长时，一个 c 可能存不下那么多信息，就会造成精度的下降
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b981bf2e-10ff-4191-841b-486e36ee109b)
- 增加 Encoder 信息输入到 Decoder 中对应时刻的联系，并减弱其他时刻的联系
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/707eb470-fbb8-4e41-bdf6-1abf7473d15c)
  - 可以看到 C 与 Encoder 对应时刻的联系加粗，与 Encoder 其他时刻的联系变细

## 4. Transformer

- [参考文献 1](https://juejin.cn/post/6953818601350496287)，[参考文献 2](https://www.cnblogs.com/mantch/p/11591937.html)
- 传统的 RNN 存在梯度消失和梯度爆炸的问题。LSTM 和 GRU 都缓解了梯度消失和梯度爆炸的问题，但是没有彻底解决该问题。因此，为了让模型的能力进一步提升，在论文《Attention is All You Need》中提出了 Transformer 模型，该模型利用 Attention 机制可以完全代替 RNN、CNN 等系列模型
- Transformer 的结构和 Attention 模型一样，Transformer 模型中也采用了 encoer-decoder 架构。但其结构相比于 Attention 更加复杂，论文中 encoder 层由 6 个 encoder 堆叠在一起，decoder 层也一样
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d61184ef-f477-490c-8ea5-966a856c291e)
- 每一个 Encoder 和 Decoder 的内部结构如下图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/df7a175d-ead5-4054-8330-736ecc5ffdff)
  - encoder 包含两层，一个 self-attention 层和一个前馈神经网络，self-attention 能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义
  - decoder 也包含 encoder 提到的两层网络，但是在这两层中间还有一层 attention 层，帮助当前节点获取到当前需要关注的重点内容

#### 4.1 Transformer Encoder 的结构

- 首先，模型需要对输入的数据进行一个 embedding 操作，也可以理解为类似 word2vec 的操作，embedding 结束之后，输入到 Encoder 层，Self-attention 处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个 Encoder
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0a4cb225-8070-4124-95be-b762ac28e213)

#### 4.2 Positional Encoding

- Transformer 模型中缺少一种解释输入序列中单词顺序的方法，它跟序列模型还不一样。为了处理这个问题，Transformer 给 Encoder 层和 Decoder 层的输入添加了一个额外的向量 Positional Encoding，维度和 embedding 的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/63cd2bed-a722-43a6-baf3-f4d53927a34f)
  - 其中 pos 是指当前词在句子中的位置， 是指向量中每个值的 index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码
- 最后把这个 Positional Encoding 与 embedding 的值相加，作为输入送到下一层
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ce79a60a-e969-4009-a3da-1fee3aa3c2b7)

#### 4.3 Self-Attention

- 例如，下列句子是我们想要翻译的输入句子：
  > The animal didn't cross the street because it was too tired
- 这个 it 在这个句子是指什么呢？它指的是 street 还是这个 animal 呢？当模型处理这个单词 it 的时候，自注意力机制会允许 it 与 animal 建立联系
- 随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。例如 RNN 会将它已经处理过的前面的所有单词（ 向量 ）的表示与它正在处理的当前单词（ 向量 ）结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/bed9acdb-3184-4999-99eb-6d0276f9de9a)
  - 如上图所示，当我们在编码器中编码 it 这个单词的时，注意力机制的部分会去关注 The Animal，将它的表示的一部分编入 it 的编码中

#### 4.4 残差与层归一化

- 编码器架构中的细节：在每个编码器中的每个子层（ 自注意力、前馈网络 ）的周围都有一个残差连接，并且都跟随着一个 Layer Normalization（ 层归一化 ）步骤
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/276111d6-ef68-4500-b998-0a9c79b0fdd9)
- 如果去可视化这些向量以及这个和自注意力相关联的 layer-norm 操作，那么看起来就像下面这张图描述一样：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/20533e23-b62f-4a38-8103-edec8f9f0ed0)
- 解码器的子层也是这样样的。如果我们想象一个 2 层编码器-解码器结构的 Transformer，它看起来会像下面这张图一样
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/557efcac-92ad-4e42-9470-08d10ab0e3e7)

#### 4.5 Transformer Decoder 的结构

- 根据总体结构图可以看出，Decoder 部分其实和 Encoder 部分大同小异，刚开始也是先添加一个位置向量 Positional Encoding，接下来接的是 masked mutil-head attetion，这里的 mask 也是 transformer 一个很关键的技术，其余的层结构与 Encoder 一样

#### 4.6 masked mutil-head attetion

- mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。 Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 Decoder 的 self-attention 里面用到
- padding mask
  - 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理
  - 具体的做法是，把这些位置的值加上一个非常大的负数（ 负无穷 ），这样的话，经过 softmax，这些位置的概率就会接近 0
  - 而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方
- sequence mask
  - sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来
  - 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为 0。把这个矩阵作用在每一个序列上，就可以达到我们的目的
    - 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要 padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个 mask 相加作为 attn_mask
    - 其他情况，attn_mask 一律等于 padding mask

#### 4.7 最终的线性变换和 Softmax 层

- 当 Decoder 层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和 softmax 层，假如我们的词典是 1 万个词，那最终 softmax 会输出 1 万个词的概率，概率值最大的对应的词就是我们最终的结果
- 底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/46fb3af8-9559-428d-bc7c-8412a8da2fe2)

#### 4.8 Transformer 动态流程图

- 编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量 K（ 键向量 ）和 V（ 值向量 ）的注意力向量集 ，这是并行化操作。 这些向量将被每个解码器用于自身的 编码-解码注意力层 ，而这些层可以帮助解码器关注输入序列哪些位置合适：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b23d748e-3d72-4e4f-aa6b-c3448a742875)
- 在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列的元素（ 在这个例子里，是英语翻译的句子 ）
- 接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示 transformer 的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像对编码器的输入所做的那样，会嵌入并添加位置编码给那些解码器，来表示每个单词的位置
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/955a34a3-9a9d-4f90-985b-8cc84513619c)
- 而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在 softmax 步骤前，它会把后面的位置给隐去（ 把它们设为-inf ）
- 这个 编码器-解码器注意力层 工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造 Queries 矩阵，并且从编码器的输出中取得 Keys、Values 矩阵

## 5. Transformer 和 Seq2seq 的主要区别

- 结构：Seq2seq 使用循环神经网络（ 如 LSTM 或 GRU ）来建模序列信息，其中编码器和解码器是逐步进行的。而 Transformer 则使用自注意力机制，允许并行计算，没有显式的时间顺序
- 长距离依赖：由于自注意力机制的存在，Transformer 更擅长处理长距离依赖关系，而 Seq2seq 的 RNN 结构在处理长序列时可能面临梯度消失或梯度爆炸的问题
- 效率：由于并行计算和自注意力机制的使用，Transformer 在某些情况下可以更高效地处理长序列，同时减少了训练时间
- 位置编码：Transformer 引入了位置编码，以便模型能够理解输入序列中的单词位置信息。而 Seq2seq 使用循环神经网络，本身就天然具备处理序列的位置信息
- 应用场景：Seq2seq 主要应用于需要处理变长序列的任务，如机器翻译、对话生成等。而 Transformer 除了这些任务，还广泛应用于语言建模、文本分类、命名实体识别等 NLP 任务，并在计算机视觉领域也得到了应用
