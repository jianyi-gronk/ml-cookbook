## 1. 自注意力机制（ Self-Attention ）

- 基于注意力机制的 Seq2Seq 模型依赖 RNN 及其变体，需串行计算，效率低，且存在梯度消失/爆炸问题
- 2017 年论文《Attention is All You Need》提出了 Transformer 模型，完全基于注意力机制（ 自注意力和跨注意力 ），取代 RNN/CNN，解决了并行处理和长距离依赖问题
- 自注意力机制在 Transformer 的 **编码器 和 解码器** 中都有使用，但形式不同
  - 编码器使用标准自注意力
  - 解码器使用掩码自注意力（ 防止关注未来 Token ）

#### 1.1 基础介绍

- 自注意力机制是一种特殊形式的注意力机制
  - 不依赖于外部序列（ 如 编码器-解码器 间的交互 ），而是 **序列内部的自我交互**
  - 序列的每个元素（ Token ）同时作为查询（ Query ）、键（ Key ）和值（ Value ）的来源
  - 关注序列内的所有元素（ 包括自身 ）的相关性，生成上下文相关的表示，捕捉序列内部的依赖关系，允许元素根据与其他元素的关系动态调整自身的表示
- 特点
  - 自注意力关注 **同一序列内部** 的 token 间关系
  - 所有 token 同时参与计算，允许 **并行化处理**，无需像 RNN 那样按顺序计算
  - 能捕捉 **长距离依赖**，因为每个 token 都可以直接访问序列中的任意其他 token

#### 1.2 核心概念

- Token
  - Token 是 **序列中的基本处理单元**，序列由多个 token 组成，表示为
    $$\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_{T_x}]$$
    - 在 NLP 中，token 通常是一个词、子词 subword 或字符
    - 在 计算机视觉 中，token 可以是图像块 patch 或其他特征单元
  - Token 通常通过嵌入层转换为 **固定维度** 的向量，并可能附加 **位置编码** 以捕捉序列顺序
  - 自注意力机制通过计算 token 之间的关系，生成每个 token 的新表示，融合序列中其他 token 的上下文信息
- Query（ 查询向量 ）
  - Query 是自注意力机制中用于 **表示某个 token “询问” 其他 token 的表示向量**
  - 可以将 $\mathbf{q}_i$ 看作 token $i$ 的 “搜索请求”，询问序列中哪些 token 与它当前的任务或语义最相关
  - 例如，在翻译任务中，token “I” 的查询向量 $\mathbf{q}_1$ 可能更关注与主语相关的 token（ 如 “love” ）
- Key（ 键向量 ）
  - Key 是自注意力机制中用于 **表示序列中每个 token 的 “标识” 或 “特征” 的向量**
  - 可以将 $\mathbf{k}_j$ 看作 token $j$ 的 “标签” 或 “索引”，用于匹配查询向量 $\mathbf{q}_i$ 的需求
  - 如果 $\mathbf{q}_i$ 和 $\mathbf{k}_j$ 的点积值较大，说明 token $i$ 和 token $j$ 的语义或功能高度相关
- Value（ 值向量 ）
  - Value 是自注意力机制中 **表示 token 的实际内容的向量**
  - 可以将 $\mathbf{v}_j$ 看作 token $j$ 的 “内容” 或 “信息包”，在注意力机制中根据权重 $\alpha_{i,j}$ 决定贡献多少信息给 token $i$ 的新表示，如果 $\alpha_{i,j}$ 较大，说明 token $j$ 的内容对 token $i$ 的表示很重要
- Query，Key，Value 都是通过对其嵌入向量 $\mathbf{x}_j$ 进行线性变换得到的
  $$
  \mathbf{q}_i = \mathbf{W}_Q \mathbf{x}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{x}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{x}_i
  $$
  - $\mathbf{W}_Q$，$\mathbf{W}_K$，$\mathbf{W}_V$ 都是可学习的权重矩阵

#### 1.3 计算过程

- 计算注意力分数
  $$
  e_{i,j} = \mathbf{q}_i \cdot \mathbf{k}_j
  $$
- 归一化得到注意力权重
  $$
  \alpha_{i,j} = \text{Softmax}\left(\frac{e_{i,j}}{\sqrt{d_k}}\right)
  $$
- 生成新表示
  $$
  \mathbf{z}_i = \sum_{j=1}^n \alpha_{i,j} \mathbf{v}_j
  $$
  - 每个输入 $\mathbf{x}_i$ 都会生成一个对应的上下文表示 $\mathbf{z}_i$
- 自注意力机制输出 $\mathbf{Z}$（ 缩放点积注意力 ）
  $$
  \mathbf{Z} = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}
  $$
  - 其中 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 是查询、键、值矩阵，$d_k$ 是键向量的维度
- Transformer 通常使用 **多头自注意力**，并行计算多个注意力头，拼接后线性变换
  $$
  \text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i), \quad \mathbf{Z} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) \mathbf{W}_O
  $$

## 2. 掩码自注意力（ Masked Self-Attention ）

#### 2.1 基础介绍

- 是自注意力机制的一种变体，主要用于 Transformer 解码器，以符合自回归生成的需求
  - 在解码器中，掩码自注意力用于处理目标序列（ 如翻译任务中的目标语言序列 ），确保生成 token 时只依赖当前和之前的上下文信息，模拟自回归生成过程
  - 例如，在翻译 “I love learning” 到 “我爱学习” 时，生成 “我” 时只关注 “我”，生成 “爱” 时关注 “我” 和 “爱”，以此类推
- 两者的具体区别如下
  - 在标准自注意力中，每个 token 可以关注序列中的所有 token（ 包括未来 token ）
  - 在掩码自注意力中，通过引入一个 **掩码矩阵**，防止当前 token 关注序列中的未来 token，确保每个 token 仅关注当前和之前的 token

#### 2.2 计算过程

- 输入目标序列 $\mathbf{Y} = [\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_{T_y}]$，$T_y$ 是目标序列长度
- 生成查询、键、值向量
  $$
  \mathbf{q}_i = \mathbf{W}_Q \mathbf{y}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{y}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{y}_i
  $$
- 计算注意力分数
  $$
  e_{i,j} = \mathbf{q}_i \cdot \mathbf{k}_j
  $$
- 应用 掩码矩阵 $\mathbf{M}$
  - $\mathbf{T_y} \in \mathbb{R}^{T_y \times T_y}$ 是一个下三角矩阵，$M_{i,j} = 0$（ 若 $j \leq i$，允许关注 ），$M_{i,j} = -\infty$（ 若 $j > i$，屏蔽未来 token ）
  - 在计算注意力分数后，添加掩码
    $$
    e_{i,j}' = e_{i,j} + M_{i,j}
    $$
  - 归一化得到注意力权重
    $$
    \alpha_{i,j} = \text{Softmax}\left(\frac{e_{i,j}'}{\sqrt{d_k}}\right)
    $$
    - 当 $j > i$ 时，$e_{i,j}' = -\infty$，Softmax 后 $\alpha_{i,j} = 0$，确保不关注未来 token。
- 生成新表示
  $$
  \mathbf{z}_i = \sum_{j=1}^i \alpha_{i,j} \mathbf{v}_j
  $$
  - 注意：仅对 $j \leq i$ 的 token 求和，忽略未来 token
- 矩阵形式
  $$
  \mathbf{Z} = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T + \mathbf{M}}{\sqrt{d_k}}\right) \mathbf{V}
  $$
  - $\mathbf{Z} \in \mathbb{R}^{T_y \times d}$，每行 $\mathbf{z}_i$ 是 $\mathbf{y}_i$ 的上下文表示，仅基于 $\mathbf{y}_1, \dots, \mathbf{y}_i$
- 与标准自注意力类似，解码器使用多头掩码自注意力：
  $$
  \text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i, \mathbf{M}), \quad \mathbf{Z} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) \mathbf{W}_O
  $$
  - 每个注意力头独立应用掩码矩阵 $\mathbf{M}$

## 3. Transformer

- [参考文献 1](https://juejin.cn/post/6953818601350496287)，[参考文献 2](https://www.cnblogs.com/mantch/p/11591937.html)
- Transformer 的结构和 Attention 模型一样，Transformer 模型中也采用了 encoer-decoder 架构。但其结构相比于 Attention 更加复杂，论文中 encoder 层由 6 个 encoder 堆叠在一起，decoder 层也一样
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d61184ef-f477-490c-8ea5-966a856c291e)
- 每一个 Encoder 和 Decoder 的内部结构如下图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/df7a175d-ead5-4054-8330-736ecc5ffdff)
  - encoder 包含两层，一个 self-attention 层和一个前馈神经网络，self-attention 能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义
  - decoder 也包含 encoder 提到的两层网络，但是在这两层中间还有一层 attention 层，帮助当前节点获取到当前需要关注的重点内容

#### 3.1 Transformer Encoder 的结构

- 首先，模型需要对输入的数据进行一个 embedding 操作，也可以理解为类似 word2vec 的操作，embedding 结束之后，输入到 Encoder 层，Self-attention 处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个 Encoder
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0a4cb225-8070-4124-95be-b762ac28e213)

#### 3.2 Positional Encoding

- Transformer 模型中缺少一种解释输入序列中单词顺序的方法，它跟序列模型还不一样。为了处理这个问题，Transformer 给 Encoder 层和 Decoder 层的输入添加了一个额外的向量 Positional Encoding，维度和 embedding 的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/63cd2bed-a722-43a6-baf3-f4d53927a34f)
  - 其中 pos 是指当前词在句子中的位置， 是指向量中每个值的 index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码
- 最后把这个 Positional Encoding 与 embedding 的值相加，作为输入送到下一层
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ce79a60a-e969-4009-a3da-1fee3aa3c2b7)

#### 3.3 Self-Attention

- 例如，下列句子是我们想要翻译的输入句子：
  > The animal didn't cross the street because it was too tired
- 这个 it 在这个句子是指什么呢？它指的是 street 还是这个 animal 呢？当模型处理这个单词 it 的时候，自注意力机制会允许 it 与 animal 建立联系
- 随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。例如 RNN 会将它已经处理过的前面的所有单词（ 向量 ）的表示与它正在处理的当前单词（ 向量 ）结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/bed9acdb-3184-4999-99eb-6d0276f9de9a)
  - 如上图所示，当我们在编码器中编码 it 这个单词的时，注意力机制的部分会去关注 The Animal，将它的表示的一部分编入 it 的编码中

#### 3.4 残差与层归一化

- 编码器架构中的细节：在每个编码器中的每个子层（ 自注意力、前馈网络 ）的周围都有一个残差连接，并且都跟随着一个 Layer Normalization（ 层归一化 ）步骤
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/276111d6-ef68-4500-b998-0a9c79b0fdd9)
- 如果去可视化这些向量以及这个和自注意力相关联的 layer-norm 操作，那么看起来就像下面这张图描述一样：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/20533e23-b62f-4a38-8103-edec8f9f0ed0)
- 解码器的子层也是这样样的。如果我们想象一个 2 层编码器-解码器结构的 Transformer，它看起来会像下面这张图一样
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/557efcac-92ad-4e42-9470-08d10ab0e3e7)

#### 3.5 Transformer Decoder 的结构

- 根据总体结构图可以看出，Decoder 部分其实和 Encoder 部分大同小异，刚开始也是先添加一个位置向量 Positional Encoding，接下来接的是 masked mutil-head attetion，这里的 mask 也是 transformer 一个很关键的技术，其余的层结构与 Encoder 一样

#### 3.6 masked mutil-head attetion

- mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。 Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 Decoder 的 self-attention 里面用到
- padding mask
  - 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理
  - 具体的做法是，把这些位置的值加上一个非常大的负数（ 负无穷 ），这样的话，经过 softmax，这些位置的概率就会接近 0
  - 而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方
- sequence mask
  - sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来
  - 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为 0。把这个矩阵作用在每一个序列上，就可以达到我们的目的
    - 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要 padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个 mask 相加作为 attn_mask
    - 其他情况，attn_mask 一律等于 padding mask

#### 3.7 最终的线性变换和 Softmax 层

- 当 Decoder 层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和 softmax 层，假如我们的词典是 1 万个词，那最终 softmax 会输出 1 万个词的概率，概率值最大的对应的词就是我们最终的结果
- 底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/46fb3af8-9559-428d-bc7c-8412a8da2fe2)

#### 3.8 Transformer 动态流程图

- 编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量 K（ 键向量 ）和 V（ 值向量 ）的注意力向量集 ，这是并行化操作。 这些向量将被每个解码器用于自身的 编码-解码注意力层 ，而这些层可以帮助解码器关注输入序列哪些位置合适：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b23d748e-3d72-4e4f-aa6b-c3448a742875)
- 在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列的元素（ 在这个例子里，是英语翻译的句子 ）
- 接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示 transformer 的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像对编码器的输入所做的那样，会嵌入并添加位置编码给那些解码器，来表示每个单词的位置
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/955a34a3-9a9d-4f90-985b-8cc84513619c)
- 而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在 softmax 步骤前，它会把后面的位置给隐去（ 把它们设为-inf ）
- 这个 编码器-解码器注意力层 工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造 Queries 矩阵，并且从编码器的输出中取得 Keys、Values 矩阵

## 4. Transformer 和 Seq2seq 的主要区别

- 结构
  - Seq2seq 使用循环神经网络（ 如 LSTM 或 GRU ）来建模序列信息，其中编码器和解码器是逐步进行的
  - 而 Transformer 则使用自注意力机制，允许并行计算，没有显式的时间顺序
- 长距离依赖
  - Seq2seq 的 RNN 结构在处理长序列时可能面临梯度消失或梯度爆炸的问题
  - 而由于自注意力机制的存在，Transformer 更擅长处理长距离依赖关系
- 效率
  - 由于并行计算和自注意力机制的使用，Transformer 在某些情况下可以更高效地处理长序列，同时减少了训练时间
- 位置编码
  - Seq2seq 使用循环神经网络，本身就天然具备处理序列的位置信息
  - Transformer 引入了位置编码，以便模型能够理解输入序列中的单词位置信息
- 应用场景
  - Seq2seq 主要应用于需要处理变长序列的任务，如机器翻译、对话生成等
  - 而 Transformer 除了这些任务，还广泛应用于语言建模、文本分类、命名实体识别等 NLP 任务，并在计算机视觉领域也得到了应用
