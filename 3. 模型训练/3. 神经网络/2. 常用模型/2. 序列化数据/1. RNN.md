## 1. RNN（ Recurrent Neural Network，循环神经网络 ）

- **RNN 主要用于处理序列数据，如文本、语音、时间序列等，与传统的前馈神经网络不同，RNN 具有循环连接，使得它可以在处理序列时保持一种记忆状态**

#### 1.1 循环单元

- RNN 的基本结构是循环单元，核心概念有 隐藏状态，输入 和 输出
- 隐藏状态
  - $\mathbf{a}^{\langle t \rangle}$ 是网络的 **记忆载体**，编码了从序列起点到当前时刻 $t$ 的全部历史信息，不过由于长序列中梯度消失问题，所以可能无法有效保留远期信息
  - 示例：处理句子 `"I love machine learning"` 时，`"learning"` 对应的 $\mathbf{a}^{\langle 4 \rangle}$ 隐含 `"I"`、`"love"`、`"machine"` 的语义，但长期依赖可能因梯度问题丢失
- 输入
  - 当前时刻 输入 $\mathbf{x}^{\langle t \rangle}$，如 第 t 个单词的向量
  - 上一时刻 隐藏状态 $\mathbf{a}^{\langle t-1 \rangle}$，承载历史信息
- 输出
  - 当前时刻 输出 $\mathbf{y}^{\langle t \rangle}$，如 分类概率
  - 当前时刻 隐藏状态 $\mathbf{a}^{\langle t \rangle}$，传递至下一时刻

#### 1.2 具体公式

- 数学表达
  $$
  \begin{align*}
  \mathbf{a}^{\langle t \rangle} &= f(\mathbf{W}_{aa} \mathbf{a}^{\langle t-1 \rangle} + \mathbf{W}_{ax} \mathbf{x}^{\langle t \rangle} + \mathbf{b}_a) \\
  \mathbf{y}^{\langle t \rangle} &= g(\mathbf{W}_{ya} \mathbf{a}^{\langle t \rangle} + \mathbf{b}_y)
  \end{align*}
  $$
  - f：隐藏层激活函数（ 常用 tanh 或 ReLU ）
  - g：输出层激活函数，如 Softmax 用于分类
  - $\mathbf{W}_{aa}, \mathbf{W}_{ax}, \mathbf{W}_{ya}$：权重矩阵，在所有时间步中共享
  - $\mathbf{b}_a, \mathbf{b}_y$：偏置向量

#### 1.3 参数共享

- 参数共享 是 RNN 的核心特性，指的是 **在处理序列数据时，所有时间步中始终使用同一组 $\mathbf{W}{aa}$、$\mathbf{W}{ax}$、$\mathbf{W}_{ya}$、$\mathbf{b}_a$、$\mathbf{b}_y$ 参数**，而不是为每个时间步单独定义一组参数。这大幅减少参数量，并支持处理变长序列
- 参数共享 **会假设序列中的每个时间步遵循相似的模式或规律**，即序列的动态变化遵循相似的规则，所以可以使用相同的参数，但若序列前后模式差异较大，则共享参数可能限制模型建模能力
- 关键作用
  - **减少参数量**
    - 参数共享让 RNN 只需一组固定的权重矩阵，无论序列长度是 10、100 还是 1000，参数数量保持不变，这大幅降低了模型的复杂度和存储需求
  - **支持变长序列**
    - 参数共享使 RNN 能处理任意长度的序列，因为权重矩阵是固定的，模型的计算逻辑不依赖于序列的具体长度
    - 例如，处理 "I love"（ 2 个单词 ）或 "I love machine learning"（ 4 个单词 ）时，RNN 使用相同的权重矩阵，只需根据序列长度调整时间步的循环次数
  - **捕捉序列规律**
    - 参数共享使用相同的权重矩阵，学习通用的模式
    - 例如，在自然语言处理中，句子的语义规律（ 如语法、上下文依赖 ）在不同时间步中是相似的

#### 1.4 存在问题

- 梯度消失 或 梯度爆炸
  - 长序列中梯度可能指数级衰减或增长，导致无法学习长期依赖，原因是共享的权重矩阵在反向传播中反复使用，梯度累积可能导致衰减或爆炸
  - 解决方案
    - 门控机制：LSTM 和 GRU 通过门控（ 如遗忘门、输入门 ）选择性地保留或丢弃历史信息，缓解梯度问题
    - 梯度裁剪：限制梯度最大值，防止爆炸
    - 残差连接：在 RNN 变体中（ 如深度 RNN ），添加跨时间步的残差连接，允许信息直接传递，类似 Skip Connections 的思路
- 计算效率
  - 由于状态依赖前一步，所以无法并行处理序列，计算效率较低
  - 解决方案
    - Transformer 架构，即基于自注意力机制，允许所有时间步并行计算，因为自注意力不依赖时间步的顺序，而是通过注意力权重动态建模序列关系
