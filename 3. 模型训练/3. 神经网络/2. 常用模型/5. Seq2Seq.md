## 1. Encoder-Decoder

- Encoder-Decoder 模型主要是 NLP 领域里的概念，广泛用于序列到序列任务
- 它并不特值某种具体的算法，只要是符合下面的框架，都可以统称为 Encoder-Decoder 模型

  ```mermaid
  graph TD
    %% 输入序列
    X1[ x₁ ] -->|输入| Encoder
    X2[ x₂ ] -->|输入| Encoder
    X3[ x₃ ] -->|输入| Encoder
    X4[ x₄ ] -->|输入| Encoder

    %% 编码器和解码器
    Encoder[Encoder] -->|编码| Context
    Context[向量 C] -->|解码| Decoder
    Decoder[Decoder] -->|输出| Y1
    Decoder -->|输出| Y2
    Decoder -->|输出| Y3

    %% 输出序列
    Y1[ y₁ ]
    Y2[ y₂ ]
    Y3[ y₃ ]
  ```

  - 不论输入和输出的长度是什么，**中间的向量 c 长度都是固定的**（ 这也是它的缺陷 ）
  - **架构是 一个编码结构 和 一个解码结构**，根据不同的任务可以选择不同的编码器和解码器，可以是 RNN ，但通常是其变种 LSTM 或者 GRU

## 2. Seq2seq

#### 2.1 基础概念

- Seq2seq 是 Sequence-to-sequence 的缩写，如字面意思，输入一个序列，输出另一个序列，这种结构最重要的地方在于 **输入序列和输出序列的长度是可变的**
- Encoder 将一个 可变长度的信号序列 变为 固定长度的向量表达，Decoder 将这个 固定长度的向量 变成 可变长度的目标的信号序列，每一个绿色或者紫色方块都可以用 RNN，GRU，LSTM 等结构
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/84170574-92c0-4ace-9b22-0c5a3d91cc5f)
- 最主要的用途是机器翻译，输入源语言句子，输出目标语言句子，其他应用比如 文本摘要、对话系统、语音识别
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2f2c43a4-7512-499e-a8de-8dc8ff3b3982)

#### 2.2 架构组成

- Seq2Seq 模型由以下两个主要部分 编码器 和 解码器 组成
- 编码器（ Encoder ）
  - 功能
    - 将输入序列 $\mathbf{x} = (x_1, x_2, \dots, x_{T_x})$ 编码为固定长度的上下文向量 $\mathbf{c}$。
  - 实现
    - 通常使用 RNN，或其变体 LSTM 和 GRU，逐时间步处理输入序列
    - 在每个时间步 $t$，编码器更新隐藏状态
      $$
      \mathbf{h}_t = \text{RNN}(\mathbf{x}_t, \mathbf{h}_{t-1})
      $$
    - 最后一个时间步的隐藏状态 $\mathbf{h}_{T_x}$ 理论上包含了输入序列的语义和上下文，因此通常作为上下文向量 $\mathbf{c}$
      $$
      \mathbf{c} = \mathbf{h}_{T_x}
      $$
  - 特点
    - 上下文向量 $\mathbf{c}$ 包含输入序列的全部信息，传递给解码器
- 解码器（ Decoder ）
  - 功能
    - 基于上下文向量 $\mathbf{c}$，生成目标序列 $\mathbf{y} = (y_1, y_2, \dots, y_{T_y})$
  - 实现
    - 同样使用 RNN 或其变体，在每个时间步 $t$ 生成输出
      $$
      \mathbf{s}_t = \text{RNN}(\mathbf{y}_{t-1}, \mathbf{s}_{t-1})
      $$
      - $\mathbf{s}_t$ 是解码器的隐藏状态
      - $\mathbf{y}_{t-1}$ 是上一时间步的输出（ 或在训练时使用真实的目标输出 ）
    - 输出通过全连接层和激活函数（ 如 Softmax ）生成
      $$
      \mathbf{y}_t = \text{Softmax}(\mathbf{W}_y \mathbf{s}_t + \mathbf{b}_y)
      $$
  - 特点
    - 解码器逐时间步生成输出，直到生成终止符号，或达到最大长度

#### 2.3 训练与推理

- 训练
  - 使用 **教师强制**，在解码器输入中使用真实目标序列的上一时间步输出，而不是模型预测的输出
  - 损失函数通常为交叉熵损失，计算预测输出与真实输出之间的差异
- 推理
  - 解码器使用自身上一时间步的预测输出作为当前输入
  - 常用 **束搜索** 优化生成序列，保留多个高概率候选项以提高输出质量

#### 2.4 局限性

- 信息瓶颈
  - Encoder 把所有的输入序列都编码成了一个统一的语义特征 c 再输入到 Decoder 中，那么 c 中就必须包含原始序列中的所有信息，因此它的长度就成了限制模型性能的瓶颈
  - 上下文向量 $\mathbf{c}$ 可能无法完全捕捉长序列信息，导致信息丢失，尤其在长序列任务中
- 序列处理
  - 基于 RNN，LSTM，GRU 的 Seq2Seq 模型需逐时间步处理，无法并行化，计算效率较低

## 3. Attention（ 注意力机制 ）

- 为了解决基本 Seq2Seq 模型的信息瓶颈问题，Bahdanau 等人于 2015 年提出了注意力机制，所以现在 Seq2Seq 通常结合注意力机制使用

#### 3.1 基础介绍

- 注意力机制使解码器能够 **动态关注** 编码器输出的所有隐藏状态 $\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_{T_x}$，而不是仅依赖单一的上下文向量 $\mathbf{c}$
- 增加 Encoder 信息输入到 Decoder 中对应时刻的联系，并减弱其他时刻的联系
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/707eb470-fbb8-4e41-bdf6-1abf7473d15c)
  - 可以看到 C 与 Encoder 对应时刻的联系加粗，与 Encoder 其他时刻的联系变细

#### 3.2 工作原理

- **对应 解码器 的每个时间步 $t$**，通过注意力机制计算一个 **动态上下文向量** $\mathbf{c}_t$，而每个 $\mathbf{c}_t$ 包括 **所有编码器的信息**
  $$
  \mathbf{c}_t = \sum_{i=1}^{T_x} \alpha_{t,i} \mathbf{h}_i
  $$
  - $\alpha_{t,i}$ 是注意力权重，表示解码器在时间步 $t$ 对编码器隐藏状态 $\mathbf{h}_i$ 的关注程度
  - 权重 $\alpha_{t,i}$ 通常通过对齐模型计算，例如
    $$
    \alpha_{t,i} = \text{Softmax}(e_{t,i}) \\
    \quad e_{t,i} = \text{score}(\mathbf{s}_{t-1}, \mathbf{h}_i)
    $$
    - 其中 $\text{score}$ 是一个对齐函数（ 如点积、加性注意力等 ）
- 解码器的隐藏状态更新变为
  $$
  \mathbf{s}_t = \text{RNN}(\mathbf{y}_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}_t)
  $$
- 动态上下文向量 $\mathbf{c}_t$ 在每个对应的时间步都被使用，允许解码器根据当前生成需求聚焦输入序列的不同部分
