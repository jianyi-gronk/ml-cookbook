## 1. GRU（ Glavnoe Razvedivatelnoe Upravlenie，门控循环单元 ）

- GRU 是 RNN 的变体，设计目标是 **解决传统 RNN 的梯度消失和梯度爆炸问题，同时简化 LSTM 的结构**，减少参数量，适合处理长序列中的长期依赖关系

#### 1.1 与 RNN 和 LSTM 的区别

- 传统 RNN
  - 隐藏状态 $\mathbf{a}^{\langle t \rangle}$ 直接由上一时刻隐藏状态 $\mathbf{a}^{\langle t-1 \rangle}$ 和当前输入 $\mathbf{x}^{\langle t \rangle}$ 计算，缺乏选择机制，易受梯度消失影响，导致长期依赖丢失
- LSTM
  - 引入记忆单元 $\mathbf{c}^{\langle t \rangle}$ 和隐藏状态 $\mathbf{h}^{\langle t \rangle}$，通过输入门、遗忘门和输出门控制信息流，减少梯度消失，但参数量较大
- GRU
  - 简化了 LSTM 的结构，合并输入门和遗忘门为更新门，去掉单独的记忆单元，直接使用隐藏状态 $\mathbf{h}^{\langle t \rangle}$ 作为记忆载体
  - 使用更新门 $\mathbf{z}^{\langle t \rangle}$ 控制保留多少上一时刻隐藏状态和添加多少新信息，重置门 $\mathbf{r}^{\langle t \rangle}$ 控制当前输入对候选隐藏状态的影响
  - 参数量比 LSTM 少，计算效率更高，同时保留了处理长期依赖的能力
- 总结
  - RNN：简单的隐藏状态，无选择机制，易丢失长期依赖
  - LSTM：记忆单元和隐藏状态分离，三个门控（ 输入、遗忘、输出 ），更复杂但稳定
  - GRU：只有隐藏状态，两个门控（ 更新、重置 ），结构更简单，参数更少，性能接近 LSTM

#### 1.2 GPU 结构

- 核心组件
  - 隐藏状态 $\mathbf{h}^{\langle t \rangle}$
    - 既是记忆载体（ 类似 LSTM 的 $\mathbf{c}^{\langle t \rangle}$ ），又用于输出（ 类似 LSTM 的 $\mathbf{h}^{\langle t \rangle}$ ），传递到下一时间步或生成输出
  - 候选隐藏状态 $\tilde{\mathbf{h}}^{\langle t \rangle}$
    - 表示当前时间步的潜在新信息，类似 LSTM 的候选记忆 $\tilde{\mathbf{c}}^{\langle t \rangle}$
  - 门控机制
    - 更新门 $\mathbf{z}^{\langle t \rangle}$
      - 控制保留多少上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$ 和添加多少候选隐藏状态 $\tilde{\mathbf{h}}^{\langle t \rangle}$
      - 类似 LSTM 的输入门和遗忘门的组合
    - 重置门 $\mathbf{r}^{\langle t \rangle}$
      - 控制上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$ 对候选隐藏状态 $\tilde{\mathbf{h}}^{\langle t \rangle}$ 的影响，决定是否 “遗忘” 部分历史信息
- 激活函数：
  - Sigmoid
    - 用于更新门和重置门，输出范围 [0, 1]，表示门的开关程度（ 0 表示完全关闭，1 表示完全打开 ）
  - Tanh
    - 用于候选隐藏状态，输出范围 [-1, 1]，为信息添加非线性

## 2. 数学公式

- 当前输入 $\mathbf{x}^{\langle t \rangle}$，上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$

#### 2.1 门控计算

- 更新门
  $$
  \mathbf{z}^{\langle t \rangle} = \sigma(\mathbf{W}_z [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_z)
  $$
  - 决定保留多少 $\mathbf{h}^{\langle t-1 \rangle}$ 和添加多少 $\tilde{\mathbf{h}}^{\langle t \rangle}$
- 重置门
  $$
  \mathbf{r}^{\langle t \rangle} = \sigma(\mathbf{W}_r [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_r)
  $$
  - 控制 $\mathbf{h}^{\langle t-1 \rangle}$ 对候选隐藏状态的影响
- 候选隐藏状态
  $$
  \tilde{\mathbf{h}}^{\langle t \rangle} = \tanh(\mathbf{W}_h [\mathbf{r}^{\langle t \rangle} \odot \mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_h)
  $$
  - 重置门 $\mathbf{r}^{\langle t \rangle}$ 通过逐元素相乘调节 $\mathbf{h}^{\langle t-1 \rangle}$，决定候选状态中包含多少历史信息
- 其中
  - $\sigma$ 是 Sigmoid 函数，$\tanh$ 是 Tanh 函数。
  - $\mathbf{W}_z, \mathbf{W}_r, \mathbf{W}_h$ 是各门的权重矩阵。
  - $\mathbf{b}_z, \mathbf{b}_r, \mathbf{b}_h$ 是偏置向量。
  - $[\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}]$ 表示向量拼接。
  - 类似 RNN 和 LSTM，GRU 的参数在所有时间步中共享。

#### 2.2 隐藏状态和输出

- 隐藏状态
  $$
  \mathbf{h}^{\langle t \rangle} = (1 - \mathbf{z}^{\langle t \rangle}) \odot \mathbf{h}^{\langle t-1 \rangle} + \mathbf{z}^{\langle t \rangle} \odot \tilde{\mathbf{h}}^{\langle t \rangle}
  $$
  - $\odot$ 表示逐元素相乘
  - $(1 - \mathbf{z}^{\langle t \rangle}) \odot \mathbf{h}^{\langle t-1 \rangle}$ 决定保留多少上一时刻隐藏状态
  - $\mathbf{z}^{\langle t \rangle} \odot \tilde{\mathbf{h}}^{\langle t \rangle}$ 决定添加多少候选隐藏状态
- 输出（ 若需要 ）
  $$
  \mathbf{y}^{\langle t \rangle} = g(\mathbf{W}_{hy} \mathbf{h}^{\langle t \rangle} + \mathbf{b}_y)
  $$
  - g 通常为 Softmax（ 分类任务 ）或线性函数（ 回归任务 ）

## 3. 计算步骤

- 输入处理
  - 当前输入 $\mathbf{x}^{\langle t \rangle}$ 和上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$ 拼接
  - 计算更新门 $\mathbf{z}^{\langle t \rangle}$ 和重置门 $\mathbf{r}^{\langle t \rangle}$
- 候选隐藏状态生成
  - 重置门 $\mathbf{r}^{\langle t \rangle}$ 调节 $\mathbf{h}^{\langle t-1 \rangle}$，与 $\mathbf{x}^{\langle t \rangle}$ 结合生成候选隐藏状态 $\tilde{\mathbf{h}}^{\langle t \rangle}$
- 隐藏状态更新
  - 更新门 $\mathbf{z}^{\langle t \rangle}$ 决定保留多少 $\mathbf{h}^{\langle t-1 \rangle}$ 和添加多少 $\tilde{\mathbf{h}}^{\langle t \rangle}$，更新得到当前隐藏状态 $\mathbf{h}^{\langle t \rangle}$
- 输出生成
  - 根据任务需要，隐藏状态 $\mathbf{h}^{\langle t \rangle}$ 通过权重矩阵 $\mathbf{W}_{hy}$ 和激活函数 g 生成输出 $\mathbf{y}^{\langle t \rangle}$
- 流程图表示（ \* 用来代替 $\odot$ 符号 ）

  ```mermaid
  graph TD
    %% 输入和拼接
    A[输入: xᵗ, hᵗ⁻¹] -->|输入数据| B[拼接 xᵗ, hᵗ⁻¹]
    B -->|门控计算| C[计算门控信号]

    Z[更新门 zᵗ]
    Htilde[候选隐藏状态 h̃ᵗ]
    C -->|更新门| Z
    C -->|候选隐藏状态，只提供 xᵗ| Htilde

    R[重置门 rᵗ]
    Hprev[上一时刻隐藏状态 hᵗ⁻¹]
    C -->|重置门| R
    R -->|提供 rᵗ| E
    Hprev -->|提供 hᵗ⁻¹| E

    E[保留隐藏状态 rᵗ * hᵗ⁻¹]

    %% 隐藏状态更新
    Z -->|提供 zᵗ| H[隐藏状态 hᵗ =（1-zᵗ）* hᵗ⁻¹ + zᵗ * h̃ᵗ]
    E --> |提供 rᵗ * hᵗ⁻¹| Htilde
    Htilde -->|提供 h̃ᵗ| H
    Hprev -->|提供 hᵗ⁻¹| H

    %% 输出
    H -->|最终输出| J[输出 yᵗ（若需要）]
  ```

## 4. 存在问题

- 模型复杂性
  - 虽然 GRU 比 LSTM 简化（ 两个门控而非三个，无单独记忆单元 ），但相比传统 RNN，参数量仍较多，易过拟合，计算成本较高
  - 解决方案
    - 使用更简单的 RNN 变体（ 如 Minimal Gated Unit ）或轻量化模型
    - 在小数据集上使用正则化（ 如 Dropout ）防止过拟合
- 需序列处理
  - 像 RNN 和 LSTM，GRU 需逐时间步处理，无法完全并行化，计算效率低于非循环模型
  - 解决方案
    - Transformer 架构通过自注意力机制实现全序列并行计算，取代 GRU 在许多任务（ 如 NLP ）中的地位
- 长期依赖限制
  - 虽然 GRU 通过门控机制缓解梯度消失，但对于极长序列，仍然可能丢失部分远期信息
  - 解决方案
    - 使用注意力机制（ 如 Transformer ）或结合卷积网络（ 如 TCN ）增强长序列建模能力
