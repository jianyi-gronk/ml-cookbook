## 1. LSTM（ Long Short Term Memory，长短时记忆网络 ）

- LSTM 是 RNN 的变体，设计目标是 **解决传统 RNN 的梯度消失和梯度爆炸问题，特别适合处理长序列中的长期依赖关系**

#### 1.1 与 RNN 的区别

- 传统 RNN
  - 每个时间步的隐藏状态 $\mathbf{a}^{\langle t \rangle}$ 直接由上一时刻的 隐藏状态 $\mathbf{a}^{\langle t-1 \rangle}$ 和 当前输入 $\mathbf{x}^{\langle t \rangle}$ 计算，会存储所有历史信息
  - 缺乏选择机制，所有信息无差别地传递，易受梯度消失影响，导致长期信息丢失
- LSTM
  - 引入 记忆单元 $\mathbf{c}^{\langle t \rangle}$，专门存储长期信息，而 隐藏状态 $\mathbf{h}^{\langle t \rangle}$（ 对应 RNN 的 $\mathbf{a}^{\langle t \rangle}$ ）从中提取短期信息
  - 使用 门控机制（ 输入门、遗忘门、输出门 ）选择性地控制信息流，决定哪些信息 存储、遗忘 或 输出
  - 通过加法操作（ 如记忆单元的更新 ）减少梯度消失的乘法效应，稳定梯度传播
- 总结
  - RNN：隐藏状态像一个简单的 “记忆池”，无选择地存储所有信息
  - LSTM：记忆单元像一个 “智能过滤器”，通过门控机制动态选择重要信息存储或丢弃

#### 1.2 LSTM 结构

- 核心组件
  - 记忆单元 $\mathbf{c}^{\langle t \rangle}$
    - 保存长期信息，贯穿整个序列，通过加法更新减少梯度消失
  - 隐藏状态 $\mathbf{h}^{\langle t \rangle}$
    - 从记忆单元提取短期信息，传递到下一时间步或用于输出
  - 门控机制，各个门的权重矩阵不同
    - 输入门 $\mathbf{i}^{\langle t \rangle}$
      - 控制当前输入是否加入记忆单元
    - 遗忘门 $\mathbf{f}^{\langle t \rangle}$
      - 控制记忆单元中哪些信息需要遗忘
    - 输出门 $\mathbf{o}^{\langle t \rangle}$
      - 控制记忆单元中哪些信息输出到隐藏状态
    - 先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘记忆单元里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出
- 激活函数
  - Sigmoid
    - 用于输入门、遗忘门、输出门，输出范围 [0, 1]，表示门的开关程度（ 0 表示完全关闭，1 表示完全打开 ）
  - Tanh
    - 用于候选记忆和隐藏状态，输出范围 [-1, 1]，为信息添加非线性，是真正作为输入的，而不是门控装置

## 2. 数学公式

- 当前输入 $\mathbf{x}^{\langle t \rangle}$，上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$

#### 2.1 门控计算

- 输入门
  $$
  \mathbf{i}^{\langle t \rangle} = \sigma(\mathbf{W}_i [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_i)
  $$
- 遗忘门
  $$
  \mathbf{f}^{\langle t \rangle} = \sigma(\mathbf{W}_f [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_f)
  $$
- 输出门
  $$
  \mathbf{o}^{\langle t \rangle} = \sigma(\mathbf{W}_o [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_o)
  $$
- 候选记忆
  $$
  \tilde{\mathbf{c}}^{\langle t \rangle} = \tanh(\mathbf{W}_c [\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_c)
  $$
- 其中
  - $\sigma$ 是 Sigmoid 函数，$\tanh$ 是 Tanh 函数
  - $\mathbf{W}_i, \mathbf{W}_f, \mathbf{W}_o, \mathbf{W}_c$ 是各门的权重矩阵
  - $\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o, \mathbf{b}_c$ 是偏置向量
  - $[\mathbf{h}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}]$ 表示向量拼接
  - LSTM 中的参数也是和 RNN 一样参数共享，即在所有时间步中共享

#### 2.2 记忆单元更新

- 当前记忆单元
  $$
  \mathbf{c}^{\langle t \rangle} = \mathbf{f}^{\langle t \rangle} \odot \mathbf{c}^{\langle t-1 \rangle} + \mathbf{i}^{\langle t \rangle} \odot \tilde{\mathbf{c}}^{\langle t \rangle}
  $$
  - $\odot$ 表示逐元素相乘
  - $\mathbf{f}^{\langle t \rangle} \odot \mathbf{c}^{\langle t-1 \rangle}$ 决定保留多少上一时刻的记忆
  - $\mathbf{i}^{\langle t \rangle} \odot \tilde{\mathbf{c}}^{\langle t \rangle}$ 决定添加多少新信息

#### 2.3 隐藏状态和输出

- 隐藏状态
  $$
  \mathbf{h}^{\langle t \rangle} = \mathbf{o}^{\langle t \rangle} \odot \tanh(\mathbf{c}^{\langle t \rangle})
  $$
- 输出（ 若需要 ）
  $$
  \mathbf{y}^{\langle t \rangle} = g(\mathbf{W}_{hy} \mathbf{h}^{\langle t \rangle} + \mathbf{b}_y)
  $$
  - g 通常为 Softmax（ 分类任务 ）或线性函数（ 回归任务 ）

## 3. 计算步骤

- 输入处理
  - 当前输入 $\mathbf{x}^{\langle t \rangle}$ 和上一时刻隐藏状态 $\mathbf{h}^{\langle t-1 \rangle}$ 拼接
  - 计算 输入门 $\mathbf{i}^{\langle t \rangle}$、遗忘门 $\mathbf{f}^{\langle t \rangle}$、输出门 $\mathbf{o}^{\langle t \rangle}$ 和 候选记忆 $\tilde{\mathbf{c}}^{\langle t \rangle}$
- 记忆单元更新
  - 遗忘门决定保留多少上一时刻的记忆单元 $\mathbf{c}^{\langle t-1 \rangle}$
  - 输入门决定添加多少候选记忆 $\tilde{\mathbf{c}}^{\langle t \rangle}$
  - 更新得到当前记忆单元 $\mathbf{c}^{\langle t \rangle}$
- 隐藏状态生成
  - 输出门控制从当前记忆单元 $\mathbf{c}^{\langle t \rangle}$ 中提取多少信息
  - 经过 tanh 激活生成隐藏状态 $\mathbf{h}^{\langle t \rangle}$
- 输出生成
  - 根据任务需要，隐藏状态 $\mathbf{h}^{\langle t \rangle}$ 通过权重矩阵 $\mathbf{W}_{hy}$ 和激活函数 g 生成输出 $\mathbf{y}^{\langle t \rangle}$
- 流程图表示（ \* 用来代替 $\odot$ 符号 ）

  ```mermaid
  graph TD
    %% 输入和拼接
    A[输入: xᵗ, hᵗ⁻¹] -->|输入数据| B[拼接 xᵗ, hᵗ⁻¹]
    B -->|门控计算| C[计算门控信号]
    C -->|输入门| D
    C -->|候选记忆| G
    C -->|遗忘门| E

    D[输入门 iᵗ]
    G[候选记忆 c̃ᵗ]
    E[遗忘门 fᵗ]
    K[上一时刻记忆 cᵗ⁻¹]

    N[上一时刻记忆输入]
    L[当前记忆输入]

    %% 输出门
    C -->|输出门| F[输出门 oᵗ]

    %% 记忆单元更新
    G -->|提供 c̃ᵗ| L
    K -->|提供 cᵗ⁻¹| N
    D -->|提供 iᵗ| L
    E -->|提供 fᵗ| N

    L --> |提供 iᵗ * c̃ᵗ| H
    N --> |提供 fᵗ * cᵗ⁻¹| H

    H[记忆单元 cᵗ = fᵗ * cᵗ⁻¹ + iᵗ * c̃ᵗ]

    %% 隐藏状态和输出
    H -->|提供 cᵗ| I[隐藏状态 hᵗ = oᵗ * cᵗ]
    F -->|提供 oᵗ| I
    I -->|最终输出| J[输出 yᵗ（ 若需要 ）]
  ```

## 4. 存在问题

- 模型复杂
  - 相比传统 RNN，LSTM 引入多个门控和记忆单元，参数量显著增加，所以计算量也会增加，同时在小数据集上可能过拟合
  - 解决方案
    - GRU 是简化版的 LSTM，合并输入门和遗忘门为更新门，减少参数量
- 需序列处理
  - 虽然比 RNN 更稳定，但 LSTM 仍需逐时间步处理，无法完全并行化
  - 解决方案
    - Transformer 通过自注意力机制实现并行计算，取代 LSTM 在许多任务中的地位
