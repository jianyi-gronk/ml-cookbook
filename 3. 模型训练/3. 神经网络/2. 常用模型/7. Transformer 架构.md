## 1. 基础介绍

- Transformer 是 2017 年论文《Attention is All You Need》中提出的序列到序列（ Seq2Seq ）模型，完全基于注意力机制（ 自注意力、掩码自注意力、跨注意力 ），取代传统的 RNN 和 CNN，广泛应用于自然语言处理（ NLP ）、计算机视觉等任务
- 特点
  - **并行计算**：通过注意力机制实现 token 的并行处理，显著提高计算效率
  - **长距离依赖**：注意力机制允许每个 token 直接访问序列中的任意其他 token，捕捉长距离依赖
  - **模块化架构**：编码器和解码器由多层堆叠组成，每层包含注意力机制和前馈网络，易于扩展

## 2. 主要组件

- 通常 encoder 层由 6 个 encoder 堆叠在一起，decoder 层也由 6 个堆叠（ 因为论文中这样 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d61184ef-f477-490c-8ea5-966a856c291e)
- Tranformer 结构如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/df7a175d-ead5-4054-8330-736ecc5ffdff)
- 核心模块包括：嵌入层，编码器，解码器，输出层

## 3. 嵌入层（ Embedding Layer ）

#### 3.1 作用

- 嵌入层将 token 编码后的索引 ID 序列 映射为固定维度的密集向量（ 嵌入向量 ），表示其语义特征，类似于 Word2Vec 或 GloVe 的词嵌入
- 嵌入层的目的是将离散的索引 ID（ 来自编码阶段 ）转换为连续的向量表示，便于神经网络（ 如 Transformer ）处理，同时捕捉 token 的语义信息
- 在 Transformer 模型中，嵌入层是第一层，通常结合位置编码（ Positional Encoding ）和分段编码（ Segment Embedding，可选 ），这两个编码的维度和 embedding 的维度一样，为后续注意力机制提供输入

#### 3.2 特点

- 嵌入层通过训练捕捉 token 间的语义关系，例如 “cat” 和 “dog” 可能具有相似的嵌入向量
- 在 NLP 中，token 可以是词、子词（ subword，如 BPE 分词 ）或字符；在计算机视觉中，token 可以是图像块（ patch ）
- 嵌入向量是静态的（ 不考虑上下文 ），通过注意力机制生成上下文相关的表示

#### 3.3 嵌入向量

- 嵌入层输出的向量是固定维度的密集向量（ 嵌入向量 ），用于表示输入序列中每个 token 的语义特征
- 嵌入维度 $d$ 通常为 512 或 768（ 例如 BERT-base 使用 768 维，BERT-large 使用 1024 维 ）
- 示例：输入 token：“love” 对应 编码后索引 ID：2293
  - 嵌入层输出：$\mathbf{x} \in \mathbb{R}^{768}$，一个 768 维向量，表示 “love” 的语义

#### 3.4 计算过程

- 输入：编码后的索引 ID 序列 $\mathbf{X} = [\text{id}_1, \text{id}_2, \dots, \text{id}_{T_x}]$，其中 $T_x$ 是序列长度
- 词嵌入（ Token Embedding ）
  - 每个索引 ID 通过嵌入矩阵 $\mathbf{E} \in \mathbb{R}^{V \times d}$ 转换为固定维度向量
    $$\mathbf{x}_i = \text{Embedding}(\text{id}_i) = \mathbf{E}[\text{id}_i]$$
    - $V$：词汇表大小，包括特殊 token，如 [CLS]，[SEP]
    - $d$：嵌入维度，通常 $d$ 为 $512$ 或 $768$
    - $\mathbf{x}_i \in \mathbb{R}^d$：第 $i$ 个 token 的嵌入向量
  - 嵌入矩阵 $\mathbf{E}$ 是可学习的参数，在模型训练过程中通过梯度下降优化，学习 token 的语义表示
- 位置编码（ Positional Encoding ）
  - Transformer 模型不使用循环结构，**无法直接捕捉 token 顺序**，因此在嵌入层后添加位置编码
    $$\mathbf{z}_i = \mathbf{x}_i + \mathbf{p}_i$$
    - $\mathbf{p}_i \in \mathbb{R}^d$：第 $i$ 个位置的位置编码向量
    - 位置编码可以是固定的（ 如 BERT 使用正弦函数 ）或可学习的参数
    - 这个位置向量的具体计算方法有很多种，论文中的计算方法如下：
      $$
      PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\
      PE(pos, 2i + 1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
      $$
      - 其中 pos 是指当前词在句子中的位置， 是指向量中每个值的 index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码
- 分段编码（ Segment Embedding，可选 ）
  - 用于区分不同句子（ 如 BERT 的句 A 和句 B ）
  - 示例
    - 输入序列 ["[CLS]", "i", "love", "[SEP]", "it", "is", "fun", "[SEP]"]
    - 分段编码为 [0, 0, 0, 0, 1, 1, 1, 1]（ 句 A 用 0，句 B 用 1 ）
  - 分段编码向量 $\mathbf{s}_i \in \mathbb{R}^d$ 与词嵌入和位置编码相加
    $$\mathbf{z}_i = \mathbf{x}_i + \mathbf{p}_i + \mathbf{s}_i$$

## 4. 编码器 和 解码器

- 两层编码器-解码器结构的 Transformer 如下图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/557efcac-92ad-4e42-9470-08d10ab0e3e7)

#### 4.1 核心组件

- 前馈神经网络（ FNN ）
  - 对每个 token 独立应用两层线性变换，中间使用 ReLU 激活
    $$\text{FFN}(\mathbf{x}) = \max(0, \mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2$$
    - $\mathbf{W}_1 \in \mathbb{R}^{d \times d_{ff}}$，$d_{ff}$ 通常为 $4d$（ 如 2048 ）
    - $\mathbf{W}_2 \in \mathbb{R}^{d_{ff} \times d}$，确保输出维度为 $d$
  - 并行性
    - FFN 对序列中每个 token 的计算是独立的，因此 **所有 token 的变换可以并行执行**
  - 作用
    - 引入非线性变换，增强特征表达能力
- 残差连接和层归一化
  - 每个子层（ 自注意力、FFN ）后添加残差连接
    $$\mathbf{Z}' = \mathbf{Z} + \text{SubLayer}(\mathbf{Z})$$
  - 然后应用层归一化
    $$\mathbf{Z}'' = \text{LayerNorm}(\mathbf{Z}')$$
  - 作用
    - 稳定训练，保持维度一致，缓解梯度消失/爆炸问题

#### 4.2 编码器

- 目的
  - 将输入序列的嵌入向量转换为上下文丰富的特征表示 $\mathbf{Z}_{\text{enc}} \in \mathbb{R}^{T_x \times d}$，通过多头自注意力捕捉全局依赖关系，传递给解码器
- 由 $N$ 层堆叠组成，每层包含
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0a4cb225-8070-4124-95be-b762ac28e213)
  - 多头自注意力 → 残差连接和层归一化 → 前馈神经网络 → 残差连接和层归一化
  - 这种顺序确保了注意力机制先捕捉全局依赖，然后通过 FFN 进行局部非线性变换，同时残差连接在每个子层后应用，以保持信息流动
- 输入
  - 嵌入层输出 $\mathbf{Z} \in \mathbb{R}^{T_x \times d}$，包括词嵌入 + 位置编码 + 分段编码（ 可选 ）
- 输出
  - 得到的输出会输入到下一个 Encoder
  - 编码器最后一层生成 $\mathbf{Z}_{\text{enc}} \in \mathbb{R}^{T_x \times d}$，传递给解码器

#### 4.3 解码器

- 目的
  - 基于编码器输出 $\mathbf{Z}_{\text{enc}}$ 和目标序列的嵌入向量，生成上下文丰富的特征表示 $\mathbf{S} \in \mathbb{R}^{T_y \times d}$，用于自回归生成（ 如逐词翻译 ）
  - 通过掩码自注意力和跨注意力，融合源序列和目标序列信息，支持序列生成任务
- 由 $N$ 层堆叠组成，每层包含
  - 多头掩码自注意力 -> 残差连接和层归一化 -> 多头跨注意力 -> 残差连接和层归一化 -> FNN -> 残差连接和层归一化
  - 这种顺序确保掩码自注意力捕捉目标序列的上下文，跨注意力融合源序列信息，FFN 增强特征表达
- 输入
  - 目标序列的嵌入向量 $\mathbf{Z}_{\text{dec}} \in \mathbb{R}^{T_y \times d}$，包括词嵌入 + 位置编码 + 分段编码（ 可选 ），其中 $T_y$ 是目标序列长度
  - 编码器输出 $\mathbf{Z}_{\text{enc}} \in \mathbb{R}^{T_x \times d}$，作为跨注意力的键和值
- 输出
  - 得到的输出会输入到下一个 Decoder（ 在训练时并行处理整个目标序列；在推理时逐 token 生成 ）
  - 解码器最后一层生成 $\mathbf{S} \in \mathbb{R}^{T_y \times d}$，传递给输出层生成 token 概率分布

## 5. 输出层

- 假如词典是 1 万个词，那最终 softmax 会输出 1 万个词的概率，概率值最大的对应的词就是最终的结果
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/46fb3af8-9559-428d-bc7c-8412a8da2fe2)
- 解码器输出经过线性层和 Softmax，生成每个时间步的 token 概率分布
  $$
  \mathbf{p}_t = \text{Softmax}(\mathbf{W}_o \mathbf{s}_t + \mathbf{b}_o)
  $$
  - $\mathbf{s}_t$ 是解码器最后一层的输出向量

## 6. Transformer 动态流程图

- 编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量 K（ 键向量 ）和 V（ 值向量 ）的注意力向量集 ，这是并行化操作。 这些向量将被每个解码器用于自身的 编码-解码注意力层 ，而这些层可以帮助解码器关注输入序列哪些位置合适
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b23d748e-3d72-4e4f-aa6b-c3448a742875)
- 在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列的元素（ 在这个例子里，是英语翻译的句子 ）
- 接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示 transformer 的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像对编码器的输入所做的那样，会嵌入并添加位置编码给那些解码器，来表示每个单词的位置
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/955a34a3-9a9d-4f90-985b-8cc84513619c)
- 而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在 softmax 步骤前，它会把后面的位置给隐去（ 把它们设为-inf ）
- 这个 编码器-解码器注意力层 工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造 Queries 矩阵，并且从编码器的输出中取得 Keys、Values 矩阵

## 7. Transformer 和 Seq2seq 的主要区别

- 结构
  - Seq2seq 使用循环神经网络（ 如 LSTM 或 GRU ）来建模序列信息，其中编码器和解码器是逐步进行的
  - 而 Transformer 则使用自注意力机制，允许并行计算，没有显式的时间顺序
- 长距离依赖
  - Seq2seq 的 RNN 结构在处理长序列时可能面临梯度消失或梯度爆炸的问题
  - 而由于自注意力机制的存在，Transformer 更擅长处理长距离依赖关系
- 效率
  - 由于并行计算和自注意力机制的使用，Transformer 在某些情况下可以更高效地处理长序列，同时减少了训练时间
- 位置编码
  - Seq2seq 使用循环神经网络，本身就天然具备处理序列的位置信息
  - Transformer 引入了位置编码，以便模型能够理解输入序列中的单词位置信息
- 应用场景
  - Seq2seq 主要应用于需要处理变长序列的任务，如机器翻译、对话生成等
  - 而 Transformer 除了这些任务，还广泛应用于语言建模、文本分类、命名实体识别等 NLP 任务，并在计算机视觉领域也得到了应用
