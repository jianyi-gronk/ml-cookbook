## 1. CNN（ Convolutional Neural Networks，卷积神经网络 ）

- 卷积神经网络 是一种专门为处理结构化网格数据（ 如图像 ）设计的神经网络，**至少包含一层 卷积层**，广泛应用于 图像分类、目标检测、语义分割 等任务
- CNN 通过卷积操作提取空间特征，结合池化层和全连接层实现高效的特征学习和任务预测

#### 1.1 通道

- 通道 是指 输入数据 或 特征图 在深度维度上的分量，是一个二维矩阵，通常反映数据的不同特征或颜色信息
  - 对于输入图像
    - 灰度图像：1 个通道，只有亮度信息
    - RGB 彩色图像：3 个通道，即 红、绿、蓝 分别对应一个通道
    - 其他类型图像：如 hyperspectral 图像可能有更多通道
  - 对于特征图（ 卷积层 或 池化层 的输出 ）
    - 通道数由上一层的过滤器数量决定，每个通道表示一种提取的特征，如 边缘、纹理 等
- 作用
  - 通道允许 CNN 处理多维信息。例如，RGB 图像的 3 个通道分别捕获红、绿、蓝颜色信息，卷积和池化操作对每个通道独立处理，最终融合结果
  - 在深层网络中，通道数通常递增，表示更复杂的特征组合，如 从 3 到 16、32、64 等

#### 1.2 卷积核（ Kernel ）

- 定义
  - 卷积核 是一个高度和宽度小于输入图像的二维矩阵，也称 卷积矩阵
  - 卷积核 在输入图像的高度和宽度上滑动，通过计算 卷积核 与 图像局部区域 的点积，提取局部特征，生成特征图，该图也称为卷积特征
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b5da219b-192c-4a4a-b8c0-1fd269b6c41d)
  - 在滑动的过程中，卷积核的参数一致，用于降低模型的参数量，从而提高了计算效率
- 核心参数如下
  - 步幅（ Stride ）
    - 卷积核滑动的距离，控制输出特征图尺寸，步幅越大，输出尺寸越小
    - 输出尺寸公式
      $$\text{Output Size} = \left\lfloor \frac{\text{Input Size} + 2 \times \text{Padding} - \text{Kernel Size}}{\text{Stride}} \right\rfloor + 1$$
    - 例如，输入 6×6 图像，3×3 卷积核，步幅 1，无 padding，输出尺寸为
      $$\text{Output Size} = \left\lfloor \frac{6 + 2 \times 0 - 3}{1} \right\rfloor + 1 = 4$$
  - 填充（ padding ）
    - 在卷积过程中，图像中间区域会被多次计算，而边缘区域（ 比如下图绿色阴影部分 ）只会被卷积计算一次
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/84cb481c-3251-4a2f-9b6b-dd65ced7ca41)
    - 可以在输入图像周围添加边界（ 如零填充 ），目的是控制卷积层输出的大小，以及在进行卷积操作时保持输入数据边界信息，p = 1 时，即会在最外层加一圈（ 蓝色区域 ）
  - 偏置（ bias ）
    - bias 通常不需要指定，模型会自己进行训练找到最佳值，每个卷积层通常包括多个卷积核，每个卷积核与一个 bias 相关联
    - 它的作用是在卷积操作后，对每个卷积核的输出加上一个常数值（ 即偏置值 ），这样提供了模型在学习过程中的额外自由度，有助于模型更好地拟合训练数据，提高模型的自由度

#### 1.3 过滤器（ Filter ）

- 每个过滤器 由 多个卷积核 组成，且每个卷积核对应输入的一个通道，因此过滤器中的通道数（ 深度 ）必须与输入图像中的通道数相同
- 当使用通道为 3 卷积彩色图像（ RGB 图像 ）时，过滤器的通道数（ 深度 ）也必须为 3
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/48dd4436-a625-48d9-8099-61408310ed49)
- 每个过滤器负责提取一种特定特征，如 边缘、纹理、角点 等，可以多个过滤器组合使用，输出的通道数等于过滤器数（ 此时所有卷积核的大小必须相同 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/f4806b26-c38e-40ed-8d1e-5b9f194ec025)
- 过滤器的权重是可学习的，通过训练优化以捕捉输入数据中最有意义的模式

#### 1.4 激活函数

- 激活函数是卷积层的最后一个组成部分，可增加输出中的非线性，增强模型表达能力
- 通常，在卷积层中将 ReLu 函数用作激活函数
- 例如以下简单卷积层的图像，其中将 6 x 6 x 3 输入图像与大小为 4 x 4 x 3 的 2 个过滤器以得到大小为 3 x 3 x 2 的卷积特征，对其应用激活函数以获取输出，这也称为特征图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/20565953-9ab1-4b49-bf97-21029490f608)

#### 1.5 池化层

- 在 CNN 中，通常先经过 卷积层，然后再经过 池化层
- 池化层通过下采样减小特征图尺寸，提高计算效率和特征鲁棒性，同时实现平移不变性（ 指模型对输入图像中目标物体位置的小范围移动不敏感 ）
- 池化层主要有三个超参数
  - f
    - 池化区域大小为 f \* f
    - 由于池化层过快地减少了数据的大小，目前的趋势是使用较小的池化区域大小，通常使用 (f = 2, s = 2)，偶尔也有人采用 (f = 3, s = 2)
  - s
    - 步幅，控制池化区域滑动距离
  - 类型
    - 最大池化（ 取区域最大值 ）或 平均池化（ 取区域平均值 ）
    - 因 最大池化 保留显著特征，目前更常用
- 全局平均池化
  - 原理
    - 对每个输入通道的整个特征图取平均值，将多个特征图（ 尺寸 $H \times W \times C$ ）压缩为一个向量（ 尺寸 $ 1 \times 1 \times C $ ）
  - 使用场景
    - 在 VGG、ResNet、Inception、EfficientNet 等架构中都被使用
    - 例如，ResNet-50 在卷积层后使用全局平均池化，将特征图压缩为向量，再接一个小的全连接层（ 或直接 softmax ），用于最后阶段的 降维 和 分类

#### 1.6 深度卷积神经网络（ DCNN ）

- 通过多层 卷积 和 池化 来提取图像中的特征，并在末尾通过全连接层进行分类或回归任务
- 基本原理
  - 特征提取：早期层提取低级特征（ 如边缘 ），深层提取高级特征（ 如对象形状 ）
  - 分类/回归：全连接层将特征映射为类别概率或回归值
  - 训练：通过反向传播和优化算法（ 如 Adam、SGD ）最小化损失函数（ 如交叉熵 ）
- 典型架构
  - VGG：堆叠多个 3×3 卷积层，增加网络深度
  - ResNet：引入残差连接，解决梯度消失问题
  - Inception：多尺度卷积并行处理，增强特征提取效率
