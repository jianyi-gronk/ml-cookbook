## 1. CNN（ Convolutional Neural Networks，卷积神经网络 ）

- 卷积神经网络 是一种专门为处理结构化网格数据（ 如图像 ）设计的神经网络，**至少包含一层 卷积层**，广泛应用于 图像分类、目标检测、语义分割 等任务
- CNN 通过卷积操作提取空间特征，结合池化层和全连接层实现高效的特征学习和任务预测
- 在 CNN 中，常见的标准特征提取流程是 **卷积 → 批归一化 → 激活函数 → 池化**，不过在某些特定架构中，可能会根据设计目标进行微调

#### 1.1 通道

- 通道 是指 输入数据 或 特征图 在深度维度上的分量，是一个二维矩阵，通常反映数据的不同特征或颜色信息
  - 对于输入图像
    - 灰度图像：1 个通道，只有亮度信息
    - RGB 彩色图像：3 个通道，即 红、绿、蓝 分别对应一个通道
    - 其他类型图像：如 hyperspectral 图像可能有更多通道
  - 对于特征图（ 卷积层 或 池化层 的输出 ）
    - 通道数由上一层的过滤器数量决定，每个通道表示一种提取的特征，如 边缘、纹理 等
- 作用
  - 通道允许 CNN 处理多维信息。例如，RGB 图像的 3 个通道分别捕获红、绿、蓝颜色信息，卷积和池化操作对每个通道独立处理，最终融合结果
  - 在深层网络中，通道数通常递增，表示更复杂的特征组合，如 从 3 到 16、32、64 等

#### 1.2 卷积核（ Kernel ）

- 定义
  - 卷积核 是一个高度和宽度小于输入图像的二维矩阵，也称 卷积矩阵
  - 卷积核 在输入图像的高度和宽度上滑动，通过计算 卷积核 与 图像局部区域 的点积，提取局部特征，生成特征图，该图也称为卷积特征
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b5da219b-192c-4a4a-b8c0-1fd269b6c41d)
  - 在滑动的过程中，卷积核的参数一致，用于降低模型的参数量，从而提高了计算效率
- 核心参数如下
  - 步幅（ Stride ）
    - 卷积核滑动的距离，控制输出特征图尺寸，步幅越大，输出尺寸越小
    - 输出尺寸公式
      $$\text{Output Size} = \left\lfloor \frac{\text{Input Size} + 2 \times \text{Padding} - \text{Kernel Size}}{\text{Stride}} \right\rfloor + 1$$
    - 例如，输入 6×6 图像，3×3 卷积核，步幅 1，无 padding，输出尺寸为
      $$\text{Output Size} = \left\lfloor \frac{6 + 2 \times 0 - 3}{1} \right\rfloor + 1 = 4$$
  - 填充（ padding ）
    - 在卷积过程中，图像中间区域会被多次计算，而边缘区域（ 比如下图绿色阴影部分 ）只会被卷积计算一次
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/84cb481c-3251-4a2f-9b6b-dd65ced7ca41)
    - 可以在输入图像周围添加边界（ 如零填充 ），目的是控制卷积层输出的大小，以及在进行卷积操作时保持输入数据边界信息，p = 1 时，即会在最外层加一圈（ 蓝色区域 ）
  - 偏置（ bias ）
    - bias 通常不需要指定，模型会自己进行训练找到最佳值，每个卷积层通常包括多个卷积核，每个卷积核与一个 bias 相关联
    - 它的作用是在卷积操作后，对每个卷积核的输出加上一个常数值（ 即偏置值 ），这样提供了模型在学习过程中的额外自由度，有助于模型更好地拟合训练数据，提高模型的自由度

#### 1.3 过滤器（ Filter ）

- 每个过滤器 由 多个卷积核 组成，且每个卷积核对应输入的一个通道，因此过滤器中的通道数（ 深度 ）必须与输入图像中的通道数相同
- 当使用通道为 3 卷积彩色图像（ RGB 图像 ）时，过滤器的通道数（ 深度 ）也必须为 3
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/48dd4436-a625-48d9-8099-61408310ed49)
- 每个过滤器负责提取一种特定特征，如 边缘、纹理、角点 等，可以多个过滤器组合使用，输出的通道数等于过滤器数（ 此时所有卷积核的大小必须相同 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/f4806b26-c38e-40ed-8d1e-5b9f194ec025)
- 过滤器的权重是可学习的，通过训练优化以捕捉输入数据中最有意义的模式

#### 1.4 批归一化

- 通常在 卷积层 之后进行 批归一化
- 核心作用
  - 稳定输入分布
    - 将卷积层输出的特征图进行归一化，使其均值接近 0、标准差接近 1，为后续的激活函数提供稳定的输入分布
  - 避免激活函数饱和
    - 防止输入值落入 ReLU 等激活函数的饱和区（ 如梯度为零的区域 ），确保梯度有效传播
  - 加速训练收敛
    - 通过稳定每层的输入分布，减少内部协变量偏移，允许使用更大的学习率，显著加快训练过程
- 效果：使深度网络训练更加稳定、快速，同时具有一定的正则化效果

#### 1.5 激活函数

- 通常在 批归一化 之后进行 激活函数，一般将 ReLu 函数用作激活函数
- 例如以下简单卷积层的图像，其中将 6 x 6 x 3 输入图像与大小为 4 x 4 x 3 的 2 个过滤器以得到大小为 3 x 3 x 2 的卷积特征，对其应用激活函数以获取输出，这也称为特征图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/20565953-9ab1-4b49-bf97-21029490f608)

#### 1.6 池化层

- 在 CNN 中，通常先经过 卷积层，然后再经过 池化层
- 池化层通过下采样减小特征图尺寸，提高计算效率和特征鲁棒性，同时实现平移不变性（ 指模型对输入图像中目标物体位置的小范围移动不敏感 ）
- 池化层主要有三个超参数
  - f
    - 池化区域大小为 f \* f
    - 由于池化层过快地减少了数据的大小，目前的趋势是使用较小的池化区域大小，通常使用 (f = 2, s = 2)，偶尔也有人采用 (f = 3, s = 2)
  - s
    - 步幅，控制池化区域滑动距离
  - 类型
    - 最大池化（ 取区域最大值 ）或 平均池化（ 取区域平均值 ）
    - 因 最大池化 保留显著特征，目前更常用
- 全局平均池化
  - 原理
    - 对每个输入通道的整个特征图取平均值，将多个特征图（ 尺寸 $H \times W \times C$ ）压缩为一个向量（ 尺寸 $ 1 \times 1 \times C $ ）
  - 使用场景
    - 在 VGG、ResNet、Inception、EfficientNet 等架构中都被使用
    - 例如，ResNet-50 在卷积层后使用全局平均池化，将特征图压缩为向量，再接一个小的全连接层（ 或直接 softmax ），用于最后阶段的 降维 和 分类

#### 1.7 深度卷积神经网络（ DCNN ）

- 结构
  - 通过多层 卷积 和 池化 来提取图像中的特征，会导致 特征图尺寸 和 通道数 变化
    - 特征图尺寸变小：因为 池化层 和 步长大于 1 的卷积层
    - 通道数增加：因为深层需要学习高级语义特征，所以 卷积层 的滤波器数量增加
  - 在末尾通过全连接层进行分类或回归任务
- 基本原理
  - 特征提取：早期层提取低级特征（ 如边缘 ），深层提取高级特征（ 如对象形状 ）
  - 分类/回归：全连接层将特征映射为类别概率或回归值
  - 训练：通过反向传播和优化算法（ 如 Adam、SGD ）最小化损失函数（ 如交叉熵 ）
- 典型架构
  - VGG：堆叠多个 3×3 卷积层，增加网络深度
  - ResNet：引入残差连接，解决梯度消失问题
  - Inception：多尺度卷积并行处理，增强特征提取效率

## 2. VGG16

#### 2.1 基础介绍

- VGG16 是由牛津大学 Visual Geometry Group 在 2014 年提出的深度卷积神经网络
- 核心设计原则
  - 全部使用 3×3 卷积核，替代传统的大卷积核（ 如 7×7、5×5 ）
  - 通过堆叠小尺寸卷积核来构建深度网络，证明网络深度对性能的重要性
- 优势
  - 参数更少：两个 3×3 卷积核参数量为 $2×(3^2)=18$，一个 5×5 卷积核参数量为 $25$
  - 更多非线性：两个 3×3 卷积接两个 ReLU，比一个 5×5 卷积多一次非线性变换
  - 相同感受野：两个 3×3 卷积堆叠的感受野相当于一个 5×5 卷积

#### 2.2 网络结构

- VGG16 共包含 16 层（ 特指具有权重参数的层，不包括池化层和激活函数层 ），其中 13 个卷积层和 3 个全连接层
- 卷积部分
  - VGG16 的卷积层部分由五个卷积块组成，每个卷积块包含若干卷积层，后接一个最大池化层
  - 例如 224×224×3 输入图像
    - 第一个卷积块
      - 2 个卷积层，每层 64 个滤波器，卷积核大小 3x3，步长 1，填充 1，后接一个 2x2 最大池化层，步长 2
      - 结构：Conv3-64 → ReLU → Conv3-64 → ReLU → MaxPool2×2
      - 输出：112×112×64
    - 第二个卷积块
      - 2 个卷积层，每层 128 个滤波器，卷积核大小 3x3，步长 1，填充 1，后接一个 2x2 最大池化层，步长 2
      - 结构：Conv3-128 → ReLU → Conv3-128 → ReLU → MaxPool2×2
      - 输出：56×56×128
    - 第三个卷积块
      - 3 个卷积层，每层 256 个滤波器，卷积核大小 3x3，步长 1，填充 1，后接一个 2x2 最大池化层，步长 2
      - 结构：Conv3-256 → ReLU → Conv3-256 → ReLU → Conv3-256 → ReLU → MaxPool2×2
      - 输出：28×28×256
    - 第四个卷积块
      - 3 个卷积层，每层 512 个滤波器，卷积核大小 3x3，步长 1，填充 1，后接一个 2x2 最大池化层，步长 2
      - 结构：Conv3-512 → ReLU → Conv3-512 → ReLU → Conv3-512 → ReLU → MaxPool2×2
      - 输出：14×14×512
    - 第五个卷积块
      - 3 个卷积层，每层 512 个滤波器，卷积核大小 3x3，步长 1，填充 1，后接一个 2x2 最大池化层，步长 2
      - 结构：Conv3-512 → ReLU → Conv3-512 → ReLU → Conv3-512 → ReLU → MaxPool2×2
      - 输出：7×7×512
- 全连接层
  - 先将第五个卷积块输出的 7×7×512 特征图展平为一个向量
  - nn.Linear(512 \* 7 \* 7, 4096) → ReLU → Dropout(0.5)
  - nn.Linear(4096, 4096) → ReLU → Dropout(0.5)
  - nn.Linear(4096, num_classes) → Softmax（ 对应 ImageNet 的类别数 ）

#### 2.3 参数设置

- 卷积部分
  - 卷积核大小：统一使用 3×3
  - 步长：固定为 1 像素
  - 填充：使用 1 像素填充，保持特征图尺寸不变
  - 池化：使用 2×2 最大池化，步长为 2，每次池化尺寸减半
  - 通道数：每次池化后通道数翻倍，直到达到 512 后保持稳定，3 → 64 → 128 → 256 → 512 → 512
- 全连接层部分
  - 3 个全连接层，前两个各有 4096 个神经元，最后输出 num_classes 个神经元对应 ImageNet 的类别数
  - 使用 Dropout(0.5) 防止过拟合

## 3. ResNet（ Residual Neural Network，残差网络 ）

#### 3.1 基础介绍

- ResNet 是由微软研究院的何恺明等人在 2015 年提出的深度卷积神经网络
- 在传统 CNN 中，随着网络深度增加，会出现两个问题，导致深层网络性能反而下降
  - 梯度消失/爆炸：通过 批归一化 可以有效缓解
  - **网络退化**：通过 **残差学习** 让网络能够学习 **残差映射** 而非直接映射
- 残差学习机制
  - **残差块** 是残差学习的核心实现载体，而 **跳跃连接** 是构成残差块的关键组件
  - 数学原理
    - 直接映射：直接学习目标映射 $H(x)$
    - 残差映射：学习残差映射 $F(x) = H(x) - x$，然后输出 $H(x) = F(x) + x$
  - 优势
    - 当最优映射接近恒等映射时，学习残差 $F(x) \rightarrow 0$ 比学习 $H(x) \rightarrow x$ 更容易

#### 3.2 跳跃连接

- 核心思想
  - 在网络层之间引入 **直接连接**，将前层输出直接传递到后层的输出端
  - 提供绕过非线性变换的路径，使网络输出同时包含
    - 经过非线性变换的深层特征 F(x)
    - 未经处理的原始输入特征 x
  - 确保网络在特征变换过程中能够保留重要的基础信息
- 实现机制
  - 没有跳跃连接
    - 网络的计算图就是
      > 输入 x → 权重层 → 输出 F(x)
    - 网络学习：完整的映射函数 F(x)
  - 有了跳跃连接
    - 网络的计算图才是
      > 输入 x → 权重层 → F(x)
      > 输入 x ———（ 跳跃连接 ）——→ [+ x]
    - 网络学习：残差映射 F(x) = H(x) - x
- 核心作用
  - 重构优化目标
    - 将学习完整映射 H(x) 转换为学习残差 F(x) = H(x) - x
    - 简化了学习任务，网络只需关注输入与输出之间的差异，而非从零学习完整映射
  - 保障梯度流动
    - 提供梯度反向传播的直达路径，有效缓解深度网络中的梯度消失问题
- 细节处理
  - 维度匹配：当输入输出通道数不同时，使用 1×1 卷积调整维度
  - 下采样：在残差块中使用步长 2 的卷积进行空间下采样

#### 3.3 残差块

- 基本残差块（ 用于 ResNet-18/34 ）
  - 输入 x
  - 卷积层 1：3×3, 64 滤波器 → 批归一化 → ReLU
  - 卷积层 2：3×3, 64 滤波器 → 批归一化
  - 与输入 x 相加（ 跳跃连接 ）→ ReLU
  - 输出
- 瓶颈残差块（ 用于 ResNet-50/101/152 ）
  - 输入 x
  - 卷积层 1：1×1, 64 滤波器 → 批归一化 → ReLU（ 降维 ）
  - 卷积层 2：3×3, 64 滤波器 → 批归一化 → ReLU
  - 卷积层 3：1×1, 256 滤波器 → 批归一化（ 升维 ）
  - 与 1×1 卷积调整后的 x 相加 → ReLU
  - 输出
- 像这样的残差块（ 以基本残差块举例 ）堆叠起来的深度网络就是残差网络
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3a033ae9-d708-4e5d-ae87-f6a77ebb5e10)

#### 3.4 网络架构变体

- ResNet-18
  - 层数 18，残差块类型是 基本块，参数量约 11.7M
  - 轻量级，适合移动端
- ResNet-34
  - 层数 34，残差块类型是 基本块，参数量约 21.8M
  - 中等深度，平衡性能
- ResNet-50
  - 层数 50，残差块类型是 瓶颈块，参数量约 25.6M
  - 经典版本，广泛使用
- ResNet-101
  - 层数 101，残差块类型是 瓶颈块，参数量约 44.5M
  - 更深，性能更好
- ResNet-152
  - 层数 152，残差块类型是 瓶颈块，参数量约 60.2M
  - 极深，计算量大

#### 3.5 ResNet-50 详细结构

- 输入：224×224×3
- 卷积层 1：7×7, 64 滤波器，步长 2 → 批归一化 → ReLU
- 最大池化：3×3，步长 2
- 卷积块 1（ 3 个瓶颈块 ）
  - 输出：56×56×256
- 卷积块 2（ 4 个瓶颈块 ）
  - 第一个块下采样，输出：28×28×512
- 卷积块 3（ 6 个瓶颈块 ）
  - 第一个块下采样，输出：14×14×1024
- 卷积块 4（ 3 个瓶颈块 ）
  - 第一个块下采样，输出：7×7×2048
- 全局平均池化 → 全连接层 → Softmax
