## 1. UNet

- UNet 模型是与 FCN 同年 2015 年提出来的，网络支持端到端训练，可以算是医学图像分割领域的领头者
- 命名由来：网络结构呈 U 形、对称设计，左侧为编码器（ 收缩路径 ），右侧为解码器（ 扩展路径 ），通过 跳跃连接 桥接两者
  ![image](https://github.com/user-attachments/assets/0da0cf91-8007-4309-bd37-01ea8511310f)

#### 1.1 与 FCN 的对比

- 相似点
  - 两者都采用 Encoder-Decoder 结构，通过下采样提取特征，上采样恢复分辨率，并使用跳跃连接融合多级特征
- 不同点
  - FCN 使用 相加 融合特征
    - 适合一般语义分割
  - UNet 使用 通道拼接 融合特征
    - 形成更厚的特征图，保留更多细节信息，但会消耗更多显存
    - 更注重低级特征的保留，适用于数据量小、细节丰富的医疗图像

#### 1.2 编码器（ 收缩路径 ）

- 核心
  - 重复应用卷积和最大池化操作，提取多尺度特征
- 典型结构（ 原论文 ）
  - 输入图像尺寸为 572×572×1（ 灰度医疗图像 ）
  - 经过 4 次下采样，每次 2×2 最大池化，步幅 2
- 示例过程（ 假设输入 224×224×3 ）
  - 初始块：两个 3×3 卷积 + ReLU，通道数翻倍（ 例如从 3 到 64 ）
  - 下采样 4 次：尺寸依次变为 112×112、56×56、28×28、14×14，通道数逐步增加 64 → 128 → 256 → 512 → 1024
- 输出
  - 多级特征图，底部为高语义、低分辨率的瓶颈特征

#### 1.3 解码器（ 扩展路径 ）

- 核心
  - 通过 上采样 恢复分辨率，并通过 跳跃连接 融合编码器的对应特征
- 上采样
  - 主要使用转置卷积，步幅 2，实现 2 倍放大
  - 优点
    - 可学习参数，优化细节重建
    - 相比双线性插值，更好保留语义信息
- 跳转连接
  - 下采样导致细节丢失，跳跃连接通过拼接 浅层（ 高分辨率、细节丰富 ）和 深层（ 高语义 ）特征，实现信息互补
  - 实现
    - 每个解码器层与对应编码器层拼接（ 通道维堆叠 ），然后卷积融合
    - 注意，拼接前需裁剪编码器特征，以匹配上采样后的尺寸（ 原论文中由于 padding 导致边缘差异 ）
  - 优势
    - 相比 FCN 的相加，拼接保留原始特征维度，提升分割边界精度
- 示例过程
  - 从瓶颈 14×14×1024 开始，上采样（ 转置卷积 ）到 28×28×512
  - 与编码器的 28×28×512 特征拼接，得到 28×28×1024
  - 应用两个 3×3 卷积 + ReLU，减少通道数（ 例如到 512 ）
  - 重复上采样、拼接、卷积过程 4 次，最终输出 224×224×N（ N 为类别数 ）
- 最终层
  - 1×1 卷积 + softmax 或 sigmoid，生成像素级概率图

#### 1.4 损失函数

- 常用逐像素交叉熵损失（ 类似于 FCN ）
  $$ \mathcal{L} = -\sum*{i,j} \sum*{k=1}^{N} y*{i,j,k} \log(\hat{y}*{i,j,k}) $$
- 在医疗分割中，常结合 Dice 损失以处理类别不平衡
  $$ \text{Dice} = \frac{2 |X \cap Y|}{|X| + |Y|} $$
  - 其中 $X$ 和 $Y$ 分别为预测和真实分割区域

#### 1.5 医疗图像分割场景

- 大多数医疗影像语义分割任务都会首先用 UNet 作为 baseline，原因包括
  - 语义简单、结构固定
    - 医疗图像（ 如 CT、MRI ）语义单一，所有特征（ 如边缘、纹理 ）都很重要，无需过滤无关内容（ 如自动驾驶中的背景干扰 ）
    - U 型结构的跳跃连接能有效融合低级细节和高语义信息，适合低级特征和高级语义特征都很重要的场景
  - 数据量小
    - 大型网络的优点是更强的图像表述能力，而较为简单、数量少的医学影像并没有那么多的内容需要表述，因此分割的 SOTA 模型相比轻量的 UNet 并没有什么优势
    - 医学数据获取难，样本常少于 1000。因此如果使用大型的网络例如 DeepLabv3+ 等模型，很容易过拟合，而 UNet 结构轻量（ 无全连接层 ），不易过拟合
  - 多模态支持
    - 医学影像往往是多模态的，比方说 ISLES 脑梗竞赛中，官方提供了 CBF，MTT，CBV 等多种模态的数据
    - UNet 的轻量简单结构便于自定义修改，便于自己设计网络去提取不同的模态特征，如多输入分支提取不同模态特征
  - 可解释性强
    - 输出像素级分割，提供临床诊断所需的空间细节，便于医生临床诊断（ 例如 3D CT 中定位病因的层级位置 ），这远超分类任务
- 局限性
  - 对大分辨率图像显存消耗大，后续变体（ 如 UNet++、Attention UNet ）会进一步优化

## 2. UNet++

- UNet++ 是 2018 年提出的 UNet 改进版本，旨在解决 UNet 固定深度结构的适应性问题
- 核心创新：嵌套密集跳跃连接 + 深度监督机制
- 优势：通过自适应多尺度特征融合，在适度增加参数量（ 参数量增加 15-25% ）的同时实现显著的精度提升

#### 2.1 UNet 存在的问题

- 核心问题：UNet 固定 4 层下采样结构，但不同数据集的最优深度不同
  - 浅层网络：保留更多细节，适合简单边界分割
  - 深层网络：语义更强，适合复杂场景分类
  - 实验验证：训练 1-5 层不同深度的 UNet，发现没有统一的最优深度
- 传统方法：为每个数据集单独训练不同深度的 UNet，找到最优层数，耗时巨大

#### 2.2 网络结构

- 整体架构
  ![image](https://github.com/user-attachments/assets/5874a725-1b9b-47d0-bac0-75041fda218f)
- 节点
  - 每个节点 $X^{i,j}$ 是一个标准的卷积块，包含两个 3×3 卷积 + ReLU 激活函数
  - 命名规则：$X^{i,j}$
    - 上标 i：编码器层级（ 从上到下：0, 1, 2, 3, 4 ）
      - i = 0：最浅层编码器特征
      - i = 4：最深层（ 瓶颈 ）
    - 下标 j：该层级的上采样次数（ 从左到右：0, 1, 2, 3, 4 ）
      - j = 0：编码器原始特征
      - j = 1, 2, 3, 4：连续上采样次数
    - 示例：$X^{2,1}$ 即第 1 次上采样结果的第 2 层编码器
- 节点计算规则
  - $X^{i,0}$：直接来自编码器的特征
  - $X^{i,j}$（ j > 0 ）
    - 由 $X^{i,0}$ 和上层解码特征 $X^{i+1,j-1}$ 计算得到
    - 当 j > 1 时，还会融合同层的前序节点特征
- 最终输出
  - 训练时：$X^{0,1}, X^{0,2}, X^{0,3}, X^{0,4}$ 都产生输出用于深度监督
  - 推理时：全部输出求平均，使用剪枝后最优路径 或 最优单个输出

#### 2.3 UNet++ 核心

- 设计理念
  - 将 1 ～ 4 层 UNet 全部嵌套连接，网络自动学习各层重要性
  - 一个 Encoder + 多个 Decoder 路径，共享特征提取器，且 Encoder 可以灵活的用各种不同的 CNN 骨架来代替
- 关键特性
  - 嵌套连接：解码器间相互连接，渐进式特征融合
  - 密集跳跃：所有编码器层连接所有解码器，多尺度信息最大化
  - 重塑跳跃：密集连接后用 1×1 卷积压缩减少参数、加速推理

#### 2.4 剪枝推理

- 剪枝推理过程
  - 训练完成后，分析各分支在验证集上的性能
  - 移除性能较差的分支，只保留最优路径
- 剪枝后的网络
  - 参数量大幅减少
  - 推理速度提升
  - 保持或轻微降低精度

#### 2.5 深度监督

- 在每个解码路径的末端（ $X^{0,1}, X^{0,2}, X^{0,3}, X^{0,4}$ ）都添加 1×1 卷积 + sigmoid/softmax，训练时同时计算所有输出的损失，进行深度监督
- 推理时可以选择
  - 全部输出求平均，即集成效果（ 论文标准做法 ）
  - 选择验证集性能最好的单个输出
  - 剪枝后使用最优路径输出
- 优势
  - 缓解梯度消失，加速收敛
  - 相当于同时训练了 4 个不同深度的 UNet，共享编码器参数
  - 提供隐式的模型集成，提升泛化能力

## 3. UNet3+

- UNet3+ 是 2020 年提出的 UNet 改进版本，针对 UNet++ 的复杂性进行了优化
- 核心创新：全尺度跳跃连接 + 改进的深度监督机制
- 优势：在减少参数量的同时提升特征利用效率，实现更好的性能与参数量的平衡

#### 3.1 UNet3+ 设计动机

- UNet++ 的局限性
  - 嵌套密集连接 导致网络复杂，参数量大
  - 多尺度特征融合 存在信息冗余，信号处理可能过度
  - 不同尺度特征间的直接交互不足
- UNet3+ 的解决方案
  - 全尺度跳跃连接：每个解码器层直接融合所有编码器层的特征
  - 精简的深度监督：在多个尺度进行监督，避免过度复杂化
  - 参数效率：相比 UNet++ 减少 15-30% 参数量，同时保持或提升精度

#### 3.2 网络架构

- 整体结构
  - 保持 U 型基本架构，但跳跃连接重新设计
  - 编码器：5 个下采样阶段（ E1-E5 ），通道数逐步增加
  - 解码器：5 个上采样阶段（ D1-D5 ），每个阶段融合全尺度特征
- 相比于 UNet，UNet++ 的结构变化
  ![image](https://github.com/user-attachments/assets/ff1a1f76-f0a8-44ed-96e6-7ae920e56edb)

#### 3.3 全尺寸跳跃连接

- 下图详细说明了构造 $X_{De}^{3}$ 特征图的全过程，特征图 $X_{De}^{3}$ 的全尺寸连接主要是来自三个部分：较小尺度编码器，相同尺度编码器，较大尺度解码器
  ![image](https://github.com/user-attachments/assets/3b0cb347-dfb3-47b8-8e1d-f9c7e1330906)
- 较小尺度编码器特征（ $X_{En}^1$ 和 $X_{En}^2$ ）
  - 保留细节信息，即融合细粒度语义
  - $X_{En}^1$ 和 $X_{En}^2$ 分别下采样（ 无重叠最大池化 ）4 倍 和 2 倍，以统一特征图尺寸。随后进行 3×3 卷积，输出 64 通道
- 相同尺度编码器特征（ $X_{En}^3$ ）
  - 当前尺度的直接特征
  - 无需尺寸调整，直接进行 3×3 卷积，输出 64 通道
- 较大尺度解码器特征（ $X_{Dn}^4$ 和 $X_{En}^5$ ）
  - 融合了粗粒度语义信息
  - $X_{Dn}^4$ 和 $X_{En}^5$ 分别上采样（ 双线性插值 ）2 倍 和 4 倍，以统一特征图尺寸。随后进行 3×3 卷积，输出 64 通道
- 拼接融合
  - 五层叠加形成 320 通道的特征图
  - 随后再进行 320 通道的 3\*3 卷积、BN、Relu 等操作形成新的特征图 $X_{De}^{3}$，其他解码部分的特征图生成过程类似

#### 3.4 全尺寸深监督

- UNet++ 深监督部分
  - UNet++ 是对第一层的特征图进行深监督，即对全分辨率特征图进行深监督，$X_{De}^{1,1}$，$X_{De}^{1,2}$，$X_{De}^{1,3}$，$X_{De}^{1,4}$
  - 在实际操作中 UNet++ 使用 1 \* 1 卷积分别对 $X_{De}^{1,1}$，$X_{De}^{1,2}$，$X_{De}^{1,3}$，$X_{De}^{1,4}$ 进行操作，去监督每个分支的输出
- UNet3+ 深监督部分
  - UNet3+ 全尺寸深监督是每个解码器对应一个侧输出（ side output ），通过 ground truth 进行监督
  - 为了实现深度监控，每个解码器的最后一层被送入一个普通的 3 × 3 卷积层，然后是一个双线性上采样和一个 sigmoid 函数
  - 此处进行双线性上采样的目的主要有两个
    - 上采样是将第 2、3、4、5 层扩展成全分辨率特征图，保证与第一层相同，这也是全尺寸深监督的关键操作
    - 双线性上采样的方式可以最大限度保证上采样过程中边缘信息的完整性（ 医学图像边缘的不确定性决定要尽量保障边缘信息不丢失 ）

#### 3.5 UNet、UNet++ 和 UNet3+ 参数数量计算与比较

- 通过公式表示全尺寸跳跃连接，i 表示沿着编码的方向第 i 个下采样层，N 表示编码器个数，那么特征图 $X_{De}^{i}$ 的计算公式如下
  $$
  X_{De}^i =
  \begin{cases}
  X_{En}^i, & \\
  \mathcal{H}\left(\underbrace{\left[C\left(\mathcal{D}\left(X_{En}^k\right)\right)_{k=1}^{i-1}, C\left(X_{En}^i\right)\right]}_{\text{Scales: } 1^{th} \sim i^{th}}, \underbrace{\left[C\left(\mathcal{U}\left(X_{De}^k\right)\right)_{k=i+1}^N\right]}_{\text{Scales: } (i+1)^{th} \sim N^{th}}\right), & i = 1, \cdots, N-1
  \end{cases}
  $$
- UNet、UNet++和 UNet3+编码器的结构三者都是一样的，$X_{En}^{i}$ 都为 32 × $2^i$ 通道数，编码部分的参数都是一样多的，他们的不同主要是体现在解码部分
- UNet 解码部分
  $$
  \begin{aligned}
  P_{U-De}^i = D_F \times D_F &\times \Big[ d\left( X^{i+1}_{De} \right) \times d\left( X^i_{De} \right) + d\left( X^i_{De} \right)^2 \\
  &+ d\left( X^i_{En} + X^i_{De} \right) \times d\left( X^i_{De} \right)\Big]
  \end{aligned}
  $$
  - UNet 的解码部分和编码部分是对称的，因此 $X_{De}^{i}$ 都为 32 × $2^i$ 通道
- UNet++ 解码部分
  $$
  \begin{aligned}
  P_{U^{++}-De}^{i} = D_F \times D_F \times \Bigg[ & d\left( X_{De}^{i+1} \right) \times d\left( X_{De}^{i} \right) + d\left( X_{De}^{i} \right)^2 + \\
  & d\left( X_{En}^{i} + \sum_{k=1}^{N-1-i} X_{Me}^{i,k} + X_{De}^{i} \right) \times d\left( X_{De}^{i} \right) \Bigg]
  \end{aligned}
  $$
  - 在 UNet++ 中, 它在每一条跳跃路径上都利用了稠密卷积模块
- UNet3+ 解码部分
  $$
  P_{u^{3+}-De}^i = D_F \times D_F \times \Bigg[ \left( \sum_{k=1}^{i} d(X_{En}^k) + \sum_{k=i+1}^{N} d(X_{De}^k) \right) \times 64 + d(X_{De}^i)^2 \Bigg]
  $$
  - 在 UNet3+中, 每一个解码器由 N 个尺度连接所成, 所以产生 64 × N 通道
- 通过公式可以看出，虽然从网络结构上看，UNet 最为清晰明了，貌似参数应该更少，其实并不是这样。在保障相同的编码部分的前提下，它们三者中 UNet3+ 的参数量最少，其次才是 UNet，UNet++ 的参数量是最多的（ 结构也最复杂 ）
