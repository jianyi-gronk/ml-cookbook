## 1. UNet

- 由一个编码器和一个解码器组成，前者生成图像的表示，后者使用该表示来构建分割。每个空间分辨率的两个映射连接在一起（ 灰色箭头 ），因此可以将图像的两种不同表示组合在一起
- 这个结构就是先对图片进行卷积和池化，在 UNet 论文中是池化 4 次，比如说一开始的图片是 224 \* 224 的，那么就会变成 112 \* 112，56 \* 56，28 \* 28，14 \* 14 四个不同尺寸的特征。然后我们对 14 \* 14 的特征图做上采样，得到 28 \* 28 的特征图，这个 28 \* 28 的特征图与之前的 28 \* 28 的特征图进行通道伤的拼接 concat，然后再对拼接之后的特征图做卷积和上采样，得到 56 \* 56 的特征图，再与之前的 56 \* 56 的特征拼接，卷积，再上采样，经过四次上采样可以得到一个与输入图像尺寸相同的 224 \* 224 的预测结果

- UNet 模型是与 FCN 同年 2015 年提出来的，UNet 模型可以算是医学图像分割领域的领头者
- 命名由来：网络结构呈 U 形、对称设计，左侧为收缩路径（ 编码器 ），右侧为扩展路径（ 解码器 ）
  ![image](https://github.com/user-attachments/assets/0da0cf91-8007-4309-bd37-01ea8511310f)

#### 2.1 与 FCN 的对比

- 相似点
  - 两者都采用 Encoder-Decoder 结构，通过下采样提取特征，上采样恢复分辨率，并使用跳跃连接融合多级特征
- 不同点
  - FCN 使用 相加 融合特征
    - 适合一般语义分割
  - UNet 使用 通道拼接 融合特征
    - 形成更厚的特征图，保留更多细节信息，但会消耗更多显存
    - 更注重低级特征的保留，适用于数据量小、细节丰富的医疗图像

#### 2.3 上采样和跳跃连接

- 上采样方法：
  - 主要使用转置卷积，步幅 2，实现 2 倍放大
  - 优点：可学习参数，优化细节重建；相比双线性插值，更好保留语义信息。
- 跳跃连接（Skip Connections）：
  - 必要性：下采样导致细节丢失，跳跃连接通过拼接浅层（高分辨率、细节丰富）和深层（高语义）特征，实现信息互补
  - 实现：每个解码器层与对应编码器层拼接（通道维堆叠），然后卷积融合
  - 优势：相比 FCN 的相加，拼接保留原始特征维度，提升分割边界精度。
- 注意事项：
  - 拼接前需裁剪（crop）编码器特征，以匹配上采样后的尺寸（原论文中由于 padding 导致边缘差异）
  - 总参数量适中（约 30M），适合小数据集训练。

#### 2.4 医疗图像分割场景

- 大多数医疗影像语义分割任务都会首先用 UNet 作为 baseline，原因包括
  - 语义简单、结构固定
    - 医疗图像（ 如 CT、MRI ）语义单一，所有特征（ 如边缘、纹理 ）都很重要，无需过滤无关内容（ 如自动驾驶中的背景干扰 ）
    - U 型结构的跳跃连接能有效融合低级细节和高语义信息，适合低级特征和高级语义特征都很重要的场景
  - 数据量小
    - 大型网络的优点是更强的图像表述能力，而较为简单、数量少的医学影像并没有那么多的内容需要表述，因此分割的 SOTA 模型相比轻量的 UNet 并没有什么优势
    - 医学数据获取难，样本常少于 1000。因此如果使用大型的网络例如 DeepLabv3+ 等模型，很容易过拟合，而 UNet 结构轻量（ 无全连接层 ），不易过拟合
  - 多模态支持
    - 医学影像往往是多模态的，比方说 ISLES 脑梗竞赛中，官方提供了 CBF，MTT，CBV 等多种模态的数据
    - UNet 的轻量简单结构便于自定义修改，便于自己设计网络去提取不同的模态特征，如多输入分支提取不同模态特征
  - 可解释性强
    - 输出像素级分割，提供临床诊断所需的空间细节，便于医生临床诊断（ 例如 3D CT 中定位病因的层级位置 ），这远超分类任务
- 局限性
  - 对大分辨率图像显存消耗大，后续变体（ 如 UNet++、Attention UNet ）进一步优化

## 2. UNet++

- UNet 存在问题，既然 UNet 每一层抓取的特征都很重要，为什么非要降四次之后才开始上采样回去呢，这个 “四” 真的适用所有场景吗
- 为了验证多深才好，于是每加一个深度就训练一个网络。实验证明不是越深越好，即不同层次特征的重要性对于不同的数据集是不一样的，并不是说设计一个原论文给出的那个四层结构，就一定对所有数据集的分割问题都最优，但是总不能把所有不同深度的 UNet 都训练一遍，太耗时间了，于是提出 UNet++
- UNet++ 是对 UNet 体系结构的改进，它有多个跳跃连接，可以抓取不同层次的特征，将它们通过特征叠加的方式整合，加入更浅的 UNet 结构，使得融合时的特征图尺度差异更小，同时也引进了很多参数，占用内存也变大
- 结构如下，采用了嵌套和密集跳过连接的网络结构，把 1 ～ 4 层的 Unet 全给连一起了，它的子集包含 1 层 UNet，2 层 UNet，以此类推
  ![image](https://github.com/user-attachments/assets/5874a725-1b9b-47d0-bac0-75041fda218f)
  - 第一个好处是不管哪个深度的特征有效，干脆都给用上，让网络自己去学习不同深度的特征的重要性
  - 第二个好处是它共享了一个特征提取器，也就是不需要训练一堆 Unet，而是只训练一个 encoder，它的不同层次的特征由不同的 decoder 路径来还原。这个 encoder 依旧可以灵活的用各种不同的 backbone 来代替

## 3. UNet3+

- UNet++ 虽然名义上通过嵌套和密集跳过连接进行了多尺度信息的利用，但是从本质上看基本都是短连接，基本上都对解码特征进行了再次处理，再加上各个连接的融合，多尺度信息的原始特征几乎没有得到特别好的利用，信号处理有些矫枉过正或是丢失
- UNet3+ 利用了全尺度的跳跃连接（ skip connection ）和深度监督（ deep supervisions ），并且 UNet3+ 的参数量明显小于 UNet++
  ![image](https://github.com/user-attachments/assets/ff1a1f76-f0a8-44ed-96e6-7ae920e56edb)
  - 全尺度的跳跃连接把来自不同尺度特征图中的高级语义与低级语义直接结合（ 当然需要必要的上采样操作 ）
  - 深度监督则从多尺度聚合的特征图中学习层次表示。虽然 UNet++ 和 UNet3+ 都用到了深度监督，但是监督的位置是完全不一样的

#### 3.1 全尺寸跳跃连接

- 下图详细说明了构造 $X_{De}^{3}$ 特征图的全过程，特征图 $X_{De}^{3}$ 的全尺寸连接主要是来自三个部分
  ![image](https://github.com/user-attachments/assets/3b0cb347-dfb3-47b8-8e1d-f9c7e1330906)
  ![image](https://github.com/user-attachments/assets/ac794506-d8b8-482d-9b1a-8e90bf08c9ef)
  - 五层叠加（ 拼接融合 ）形成 320 通道的特征图。随后再进行 320 通道的 3\*3 卷积、BN、Relu 等操作形成新的特征图 $X_{De}^{3}$，其他解码部分的特征图生成过程类似

#### 3.2 全尺寸深监督

- UNet++ 深监督部分
  - UNet++ 是对第一层的特征图进行深监督，即对全分辨率特征图进行深监督，$X_{De}^{1,1}$，$X_{De}^{1,2}$，$X_{De}^{1,3}$，$X_{De}^{1,4}$，在实际操作中 UNet++使用 1\*1 卷积分别对 $X_{De}^{1,1}$，$X_{De}^{1,2}$，$X_{De}^{1,3}$，$X_{De}^{1,4}$ 进行操作，去监督每个分支的输出
- UNet3+ 深监督部分
  - UNet3+ 全尺寸深监督是每个解码器对应一个侧输出（ side output ），通过 ground truth 进行监督。为了实现深度监控，每个解码器的最后一层被送入一个普通的 3 × 3 卷积层，然后是一个双线性上采样和一个 sigmoid 函数
  - 此处进行双线性上采样的目的我认为主要有两个：
    - 上采样是将第 2、3、4、5 层扩展成全分辨率特征图，保证与第一层相同，这也是全尺寸深监督的关键操作
    - 双线性上采样的方式可以最大限度保证上采样过程中边缘信息的完整性（ 医学图像边缘的不确定性决定要尽量保障边缘信息不丢失 ）

#### 3.3 UNet、UNet++ 和 UNet3+ 参数数量计算与比较

- 通过公式表示全尺寸跳跃连接，i 表示沿着编码的方向第 i 个下采样层，N 表示编码器个数，那么特征图 $X_{De}^{i}$ 的计算公式如下
  ![image](https://github.com/user-attachments/assets/8e55d474-0770-4c34-8878-35367f2ad1ac)
- UNet、UNet++和 UNet3+编码器的结构三者都是一样的，$X_{En}^{i}$ 都为 32 × $2^i$ 通道数，编码部分的参数都是一样多的，他们的不同主要是体现在解码部分
- UNet 解码部分
  ![image](https://github.com/user-attachments/assets/1259b4a1-0ee4-4396-b782-36fb2b2bd45f)
  - UNet 的解码部分和编码部分是对称的，因此 $X_{De}^{i}$ 都为 32 × $2^i$ 通道
- UNet++ 解码部分
  ![image](https://github.com/user-attachments/assets/982cf9ec-a46c-481d-9948-6c524244bbbb)
  - 在 UNet++ 中, 它在每一条跳跃路径上都利用了稠密卷积模块（ dense conv block ）
- UNet3+ 解码部分
  ![image](https://github.com/user-attachments/assets/fd060aa5-c4b3-4ede-a173-840dfa06eca7)
  - 在 UNet3+中, 每一个解码器由 N 个尺度连接所成, 所以产生 64 × N 通道
- 通过公式可以看出，虽然从网络结构上看，UNet 最为清晰明了，貌似参数应该更少，其实并不是这样。在保障相同的编码部分的前提下，它们三者中 UNet3+ 的参数量最少，其次才是 UNet，UNet++ 的参数量是最多的（ 结构也最复杂 ）
