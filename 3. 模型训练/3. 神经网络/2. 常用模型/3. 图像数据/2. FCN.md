## 1. FCN（ Fully Convolutional Networks，全卷积网络 ）

- 语义分割，本质就是按图像中物体表达的含义进行抠图，而 FCN 就是用于图像语义分割的框架
  ![image](https://github.com/user-attachments/assets/2d6f4959-f8a0-4afb-9318-5bf478cf98bd)

#### 1.1 与传统 CNN 对比

- 传统 CNN
  - 在卷积层之后会接上若干全连接层，将卷积层产生的 特征图 映射成固定长度的特征向量
  - 比如把卷积神经网络的最后一层设为 softmax 进行分类，得到整个输入图像的一个数值描述（ 概率 ）
  - 局限性
    - 全连接层参数量巨大，如 7×7×512 特征图展平后 → 25088 维 → 1000 类，需要 25M 参数
    - 固定输入尺寸要求，空间信息完全丢失
- FCN
  - 将传统 CNN 的最后的全连接层换成了卷积层，这样网络的输出是相同大小的热力图而非数值
    ![image](https://github.com/user-attachments/assets/5b5a5e2f-0a91-4024-ace6-9f08479c8f75)
  - 优势
    - 全卷积结构，参数量少
    - 支持任意尺寸输入，同时保留空间信息（ 输出每个像素的类别概率 ）
  - 存在两个问题需要解决
    - 如何解决卷积和池化导致图像尺寸的变小
    - 经过卷积和池化的特征图太小，这意味着过多细节的丢失，即使恢复原来大小的图片也丢失了过多信息

#### 1.2 网络结构

- FCN 网络结构分为 编码器 和 解码器，实现端到端训练
- 编码器（ 全卷积部分 ）
  - 核心：采用经典 CNN 骨干网络（ 如 VGG16、ResNet 等 ），通过卷积和池化提取多级特征
  - 输入：任意尺寸彩色图像，如 $H \times W \times 3$
  - 输出：多级特征图（ 如 pool3、pool4、pool5 ），尺寸逐步缩小
- 解码器（ 上采样部分 ）
  - 核心：通过 上采样 和 跳跃连接 得到原尺寸的语义分割图像
  - 输入：编码器输出的多级特征图
  - 输出：与输入尺寸相同的热力图（ $H \times W \times (N+1)$ ），即通道数为 N（ 目标类别数 ）+ 1（ 背景 ）
- 损失函数
  - 逐像素交叉熵损失是 FCN 的标准损失函数，支持端到端训练  
    $$\mathcal{L} = -\sum_{i,j} \sum_{k=1}^{N+1} y_{i,j,k} \log(\hat{y}_{i,j,k})$$
    - 其中，$y*{i,j,k}$ 是像素 $(i,j)$ 的真实类别标签，$\hat{y}*{i,j,k}$ 是预测概率
- 网络结构图
  ![image](https://github.com/user-attachments/assets/3971f239-1ebd-426b-b081-7fa24154086e)

#### 1.3 上采样

- 上采样首先将原始图像的尺寸进行放大，空出来很多需要补充的区域，然后通过算法来计算待补充的区域，从而实现图像的放大
- 核心作用
  - 尺寸恢复：将特征图从低分辨率恢复到高分辨率，匹配原始输入尺寸
  - 信息重建：在放大过程中重建空间细节，恢复像素级分类能力
  - 特征增强：通过可学习参数优化特征表示，提升分割精度
- 常用的上采样方法
  - 双线性插值
    - 原理：基于相邻 4 个像素点的距离加权平均
    - 公式
      $$
      f(x,y) \approx \frac{y_2-y}{y_2-y_1} \left( \frac{x_2-x}{x_2-x_1}f(Q_{11}) + \frac{x-x_1}{x_2-x_1}f(Q_{21}) \right) + \frac{y-y_1}{y_2-y_1} \left( \frac{x_2-x}{x_2-x_1}f(Q_{12}) + \frac{x-x_1}{x_2-x_1}f(Q_{22}) \right)
      $$
    - 特点：计算简单、速度快，但无法学习优化
  - 最近邻插值
    - 原理：选择距离目标位置最近的已知像素值作为插值结果
    - 计算过程
      - 对于目标位置 $(x,y)$，找到最近的整数坐标 $(round(x), round(y))$
      - 直接取该整数坐标处的像素值：$f(x,y) = f(round(x), round(y))$
    - 特点：计算简单、速度快，但无法学习优化，会产生明显的块状效应（ 马赛克效果 ）
  - 转置卷积
    - 原理：通过可学习的卷积核在特征图间插入零值并进行标准卷积，通过 步幅 控制放大倍数
    - 过程
      - 在输入特征图元素间插入零值，其中 步长 决定插入数量
      - 使用卷积核在扩展后的图上进行卷积操作
      - 输出尺寸
        $$H_{out} = (H_{in}-1) \times stride + kernel\_size - 2 \times padding$$
  - 反池化
    - 最大反池化
      - 池化时，记录最大值及其在原图中的位置坐标
      - 反池化时，根据记录的位置信息，将特征值精确放回原位置，其他位置补零
    - 平均反池化
      - 反池化时，将每个池化结果值均匀填充到对应的整个池化窗口区域

#### 1.4 跳跃结构

- 跳跃结构是 FCN 解决语义分割中细节丢失问题的关键创新，通过融合不同层级的特征信息来提升分割精度
- 必要性
  - 深层特征问题：网络越深，感受野越大，但空间细节信息丢失越严重
  - 浅层特征优势：浅层网络保留更多边缘、纹理等细节信息
  - 信息互补：深层提供语义信息，浅层提供空间细节，两者结合实现精确分割
- 网络架构基础
  - 在 FCN 中，输入图像通常会经过 5 次池化（ 每次缩小 2 倍 ），总下采样倍数为 $2^5 = 32$ 倍
  - 因此需要 32 倍上采样才能恢复到原始尺寸
- 在论文中提出了三个模型分别是 FCN-32s、FCN-16s、FCN-8s
  - FCN-32s
    - 将 pool5 的输出直接上采样 32 倍恢复到原图大小，将损失了原图很多细节信息的特征图直接上采样，效果较差
  - FCN-16s
    - pool5 的输出上采样 2 倍（ 采样后大小与 pool4 的输出相同 ），然后与之前 pool4 输出相加然后再直接上采样 16 倍恢复到原图大小
  - FCN-8s
    - pool5 的输出上采样 2 倍，然后与之前 pool4 输出相加然后再上采样 2 倍，然后与之前 pool3 输出相加然后再直接上采样 8 倍恢复到原图大小
    - 具体步骤如图
      ![image](https://github.com/user-attachments/assets/4b169b50-0016-4083-8788-daf544e9b5b0)
- 注意，不同层级的特征图尺寸和通道数不同，相加前，需要通过以下操作进行对齐
  - 上采样：调整特征图尺寸
  - 1×1 卷积：调整通道数，统一特征维度
  - 逐元素相加：融合不同层级的特征信息
- 在原文中给出 3 种网络结果对比，明显可以看出效果 FCN-32s < FCN-16s < FCN-8s，即使用多层 feature 融合有利于提高分割准确性，效果如下
  ![image](https://github.com/user-attachments/assets/358c6e76-a3d7-4009-8876-e04b8d4ae921)
