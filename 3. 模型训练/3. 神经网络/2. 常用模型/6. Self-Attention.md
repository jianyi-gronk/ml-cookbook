## 1. 自注意力机制（ Self-Attention ）

- 基于注意力机制的 Seq2Seq 模型依赖 RNN 及其变体，需串行计算，效率低，且存在梯度消失/爆炸问题
- 2017 年论文《Attention is All You Need》提出了 Transformer 模型，完全基于注意力机制（ 自注意力和跨注意力 ），取代 RNN/CNN，解决了并行处理和长距离依赖问题
- 自注意力机制在 Transformer 的 **编码器 和 解码器** 中都有使用，但形式不同
  - 编码器使用标准自注意力
  - 解码器使用掩码自注意力（ 防止关注未来 Token ）

#### 1.1 基础介绍

- 自注意力机制是一种特殊形式的注意力机制
  - 不依赖于外部序列（ 如 编码器-解码器 间的交互 ），而是 **序列内部的自我交互**
  - 序列的每个元素（ Token ）同时作为查询（ Query ）、键（ Key ）和值（ Value ）的来源
  - 关注序列内的所有元素（ 包括自身 ）的相关性，生成上下文相关的表示，捕捉序列内部的依赖关系，允许元素根据与其他元素的关系动态调整自身的表示
- 特点
  - 自注意力关注 **同一序列内部** 的 token 间关系
  - 所有 token 同时参与计算，允许 **并行化处理**，无需像 RNN 那样按顺序计算
  - 能捕捉 **长距离依赖**，因为每个 token 都可以直接访问序列中的任意其他 token

#### 1.2 核心概念

- Token
  - Token 是 **序列中的基本处理单元**，序列由多个 token 组成，表示为
    $$\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_{T_x}]$$
    - 在 NLP 中，token 通常是一个词、子词 subword 或字符
    - 在 计算机视觉 中，token 可以是图像块 patch 或其他特征单元
  - Token 通常通过嵌入层转换为 **固定维度** 的向量，并可能附加 **位置编码** 以捕捉序列顺序
  - 自注意力机制通过计算 token 之间的关系，生成每个 token 的新表示，融合序列中其他 token 的上下文信息
- Query（ 查询向量 ）
  - Query 是自注意力机制中用于 **表示某个 token “询问” 其他 token 的表示向量**
  - 可以将 $\mathbf{q}_i$ 看作 token $i$ 的 “搜索请求”，询问序列中哪些 token 与它当前的任务或语义最相关
  - 例如，在翻译任务中，token “I” 的查询向量 $\mathbf{q}_1$ 可能更关注与主语相关的 token（ 如 “love” ）
- Key（ 键向量 ）
  - Key 是自注意力机制中用于 **表示序列中每个 token 的 “标识” 或 “特征” 的向量**
  - 可以将 $\mathbf{k}_j$ 看作 token $j$ 的 “标签” 或 “索引”，用于匹配查询向量 $\mathbf{q}_i$ 的需求
  - 如果 $\mathbf{q}_i$ 和 $\mathbf{k}_j$ 的点积值较大，说明 token $i$ 和 token $j$ 的语义或功能高度相关
- Value（ 值向量 ）
  - Value 是自注意力机制中 **表示 token 的实际内容的向量**
  - 可以将 $\mathbf{v}_j$ 看作 token $j$ 的 “内容” 或 “信息包”，在注意力机制中根据权重 $\alpha_{i,j}$ 决定贡献多少信息给 token $i$ 的新表示，如果 $\alpha_{i,j}$ 较大，说明 token $j$ 的内容对 token $i$ 的表示很重要
- Query，Key，Value 都是通过对其嵌入向量 $\mathbf{x}_j$ 进行线性变换得到的
  $$
  \mathbf{q}_i = \mathbf{W}_Q \mathbf{x}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{x}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{x}_i
  $$
  - $\mathbf{W}_Q$，$\mathbf{W}_K$，$\mathbf{W}_V$ 都是可学习的权重矩阵

#### 1.3 计算过程

- 计算注意力分数
  $$
  e_{i,j} = \mathbf{q}_i \cdot \mathbf{k}_j
  $$
- 归一化得到注意力权重
  $$
  \alpha_{i,j} = \text{Softmax}\left(\frac{e_{i,j}}{\sqrt{d_k}}\right)
  $$
- 生成新表示
  $$
  \mathbf{z}_i = \sum_{j=1}^n \alpha_{i,j} \mathbf{v}_j
  $$
  - 每个输入 $\mathbf{x}_i$ 都会生成一个对应的上下文表示 $\mathbf{z}_i$
- 自注意力机制输出 $\mathbf{Z}$（ 缩放点积注意力 ）
  $$
  \mathbf{Z} = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}
  $$
  - 其中 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 是查询、键、值矩阵，$d_k$ 是键向量的维度
- Transformer 通常使用 **多头自注意力**，并行计算多个注意力头，拼接后线性变换
  $$
  \text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i), \quad \mathbf{Z} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) \mathbf{W}_O
  $$

## 2. 掩码自注意力（ Masked Self-Attention ）

#### 2.1 基础介绍

- 是自注意力机制的一种变体，主要用于 Transformer 解码器，以符合自回归生成的需求
  - 在解码器中，掩码自注意力用于处理目标序列（ 如翻译任务中的目标语言序列 ），确保生成 token 时只依赖当前和之前的上下文信息，模拟自回归生成过程
  - 例如，在翻译 “I love learning” 到 “我爱学习” 时，生成 “我” 时只关注 “我”，生成 “爱” 时关注 “我” 和 “爱”，以此类推
- 两者的具体区别如下
  - 在标准自注意力中，每个 token 可以关注序列中的所有 token（ 包括未来 token ）
  - 在掩码自注意力中，通过引入一个 **掩码矩阵**，防止当前 token 关注序列中的未来 token，确保每个 token 仅关注当前和之前的 token

#### 2.2 计算过程

- 输入目标序列 $\mathbf{Y} = [\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_{T_y}]$，$T_y$ 是目标序列长度
- 生成查询、键、值向量
  $$
  \mathbf{q}_i = \mathbf{W}_Q \mathbf{y}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{y}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{y}_i
  $$
- 计算注意力分数
  $$
  e_{i,j} = \mathbf{q}_i \cdot \mathbf{k}_j
  $$
- 应用 掩码矩阵 $\mathbf{M}$
  - $\mathbf{T_y} \in \mathbb{R}^{T_y \times T_y}$ 是一个下三角矩阵，$M_{i,j} = 0$（ 若 $j \leq i$，允许关注 ），$M_{i,j} = -\infty$（ 若 $j > i$，屏蔽未来 token ）
  - 在计算注意力分数后，添加掩码
    $$
    e_{i,j}' = e_{i,j} + M_{i,j}
    $$
  - 归一化得到注意力权重
    $$
    \alpha_{i,j} = \text{Softmax}\left(\frac{e_{i,j}'}{\sqrt{d_k}}\right)
    $$
    - 当 $j > i$ 时，$e_{i,j}' = -\infty$，Softmax 后 $\alpha_{i,j} = 0$，确保不关注未来 token。
- 生成新表示
  $$
  \mathbf{z}_i = \sum_{j=1}^i \alpha_{i,j} \mathbf{v}_j
  $$
  - 注意，仅对 $j \leq i$ 的 token 求和，忽略未来 token
- 矩阵形式
  $$
  \mathbf{Z} = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T + \mathbf{M}}{\sqrt{d_k}}\right) \mathbf{V}
  $$
  - $\mathbf{Z} \in \mathbb{R}^{T_y \times d}$，每行 $\mathbf{z}_i$ 是 $\mathbf{y}_i$ 的上下文表示，仅基于 $\mathbf{y}_1, \dots, \mathbf{y}_i$
- 与标准自注意力类似，解码器使用多头掩码自注意力
  $$
  \text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i, \mathbf{M}), \quad \mathbf{Z} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) \mathbf{W}_O
  $$
  - 每个注意力头独立应用掩码矩阵 $\mathbf{M}$
