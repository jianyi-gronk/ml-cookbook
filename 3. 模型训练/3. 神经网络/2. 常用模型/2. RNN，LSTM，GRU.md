## 1. RNN（ Recurrent Neural Network，循环神经网络 ）

- **RNN 主要用于处理序列数据，如文本、语音、时间序列等，与传统的前馈神经网络不同，RNN 具有循环连接，使得它可以在处理序列时保持一种记忆状态**

#### 1.1 循环单元

- RNN 的基本结构是循环单元，核心概念有 隐藏状态，输入 和 输出
- 隐藏状态
  - $\mathbf{a}^{\langle t \rangle}$ 是网络的 **记忆载体**，编码了从序列起点到当前时刻 $t$ 的全部历史信息，不过由于长序列中梯度消失问题，所以可能无法有效保留远期信息
  - 示例：处理句子 `"I love machine learning"` 时，`"learning"` 对应的 $\mathbf{a}^{\langle 4 \rangle}$ 隐含 `"I"`、`"love"`、`"machine"` 的语义，但长期依赖可能因梯度问题丢失
- 输入
  - 当前时刻 输入 $\mathbf{x}^{\langle t \rangle}$，如 第 t 个单词的向量
  - 上一时刻 隐藏状态 $\mathbf{a}^{\langle t-1 \rangle}$，承载历史信息
- 输出
  - 当前时刻 输出 $\mathbf{y}^{\langle t \rangle}$，如 分类概率
  - 当前时刻 隐藏状态 $\mathbf{a}^{\langle t \rangle}$，传递至下一时刻

#### 1.2 具体公式

- 数学表达
  $$
  \begin{align*}
  \mathbf{a}^{\langle t \rangle} &= f(\mathbf{W}_{aa} \mathbf{a}^{\langle t-1 \rangle} + \mathbf{W}_{ax} \mathbf{x}^{\langle t \rangle} + \mathbf{b}_a) \\
  \mathbf{y}^{\langle t \rangle} &= g(\mathbf{W}_{ya} \mathbf{a}^{\langle t \rangle} + \mathbf{b}_y)
  \end{align*}
  $$
  - f：隐藏层激活函数（ 常用 tanh 或 ReLU ）
  - g：输出层激活函数，如 Softmax 用于分类
  - $\mathbf{W}_{aa}, \mathbf{W}_{ax}, \mathbf{W}_{ya}$：权重矩阵，在所有时间步中共享
  - $\mathbf{b}_a, \mathbf{b}_y$：偏置向量

#### 1.3 参数共享

- 参数共享 是 RNN 的核心特性，指的是 **在处理序列数据时，所有时间步中始终使用同一组 $\mathbf{W}{aa}$、$\mathbf{W}{ax}$、$\mathbf{W}_{ya}$、$\mathbf{b}_a$、$\mathbf{b}_y$ 参数**，而不是为每个时间步单独定义一组参数。这大幅减少参数量，并支持处理变长序列
- 参数共享 **会假设序列中的每个时间步遵循相似的模式或规律**，即序列的动态变化遵循相似的规则，所以可以使用相同的参数，但若序列前后模式差异较大，则共享参数可能限制模型建模能力
- 关键作用
  - **减少参数量**
    - 参数共享让 RNN 只需一组固定的权重矩阵，无论序列长度是 10、100 还是 1000，参数数量保持不变，这大幅降低了模型的复杂度和存储需求
  - **支持变长序列**
    - 参数共享使 RNN 能处理任意长度的序列，因为权重矩阵是固定的，模型的计算逻辑不依赖于序列的具体长度
    - 例如，处理 "I love"（ 2 个单词 ）或 "I love machine learning"（ 4 个单词 ）时，RNN 使用相同的权重矩阵，只需根据序列长度调整时间步的循环次数
  - **捕捉序列规律**
    - 参数共享使用相同的权重矩阵，学习通用的模式
    - 例如，在自然语言处理中，句子的语义规律（ 如语法、上下文依赖 ）在不同时间步中是相似的

#### 1.4 存在问题

- 梯度消失 或 梯度爆炸
  - 长序列中梯度可能指数级衰减或增长，导致无法学习长期依赖，原因是共享的权重矩阵在反向传播中反复使用，梯度累积可能导致衰减或爆炸
  - 解决方案
    - 门控机制：LSTM 和 GRU 通过门控（ 如遗忘门、输入门 ）选择性地保留或丢弃历史信息，缓解梯度问题
    - 梯度裁剪：限制梯度最大值，防止爆炸
    - 残差连接：在 RNN 变体中（ 如深度 RNN ），添加跨时间步的残差连接，允许信息直接传递，类似 Skip Connections 的思路
- 计算效率
  - 由于状态依赖前一步，所以无法并行处理序列，计算效率较低
  - 解决方案
    - Transformer 架构，即基于自注意力机制，允许所有时间步并行计算，因为自注意力不依赖时间步的顺序，而是通过注意力权重动态建模序列关系

## 2. LSTM（ Long Short Term Memory，长短时记忆网络 ）

#### 2.1 RNN 存在的问题

- 最基础版本的 RNN，每一时刻的隐藏状态都不仅由该时刻的输入决定，还取决于上一时刻的隐藏层的值，但如果一个句子很长，到句子末尾时，它将记不住这个句子的开头的内容详细内容
- 具体来说，在训练 RNN 时，模型会逐个时间步处理每个单词。当模型处理到句子的末尾时，它的隐藏状态包含了整个句子的信息。然而，由于梯度消失的问题，模型可能在处理过程中忘记了句子开头的重要信息

#### 2.2 基础介绍

- LSTM 是 RNN 的变体，旨在解决传统 RNN 在处理长序列时的梯度消失和梯度爆炸问题
- LSTM 引入了一种特殊的存储单元和门控机制，以更有效地捕捉和处理序列数据中的长期依赖关系
- 在现实生活中，门就是用来控制进出的，门关上了，你就进不去房子了，门打开你就能进去，同理，这里的门是用来控制每一时刻信息记忆与遗忘的

#### 2.3 与 RNN 的区别

- RNN 每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息，把存每一时刻信息的地方叫做 Memory Cell
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/cd8d9dbd-b534-4100-8073-6cf0016cc8ce)
- LSTM 和普通 RNN 的区别在于
  - RNN 什么信息它都存下来，因为它没有挑选的能力
  - 而 LSTM 不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择
- 如下图，普通 RNN 只有中间的 Memory Cell 用来存所有的信息，而 LSTM 多了三个 Gate，也就是三个门
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ec225e68-548c-496d-87da-97a0a94b8148)
- 三个门分别作用
  - Input Gate（ 输入门 ）
    - 每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到 Memory Cell
  - Output Gate（ 输出门 ）
    - 每一时刻是否有信息从 Memory Cell 输出取决于这一道门
  - Forget Gate（ 遗忘门 ）
    - 每一时刻 Memory Cell 里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把 Memory Cell 里的值清除，也就是遗忘掉
  - 先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘 Memory Cell 里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出

#### 2.4 LSTM 结构

- LSTM 结构如下
  ![image](https://i.postimg.cc/qqCbD3yb/image.png)
- LSTM 里常用的激活函数有两个，一个是 tanh，一个是 sigmoid，下图符号代表一个激活函数
  ![image](https://i.postimg.cc/HWKdyD6P/image.png)
- 四个输入分别是什么
  ![image](https://i.postimg.cc/JzC8p3VN/image.png)
  - 经过 sigmod 激活函数后，得到的 Zi，Zf，Zo 都是在 0 到 1 之间的数值，1 表示该门完全打开，0 表示该门完全关闭
  - 其中 Z 是最为普通的输入，可以从上图中看到，Z 是通过该时刻的输入 Xt 和上一时刻存在 memory cell 里的隐藏层信息 ht-1 向量拼接，再与权重参数向量 W 点积，得到的值经过激活函数 tanh 最终会得到一个数值，也就是 Z，注意只有 Z 的激活函数是 tanh，因为 Z 是真正作为输入的，其他三个都是门控装置
  - 再来看 Zi 输入门的门控装置， 同样也是通过该时刻的输入 Xt 和上一时刻隐藏状态，也就是上一时刻存下来的信息 ht-1 向量拼接，在与权重参数向量 Wi 点积（ 注意**每个门的权重向量**都不一样 ）。得到的值经过激活函数 sigmoid 的最终会得到一个 0-1 之间的一个数值，用来作为输入门的控制信号
  - Zf 和 Zo 与 Zi 类似

## 3. GRU（ Glavnoe Razvedivatelnoe Upravlenie，门控循环单元 ）

- GRU 背后的原理与 LSTM 非常相似，即用门控机制控制输入、记忆等信息而在当前时间步做出预测
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/be1d41c2-ea72-49d7-a1ef-97493024161e)
- GRU 只有有两个门，即一个重置门（ reset gate ）和一个更新门（ update gate ）
  - 重置门 决定到底有多少过去的信息需要遗忘
  - 更新门 决定到底要将多少过去的信息传递到未来
  - 如果将重置门设置为 1，更新门设置为 0，那么将再次获得标准 RNN 模型
