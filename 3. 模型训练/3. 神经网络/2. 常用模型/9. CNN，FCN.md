## 1. CNN（ Convolutional Neural Networks，卷积神经网络 ）

- 卷积神经网络 是一种专门为处理结构化网格数据（ 如图像 ）设计的神经网络，**至少包含一层 卷积层**，广泛应用于 图像分类、目标检测、语义分割 等任务
- CNN 通过卷积操作提取空间特征，结合池化层和全连接层实现高效的特征学习和任务预测

#### 1.1 通道

- 通道 是指 输入数据 或 特征图 在深度维度上的分量，是一个二维矩阵，通常反映数据的不同特征或颜色信息
  - 对于输入图像
    - 灰度图像：1 个通道，只有亮度信息
    - RGB 彩色图像：3 个通道，即 红、绿、蓝 分别对应一个通道
    - 其他类型图像：如 hyperspectral 图像可能有更多通道
  - 对于特征图（ 卷积层 或 池化层 的输出 ）
    - 通道数由上一层的过滤器数量决定，每个通道表示一种提取的特征，如 边缘、纹理 等
- 作用
  - 通道允许 CNN 处理多维信息。例如，RGB 图像的 3 个通道分别捕获红、绿、蓝颜色信息，卷积和池化操作对每个通道独立处理，最终融合结果
  - 在深层网络中，通道数通常递增，表示更复杂的特征组合，如 从 3 到 16、32、64 等

#### 1.2 卷积核（ Kernel ）

- 定义
  - 卷积核 是一个高度和宽度小于输入图像的二维矩阵，也称 卷积矩阵
  - 卷积核 在输入图像的高度和宽度上滑动，通过计算 卷积核 与 图像局部区域 的点积，提取局部特征，生成特征图，该图也称为卷积特征
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b5da219b-192c-4a4a-b8c0-1fd269b6c41d)
  - 在滑动的过程中，卷积核的参数一致，用于降低模型的参数量，从而提高了计算效率
- 核心参数如下
  - 步幅（ Stride ）
    - 卷积核滑动的距离，控制输出特征图尺寸，步幅越大，输出尺寸越小
    - 输出尺寸公式
      $$\text{Output Size} = \left\lfloor \frac{\text{Input Size} + 2 \times \text{Padding} - \text{Kernel Size}}{\text{Stride}} \right\rfloor + 1$$
    - 例如，输入 6×6 图像，3×3 卷积核，步幅 1，无 padding，输出尺寸为
      $$\text{Output Size} = \left\lfloor \frac{6 + 2 \times 0 - 3}{1} \right\rfloor + 1 = 4$$
  - 填充（ padding ）
    - 在卷积过程中，图像中间区域会被多次计算，而边缘区域（ 比如下图绿色阴影部分 ）只会被卷积计算一次
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/84cb481c-3251-4a2f-9b6b-dd65ced7ca41)
    - 可以在输入图像周围添加边界（ 如零填充 ），目的是控制卷积层输出的大小，以及在进行卷积操作时保持输入数据边界信息，p = 1 时，即会在最外层加一圈（ 蓝色区域 ）
  - 偏置（ bias ）
    - bias 通常不需要指定，模型会自己进行训练找到最佳值，每个卷积层通常包括多个卷积核，每个卷积核与一个 bias 相关联
    - 它的作用是在卷积操作后，对每个卷积核的输出加上一个常数值（ 即偏置值 ），这样提供了模型在学习过程中的额外自由度，有助于模型更好地拟合训练数据，提高模型的自由度

#### 1.3 过滤器（ Filter ）

- 每个过滤器 由 多个卷积核 组成，且每个卷积核对应输入的一个通道，因此过滤器中的通道数（ 深度 ）必须与输入图像中的通道数相同
- 当使用通道为 3 卷积彩色图像（ RGB 图像 ）时，过滤器的通道数（ 深度 ）也必须为 3
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/48dd4436-a625-48d9-8099-61408310ed49)
- 每个过滤器负责提取一种特定特征，如 边缘、纹理、角点 等，可以多个过滤器组合使用，输出的通道数等于过滤器数（ 此时所有卷积核的大小必须相同 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/f4806b26-c38e-40ed-8d1e-5b9f194ec025)
- 过滤器的权重是可学习的，通过训练优化以捕捉输入数据中最有意义的模式

#### 1.4 激活函数

- 激活函数是卷积层的最后一个组成部分，可增加输出中的非线性，增强模型表达能力
- 通常，在卷积层中将 ReLu 函数用作激活函数
- 例如以下简单卷积层的图像，其中将 6 x 6 x 3 输入图像与大小为 4 x 4 x 3 的 2 个过滤器以得到大小为 3 x 3 x 2 的卷积特征，对其应用激活函数以获取输出，这也称为特征图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/20565953-9ab1-4b49-bf97-21029490f608)

#### 1.5 池化层

- 在 CNN 中，通常先经过 卷积层，然后再经过 池化层
- 池化层通过下采样减小特征图尺寸，提高计算效率和特征鲁棒性，同时实现平移不变性（ 指模型对输入图像中目标物体位置的小范围移动不敏感 ）
- 池化层主要有三个超参数
  - f
    - 池化区域大小为 f \* f
    - 由于池化层过快地减少了数据的大小，目前的趋势是使用较小的池化区域大小，通常使用 (f = 2, s = 2)，偶尔也有人采用 (f = 3, s = 2)
  - s
    - 步幅，控制池化区域滑动距离
  - 类型
    - 最大池化（ 取区域最大值 ）或 平均池化（ 取区域平均值 ）
    - 因 最大池化 保留显著特征，目前更常用
- 全局平均池化
  - 原理
    - 对每个输入通道的整个特征图取平均值，将多个特征图（ 尺寸 $H \times W \times C$ ）压缩为一个向量（ 尺寸 $ 1 \times 1 \times C $ ）
  - 使用场景
    - 在 VGG、ResNet、Inception、EfficientNet 等架构中都被使用
    - 例如，ResNet-50 在卷积层后使用全局平均池化，将特征图压缩为向量，再接一个小的全连接层（ 或直接 softmax ），用于最后阶段的 降维 和 分类

#### 1.6 深度卷积神经网络（ DCNN ）

- DCNN 的基本原理是通过卷积和池化操作来提取图像中的特征，并通过全连接层进行分类或回归任务
- 基本原理
  - 特征提取：早期层提取低级特征（ 如边缘 ），深层提取高级特征（ 如对象形状 ）
  - 分类/回归：全连接层将特征映射为类别概率或回归值
  - 训练：通过反向传播和优化算法（ 如 Adam、SGD ）最小化损失函数（ 如交叉熵 ）
- 典型架构
  - VGG：堆叠多个 3×3 卷积层，增加网络深度
  - ResNet：引入残差连接，解决梯度消失问题
  - Inception：多尺度卷积并行处理，增强特征提取效率

## 2. FCN（ Fully Convolutional Networks，全卷积网络 ）

- 传统 CNN 在卷积层之后会接上若干全连接层，将卷积层产生的特征图（ feature map ）映射成一个固定长度的特征向量，比如把卷积神经网络的最后一层设为 softmax 进行分类，得到整个输入图像的一个数值描述（ 概率 ）
- FCN 是用于图像语义分割的一种框架，将传统 CNN 的最后的全连接层换成了卷积层，这样网络的输出是相同大小的热力图而非类别
  ![image](https://github.com/user-attachments/assets/5b5a5e2f-0a91-4024-ace6-9f08479c8f75)
- 语义分割，本质就是按图像中物体表达的含义进行抠图
  ![image](https://github.com/user-attachments/assets/2d6f4959-f8a0-4afb-9318-5bf478cf98bd)
- 存在两个问题需要解决
  - 如何解决卷积和池化导致图像尺寸的变小
  - 经过卷积和池化的特征图太小，这意味着过多细节的丢失，即使恢复原来大小的图片也丢失了过多信息

#### 2.1 网络结构

- FCN 网络结构主要分为两个部分：全卷积部分和反卷积部分
  - 全卷积部分为一些经典的 CNN 网络（ 如 VGG，ResNet 等 ），用于提取特征
  - 反卷积部分则是通过上采样得到原尺寸的语义分割图像
- FCN 的输入可以为任意尺寸的彩色图像，输出与输入尺寸相同，通道数为 n（ 目标类别数 ）+ 1（ 背景 ），FCN 网络结构如下
  ![image](https://github.com/user-attachments/assets/3971f239-1ebd-426b-b081-7fa24154086e)

#### 2.2 上采样（ upsampling ）

- 上采样的意义在于将小尺寸的高维度特征图恢复回去，以便做像素级的预测（ pixelwise prediction ），获得每个点的分类信息（ 语义分割 ）
- 上采样方式首先将原始图像的尺寸进行放大，空出来很多需要补充的区域，然后通过一定的插值算法来计算待补充的区域，从而实现图像的放大，常用的插值算法是双线性插值和反卷积

#### 2.3 跳跃结构

- 模型前期通过卷积、池化、非线性激活函数等作用输出了特征权重图像，所以卷积池化的最后特征图（ 也叫热图， heatmap ）太小，通常会损失很多细节
- 在 FCN 中，通常将热图扩大了 32 倍
  - ![image](https://github.com/user-attachments/assets/4b169b50-0016-4083-8788-daf544e9b5b0)
- 在论文中提出了三个模型分别是 FCN-32s、FCN-16s、FCN-8s
- FCN-32s
  - 将 pool5 的输出直接上采样 32 倍恢复到原图大小，将损失了原图很多细节信息的特征图直接上采样，效果较差
- FCN-16s
  - pool5 的输出上采样 2 倍（ 采样后大小与 pool4 的输出相同 ），然后与之前 pool4 输出相加然后再直接上采样 16 倍恢复到原图大小
- FCN-8s
  - pool5 的输出上采样 2 倍，然后与之前 pool4 输出相加然后再上采样 2 倍，然后与之前 pool3 输出相加然后再直接上采样 8 倍恢复到原图大小
- 在原文中给出 3 种网络结果对比，明显可以看出效果 FCN-32s < FCN-16s < FCN-8s，即使用多层 feature 融合有利于提高分割准确性，效果如下
  ![image](https://github.com/user-attachments/assets/358c6e76-a3d7-4009-8876-e04b8d4ae921)
