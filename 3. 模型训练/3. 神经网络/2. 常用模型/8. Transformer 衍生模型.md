- 基于 Transformer 架构的模型在自然语言处理（ NLP ）领域中占据了主导地位，许多重要模型都以 Transformer 为基础，针对不同任务进行了优化

## 1. 生成类模型

- 这些模型主要 **使用 Transformer 的解码器部分**，专注于生成任务，通常采用自回归方式

#### 1.1 GPT 系列（ Generative Pre-trained Transformer ）

- 提出者：OpenAI
- 代表模型：GPT-1 (2018)、GPT-2 (2019)、GPT-3 (2020)、ChatGPT、GPT-4、GPT-4o (2024)
- 特点：单向自回归语言模型，擅长文本生成、对话、问答等任务。GPT-3 和 GPT-4 通过大规模预训练支持零样本和少样本学习
- 应用：对话系统、文本创作、代码生成等
- 拓展：其他包括 grok，通义千问，kimi 等，都是基于解码器部分的生成类模型

#### 1.2 XLNet

- 提出者：Google/CMU (2019)
- 特点：结合自回归和双向建模的优点，使用排列语言建模（Permutation Language Modeling），捕捉更灵活的上下文依赖
- 应用：文本生成、分类等，性能优于早期 GPT 和 BERT

## 2. 理解类模型

- 这些模型主要 **使用 Transformer 的编码器部分**，专注于语义理解，适合分类、问答等任务

#### 2.1 BERT（ Bidirectional Encoder Representations from Transformers ）

- 提出者：Google (2018)
- 特点：双向语言模型，通过掩码语言建模（ MLM ）和下一句预测（ NSP ）预训练，捕捉上下文语义
- 应用：文本分类、情感分析、命名实体识别、问答系统等

#### 2.2 RoBERTa（ Robustly optimized BERT approach ）

- 提出者：Facebook AI (2019)
- 特点：优化 BERT 的训练方法，去掉 NSP，使用更大数据集、更长训练时间，性能更强
- 应用：与 BERT 类似，但在多项任务上表现更优

#### 2.3 ALBERT（ A Lite BERT ）

- 提出者：Google (2019)
- 特点：通过参数共享和分解嵌入降低 BERT 的参数量，提高效率，同时保持性能
- 应用：适用于资源受限场景的语义理解任务

#### 2.4 DistilBERT

- 提出者：Hugging Face (2019)
- 特点：通过知识蒸馏压缩 BERT，模型更小、推理更快，保留约 97% 的性能
- 应用：轻量级 NLP 任务，如移动设备上的文本处理

## 3. 编码器-解码器模型

- 这些模型使用完整的 Transformer 架构（ 编码器 + 解码器 ），适合序列到序列任务，如机器翻译、文本摘要等

#### 3.1 T5（ Text-to-Text Transfer Transformer ）

- 提出者：Google (2019)
- 特点：将所有 NLP 任务统一为“文本到文本”框架，编码器处理输入，解码器生成输出，支持分类、翻译、摘要等多种任务
- 应用：机器翻译、文本摘要、问答、文本生成 等

#### 3.2 BART（ Bidirectional and Auto-Regressive Transformer ）

- 提出者：Facebook AI (2019)
- 特点：结合双向编码器和自回归解码器，预训练时通过文本破坏与重建（ 如打乱句子、掩盖词 ）学习，适合生成和理解任务
- 应用：文本摘要、翻译、对话生成 等

#### 3.3 mT5 / mBART

- 提出者：Google / Facebook AI
- 特点：T5 和 BART 的多语言扩展，支持多种语言的翻译、摘要等任务
- 应用：跨语言 NLP 任务

## 4. 其他重要衍生模型

#### 4.1 Electra

- 提出者：Google (2020)
- 特点：提出替换令牌检测预训练任务，效率高于 BERT，性能接近 RoBERTa
- 应用：高效的文本分类、问答 等任务

#### 4.2 LLaMA 系列

- 提出者：Meta AI (2023)
- 特点：高效的 Transformer 模型，专为研究设计，参数量较小但性能优异，接近 GPT 系列
- 应用：研究领域，文本生成、分类 等

#### 4.3 ViT（ Vision Transformer ）

- 提出者：Google (2020)
- 特点：将 Transformer 架构应用于计算机视觉，处理图像分块的序列，取代传统 CNN
- 应用：扩展了 Transformer 的应用场景，应用于 图像分类、目标检测 等
