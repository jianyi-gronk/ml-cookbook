## 1. 神经网络

- 神经网络概念已经存在了几十年，最近才热门起来的原因是，随着互联网发展，数据量的飙升，线性回归和逻辑回归及时提供了更多的数据可能性能也已经到了瓶颈，而神经网络在如今大量数据的前提下，性能更好。并且现在 GPU 等硬件的提升也创造了条件
- 本篇说的一直都是全连接层，全连接层的神经元与其上一层的所有神经元进行全连接，还有其他例如卷积层等其他种神经网络层

#### 1.1 神经网络样例

- 一个简单的神经网络模型，存在输入层，隐藏层（ 可以有 0 或多层 ），输出层
- 比如判断某件 T 恤是否畅销，可能输入层由价格，运费，营销，材料质量组成，隐藏层可能由负担能力（ 价格 + 运费 ），认知度（ 营销 ），感知力（ 材料 ） 组成，最终通过隐藏层特征得到是否畅销
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b4c900db-22e2-4dd8-8d86-3f0bae8c8200)
- 实际中，每一层可以访问到上一层的所有特征，只不过有的特征的 w 可能接近 0
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/050eaf9e-77d4-438c-932c-cd1efd2747c6)
- 叫隐藏层是因为，训练集只给了 (x, y)，并没有告诉隐藏层的特征
- 在实际训练中，并不会明确隐藏层的特征，例如客户负担能力，神经网络会自己计算得出
- 神经网络的层数 = 隐藏层数 + 输出层
- 每一层的圆圈即神经元，输入层的神经元个数即输入数据特征数

#### 1.2 神经元计算流程

- 神经元的计算流程通常如下：
  - 输入：从上一层接收输入（ 激活值或特征 ）
  - 线性变换：进行权重矩阵乘法和偏置加法（例如，$ z = Wx + b $）
  - 归一化（ 如 BN、LN ）：对线性变换的结果进行标准化
  - 激活函数：对归一化后的结果应用非线性激活函数（如 ReLU、Sigmoid 等）
  - 输出：将结果传递到下一层

## 2. 图像感知举例

- 比如存在 1000 \* 1000 像素的黑白人脸图像，先将它摊开成 1000000 的一维数组当作输入层
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/82d52b62-08c1-46ec-8c90-98e72f4f0dd1)
- 然后进行训练，第一个隐藏层正在寻找非常短的线条；第二个隐藏层正在组合小短线，为了寻找脸的部分；第三个隐藏层开始拼接粗糙的面部形状
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3edc661f-52e0-4101-87e4-3a8c1020f06c)
- 车图像同理，先找非常短的线条，在拼接车的零件，最后拼接车的形状

## 3. 前向传播 和 反向传播

- 前向传播负责将输入数据传递到输出层得到预测结果
- 反向传播则负责根据预测结果与真实值之间的差异来调整权重和偏差，以逐渐提高模型的准确性和性能（ 梯度下降 ）
- 通过不断地迭代前向传播和反向传播的过程，可以训练出一个高效的神经网络模型来解决各种复杂的问题

#### 3.1 前向传播

- 即从左向右传播，这种传播一般最左边的隐藏层会有更多的神经元，越靠右，神经元的个数会越少
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a401c498-e553-418a-95fd-0fb760deb7de)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c2029b67-266f-47da-b214-d54a7a7b335a)
- 其中 dense 可以通过矩阵乘优化
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e3b019d9-96e7-4395-8f82-c43fd5ccbe88)
- 真实代码实现，其中 loss 用来指定损失函数，BinaryCrossentropy 为二元交叉熵（ 之前分类问题的损失函数 ），MeanSquaredError 为平方误差（ 之前回归问题的损失函数 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/1794092d-c269-4a60-ab5d-999b263fbf62)

#### 3.2 反向传播（ 梯度消失 和 梯度爆炸 ）

- 当神经网络进行反向传播时，根据前向传播的预测输出和目标输出之间的误差，来确定如何更新网络中的权重。这个误差就是 loss，其权重更新方式如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e9ca1016-98a7-489b-91d9-2e0c82429c00)
- 其中 alpha 为给定的学习率，假如要更新第二层的权重，根据反向传播的链式法则（ Chain Rule ），其数学公式如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ad882d55-8603-4550-bbc4-04fe930dff03)
- 在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化
- 梯度消失和梯度爆炸是深度网络训练过程中容易遇到的问题，由于网络层数的加深，梯度的膨胀或缩小效应不断累积，最终很容易造成模型无法收敛
- 如果框中每一项的值都远小于 1，那么随着层数的增加，框中的值将会以指数形式越来越小，最后浅层（ 靠近输出层 ）的权重变动会越来越小，当权值过小时，神经网络中浅层比深层的梯度变化更慢，就会引起梯度消失问题
- 而梯度爆炸则是刚好相反，框中的每一项其值都大于 1，最后框中的值将随着层数的增加以指数形式越变越大，越是靠近浅层（ 靠近输出层 ）的权重变动会越来越大，当权值过大时，神经网络中浅层比深层的梯度变化更快，就会引起梯度爆炸问题

## 4. tensor（ 张量 ）

- 在神经网络中，“tensor” 是数据的基本组织形式，也是算法和模型的基本操作对象。它是多维数组的数学抽象，数据通常以张量的形式进行处理，张量通常用于表示输入数据、模型参数以及模型的输出
- 张量可以是标量（ 0 维张量，即单个数字 ）、向量（ 1 维张量，如一维数组 ）、矩阵（ 2 维张量 ）以及更高维度的数组（ n 维张量 ）
- TensorFlow 和 PyTorch 等流行的深度学习框架都提供了张量操作的功能，允许用户对张量进行各种数学运算和变换。通过张量，可以方便地表示和处理复杂的数据结构，例如图像、文本序列等，同时也为神经网络的训练和推断提供了基础数据结构
- 常用方法

  ```
  # data - 可以是 list, tuple, numpy array, scalar 或其他类型
  # dtype - 可以返回想要的 tensor 类型
  # device - 可以指定返回的设备
  # requires_grad - 可以指定是否进行记录图的操作，默认为 False
  torch.tensor(data, dtype=None, device=None,requires_grad=False)

  # 将 numpy 中的数据给到 tensor 中
  torch.from_numpy(ndarry)

  # 返回形状为 size 的空 tensor
  torch.empty(size)

  # 返回形状为 size 的全部是 0 的 tensor
  torch.zeros(size)

  # 返回跟 input 相同 size 的全 0 的 tensor
  torch.zeros_like(input)

  # 返回形状为 size 的全部是 1 的 tensor
  torch.ones(size)

  # 返回跟 input 相同 size 的全 1 的 tensor
  torch.ones_like(input)

  # 返回一个从start到end的序列
  # 可以只输入一个end参数，就跟python的range()一样了
  # 实际上 PyTorch 也有 range()，但是这个要被废掉了，替换成 arange 了
  torch.arange(start=0, end, step=1)

  # 返回形状为 size 的全部是 fill_value 的 tensor
  torch.full(size, fill_value)

  # 返回形状为 size 的 tensor，并填充随机数
  torch.randn(size)
  ```
