## 1. 神经网络

- 神经网络概念已经存在了几十年，最近才热门起来的原因是，随着互联网发展，数据量的飙升，线性回归和逻辑回归及时提供了更多的数据可能性能也已经到了瓶颈，而神经网络在如今大量数据的前提下，性能更好。并且现在 GPU 等硬件的提升也创造了条件
- 本篇说的一直都是全连接层，全连接层的神经元与其上一层的所有神经元进行全连接，还有其他例如卷积层等其他种神经网络层

#### 1.1 神经网络样例

- 一个简单的神经网络模型，存在输入层，隐藏层（ 可以有 0 或多层 ），输出层
- 比如判断某件 T 恤是否畅销，可能输入层由价格，运费，营销，材料质量组成，隐藏层可能由负担能力（ 价格 + 运费 ），认知度（ 营销 ），感知力（ 材料 ） 组成，最终通过隐藏层特征得到是否畅销
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b4c900db-22e2-4dd8-8d86-3f0bae8c8200)
- 实际中，每一层可以访问到上一层的所有特征，只不过有的特征的 w 可能接近 0
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/050eaf9e-77d4-438c-932c-cd1efd2747c6)
- 叫隐藏层是因为，训练集只给了 (x, y)，并没有告诉隐藏层的特征
- 在实际训练中，并不会明确隐藏层的特征，例如客户负担能力，神经网络会自己计算得出
- 神经网络的层数 = 隐藏层数 + 输出层
- 每一层的圆圈即神经元，输入层的神经元个数即输入数据特征数

#### 1.2 神经元计算流程

- 神经元的计算流程通常如下：
  - 输入：从上一层接收输入（ 激活值或特征 ）
  - 线性变换：进行权重矩阵乘法和偏置加法（例如，$ z = Wx + b $）
  - 归一化（ 如 BN、LN ）：对线性变换的结果进行标准化
  - 激活函数：对归一化后的结果应用非线性激活函数（如 ReLU、Sigmoid 等）
  - 输出：将结果传递到下一层

## 2. 图像感知举例

- 比如存在 1000 \* 1000 像素的黑白人脸图像，先将它摊开成 1000000 的一维数组当作输入层
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/82d52b62-08c1-46ec-8c90-98e72f4f0dd1)
- 然后进行训练，第一个隐藏层正在寻找非常短的线条；第二个隐藏层正在组合小短线，为了寻找脸的部分；第三个隐藏层开始拼接粗糙的面部形状
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3edc661f-52e0-4101-87e4-3a8c1020f06c)
- 车图像同理，先找非常短的线条，在拼接车的零件，最后拼接车的形状

## 3. 前向传播 和 反向传播

- 前向传播负责将输入数据传递到输出层得到预测结果
- 反向传播则负责根据预测结果与真实值之间的差异来调整权重和偏差，以逐渐提高模型的准确性和性能（ 梯度下降 ）
- 通过不断地迭代前向传播和反向传播的过程，可以训练出一个高效的神经网络模型来解决各种复杂的问题

#### 3.1 前向传播

- 即从左向右传播，这种传播一般最左边的隐藏层会有更多的神经元，越靠右，神经元的个数会越少
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a401c498-e553-418a-95fd-0fb760deb7de)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c2029b67-266f-47da-b214-d54a7a7b335a)
- 其中 dense 可以通过矩阵乘优化
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e3b019d9-96e7-4395-8f82-c43fd5ccbe88)
- 真实代码实现，其中 loss 用来指定损失函数，BinaryCrossentropy 为二元交叉熵（ 之前分类问题的损失函数 ），MeanSquaredError 为平方误差（ 之前回归问题的损失函数 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/1794092d-c269-4a60-ab5d-999b263fbf62)

#### 3.2 反向传播（ 梯度消失 和 梯度爆炸 ）

- 当神经网络进行反向传播时，根据前向传播的预测输出和目标输出之间的误差，来确定如何更新网络中的权重。这个误差就是 loss，其权重更新方式如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e9ca1016-98a7-489b-91d9-2e0c82429c00)
- 其中 alpha 为给定的学习率，假如要更新第二层的权重，根据反向传播的链式法则（ Chain Rule ），其数学公式如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ad882d55-8603-4550-bbc4-04fe930dff03)
- 在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化
- 梯度消失和梯度爆炸是深度网络训练过程中容易遇到的问题，由于网络层数的加深，梯度的膨胀或缩小效应不断累积，最终很容易造成模型无法收敛
- 如果框中每一项的值都远小于 1，那么随着层数的增加，框中的值将会以指数形式越来越小，最后浅层（ 靠近输出层 ）的权重变动会越来越小，当权值过小时，神经网络中浅层比深层的梯度变化更慢，就会引起梯度消失问题
- 而梯度爆炸则是刚好相反，框中的每一项其值都大于 1，最后框中的值将随着层数的增加以指数形式越变越大，越是靠近浅层（ 靠近输出层 ）的权重变动会越来越大，当权值过大时，神经网络中浅层比深层的梯度变化更快，就会引起梯度爆炸问题

## 4. tensor（ 张量 ）

- 在神经网络中，“tensor” 是数据的基本组织形式，也是算法和模型的基本操作对象。它是多维数组的数学抽象，数据通常以张量的形式进行处理，张量通常用于表示输入数据、模型参数以及模型的输出
- 张量可以是标量（ 0 维张量，即单个数字 ）、向量（ 1 维张量，如一维数组 ）、矩阵（ 2 维张量 ）以及更高维度的数组（ n 维张量 ）
- TensorFlow 和 PyTorch 等流行的深度学习框架都提供了张量操作的功能，允许用户对张量进行各种数学运算和变换。通过张量，可以方便地表示和处理复杂的数据结构，例如图像、文本序列等，同时也为神经网络的训练和推断提供了基础数据结构
- 常用方法

  ```
  # data - 可以是 list, tuple, numpy array, scalar 或其他类型
  # dtype - 可以返回想要的 tensor 类型
  # device - 可以指定返回的设备
  # requires_grad - 可以指定是否进行记录图的操作，默认为 False
  torch.tensor(data, dtype=None, device=None,requires_grad=False)

  # 将 numpy 中的数据给到 tensor 中
  torch.from_numpy(ndarry)

  # 返回形状为 size 的空 tensor
  torch.empty(size)

  # 返回形状为 size 的全部是 0 的 tensor
  torch.zeros(size)

  # 返回跟 input 相同 size 的全 0 的 tensor
  torch.zeros_like(input)

  # 返回形状为 size 的全部是 1 的 tensor
  torch.ones(size)

  # 返回跟 input 相同 size 的全 1 的 tensor
  torch.ones_like(input)

  # 返回一个从start到end的序列
  # 可以只输入一个end参数，就跟python的range()一样了
  # 实际上 PyTorch 也有 range()，但是这个要被废掉了，替换成 arange 了
  torch.arange(start=0, end, step=1)

  # 返回形状为 size 的全部是 fill_value 的 tensor
  torch.full(size, fill_value)

  # 返回形状为 size 的 tensor，并填充随机数
  torch.randn(size)
  ```

## 5. 残差网络（ ResNet ）

- 在传统的神经网络中，每一层的输入都是前一层的输出，通过不断地进行非线性变换，逐渐提取高级别的特征
- 理论上神经网络层数越多，网络越复杂，训练出来的模型越好。但是，随着网络层数的增加，梯度在反向传播过程中出现爆炸和消失问题，从而导致模型训练出现困难

#### 2.1 工作原理

- 残差连接的核心思想是在网络的一层或多层之间引入直接连接（ 也叫跳跃连接 ），直接将前一层的输出加到后一层的输入中，从而提供了一种绕过非线性变换的路径，使得这些层的输出不仅包括经过非线性变换的特征，还包括未经处理的输入特征，这样网络就可以学习到在信息压缩或拉伸后保留重要信息
- 这样做的目的是允许网络学习到的是输入和输出之间的残差（ 即差异 ），而不是直接学习一个完整的映射。这种方式有助于梯度在训练过程中更容易地传递到前一层，减轻深度网络中梯度消失的问题。此外，残差连接还能够减少模型的训练误差，并且可以提高模型的泛化能力，从而更好地适应未见过的数据

#### 2.2 残差块

- 在数学上，假设 x 表示前一层的输入，F(x) 表示当前层的非线性变换，那么残差连接的输出可以表示为：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/36f559ce-dc89-4d5f-a385-0a9ef35f6d98)
- 举例说明
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/66aa9c67-265b-4892-a684-26d2c1ff0f48)
  - 在主路径上，主路径为 a[l] -> Linaer -> ReLU -> Linear -> ReLU -> a[l + 2]
  - 在残差网络中，需要讲 a[l] 复制到神经网络中，并应用到非线性计算之前（ ReLU 等 ）
  - 因此最后的 ReLU 的公式需要更改，从 a[l + 2] = g(z[l + 2]) 改成 a[l + 2] = g(z[l + 2] + a[l])
  - 有的时候会跳过一层，有的时候会跳过两层为了将信息更深入的传递到神经网络中
  - 这样 a[l] 到 a[l + 2] 最外层圈出来的也就是残差块
  - 注意，既然存在 a[l] + a[l + 2]，那么 a[l] 和 a[l + 2] 的维度需要相同，如果当前层的输入和输出的维度不同，可以使用一个全连接层来将输入的维度映射到输出的维度，然后再进行残差连接
- 像这样的残差块堆叠起来的深度网络就是残差网络
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3a033ae9-d708-4e5d-ae87-f6a77ebb5e10)

#### 2.3 优点

- 缓解梯度消失：残差连接通过直接传递信息，有助于梯度在深层网络中更有效地传播，减少了梯度消失的问题
- 加速收敛：由于残差连接的引入，网络可以更快地学习到有效的特征表示，加速了训练过程中的收敛速度
- 增强网络表达能力：残差连接使得网络可以通过学习输入和输出之间的残差来增强模型的表达能力，提高了模型处理复杂数据的能力
- 支持构建深层网络，残差连接使得训练非常深的网络成为可能，而不会导致性能下降
- 灵活性和扩展性：残差连接可以很容易地添加到各种网络结构中，包括卷积神经网络（ CNN ）、循环神经网络（ RNN ）和自注意力网络（ 如 Transformer ），使其受益于残差学习的优势
