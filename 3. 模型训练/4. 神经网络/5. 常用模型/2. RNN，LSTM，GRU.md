## 2. LSTM（ Long Short Term Memory，长短时记忆网络 ）

#### 2.1 RNN 存在的问题

- 最基础版本的 RNN，每一时刻的隐藏状态都不仅由该时刻的输入决定，还取决于上一时刻的隐藏层的值，但如果一个句子很长，到句子末尾时，它将记不住这个句子的开头的内容详细内容
- 具体来说，在训练 RNN 时，模型会逐个时间步处理每个单词。当模型处理到句子的末尾时，它的隐藏状态包含了整个句子的信息。然而，由于梯度消失的问题，模型可能在处理过程中忘记了句子开头的重要信息

#### 2.2 基础介绍

- LSTM 是 RNN 的变体，旨在解决传统 RNN 在处理长序列时的梯度消失和梯度爆炸问题
- LSTM 引入了一种特殊的存储单元和门控机制，以更有效地捕捉和处理序列数据中的长期依赖关系
- 在现实生活中，门就是用来控制进出的，门关上了，你就进不去房子了，门打开你就能进去，同理，这里的门是用来控制每一时刻信息记忆与遗忘的

#### 2.3 与 RNN 的区别

- RNN 每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息，把存每一时刻信息的地方叫做 Memory Cell
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/cd8d9dbd-b534-4100-8073-6cf0016cc8ce)
- LSTM 和普通 RNN 的区别在于
  - RNN 什么信息它都存下来，因为它没有挑选的能力
  - 而 LSTM 不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择
- 如下图，普通 RNN 只有中间的 Memory Cell 用来存所有的信息，而 LSTM 多了三个 Gate，也就是三个门
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ec225e68-548c-496d-87da-97a0a94b8148)
- 三个门分别作用
  - Input Gate（ 输入门 ）
    - 每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到 Memory Cell
  - Output Gate（ 输出门 ）
    - 每一时刻是否有信息从 Memory Cell 输出取决于这一道门
  - Forget Gate（ 遗忘门 ）
    - 每一时刻 Memory Cell 里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把 Memory Cell 里的值清除，也就是遗忘掉
  - 先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘 Memory Cell 里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出

#### 2.4 LSTM 结构

- LSTM 结构如下
  ![image](https://i.postimg.cc/qqCbD3yb/image.png)
- LSTM 里常用的激活函数有两个，一个是 tanh，一个是 sigmoid，下图符号代表一个激活函数
  ![image](https://i.postimg.cc/HWKdyD6P/image.png)
- 四个输入分别是什么
  ![image](https://i.postimg.cc/JzC8p3VN/image.png)
  - 经过 sigmod 激活函数后，得到的 Zi，Zf，Zo 都是在 0 到 1 之间的数值，1 表示该门完全打开，0 表示该门完全关闭
  - 其中 Z 是最为普通的输入，可以从上图中看到，Z 是通过该时刻的输入 Xt 和上一时刻存在 memory cell 里的隐藏层信息 ht-1 向量拼接，再与权重参数向量 W 点积，得到的值经过激活函数 tanh 最终会得到一个数值，也就是 Z，注意只有 Z 的激活函数是 tanh，因为 Z 是真正作为输入的，其他三个都是门控装置
  - 再来看 Zi 输入门的门控装置， 同样也是通过该时刻的输入 Xt 和上一时刻隐藏状态，也就是上一时刻存下来的信息 ht-1 向量拼接，在与权重参数向量 Wi 点积（ 注意**每个门的权重向量**都不一样 ）。得到的值经过激活函数 sigmoid 的最终会得到一个 0-1 之间的一个数值，用来作为输入门的控制信号
  - Zf 和 Zo 与 Zi 类似

## 3. GRU（ Glavnoe Razvedivatelnoe Upravlenie，门控循环单元 ）

- GRU 背后的原理与 LSTM 非常相似，即用门控机制控制输入、记忆等信息而在当前时间步做出预测
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/be1d41c2-ea72-49d7-a1ef-97493024161e)
- GRU 只有有两个门，即一个重置门（ reset gate ）和一个更新门（ update gate ）
  - 重置门 决定到底有多少过去的信息需要遗忘
  - 更新门 决定到底要将多少过去的信息传递到未来
  - 如果将重置门设置为 1，更新门设置为 0，那么将再次获得标准 RNN 模型
