## 1. Dropout 层（ 随机失活 ）

- Dropout 是一种神经网络正则化技术，**通过在训练阶段中随机将部分神经元的输出置为 0（ 即 “丢弃” ），减少过拟合，提高模型泛化能力**，但在推理阶段（ 即模型用于预测或测试时 ），Dropout 会关闭，所有神经元都参与计算

#### 1.1 详细原理

- 在每次训练迭代中，Dropout 层为每个神经元生成一个二进制掩码（ 0 或 1 ）
  - 以概率 p（ 丢弃率，通常为 0.2~0.5 ）随机决定哪些神经元的输出保留（ 1 ）或丢弃（ 0 ）
  - 这种 随机丢弃 **防止神经元形成共适应**，即某些神经元过于依赖彼此，像是 “抱团合作” 的队员，忽略其他神经元，Dropout **逼迫模型用不同的神经元组合完成任务，从而避免过拟合**
- 除了掩码，还需要考虑 缩放因子
  - 假如训练时 30% 的神经元输出被置为 0，那么总输出大约只有之前的 70%，但在推理阶段，Dropout 会关闭，总输出又变为 100%，会导致模型训练和推理时的表现不同
  - **因此训练时，需要通过缩放因子将总输出放大至 100%**，公式为 $\frac{1}{1 - p}$
- 数学上，对于输入 $x_i$，Dropout 操作为
  $$y_i = \frac{x_i \cdot m_i}{1 - p}$$
  - $m_i$ 是二进制掩码（ 0 或 1 ）
  - p 是丢弃率
  - $\frac{1}{1 - p}$ 是缩放因子

#### 1.2 不同场景

- 全连接层
  - 常用于全连接层之后，尤其在隐藏层神经元较多（ 如 > 256 ）时，通过随机丢弃减少神经元间的共适应，降低过拟合风险
- 卷积神经网络
  - 在卷积层后通常不使用 Dropout，因卷积层的共享权重和空间一致性已提供一定正则化，过拟合风险较低
- 循环神经网络
  - 可用于 RNN（ 如 LSTM、GRU ）的隐藏状态，但需谨慎设置丢弃率，因序列数据对连续性敏感
- 生命周期
  - Dropout 仅在训练时使用，推理时关闭，所有神经元输出按比例缩放，以保持输出期望一致
