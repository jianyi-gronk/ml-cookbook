## 1. SLP

#### 1.1 基本概念

- SLP 是最简单的前馈神经网络，**结构只有 一个输入层 和 一个输出层**
  - 输入层：$n$ 个 **输入节点**，对应 $n$ 个特征
    $$\mathbf{x} = [x_1, x_2, \dots, x_n]^T$$
  - 输出层：$k$ 个 **神经元**，对应 $k$ 类分类
- 简单介绍下 神经元 和 输入节点
  - 神经元
    - 先接受来自前一层所有节点的输出
    - 进行计算（ 权重和偏置项 ）得到函数结果值
    - 然后通过激活函数进行处理
    - 最终得到神经元的输出值
  - 输入节点
    - 仅被动传递原始数据（ 无计算，无激活函数 ），直接把输入的特征值传给下一层

#### 1.2 输出层计算

- 基本公式
  $$z = \mathbf{w}^T \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b$$
  - 权重向量：$\mathbf{w} = [w_1, w_2, \dots, w_n]^T$
  - 偏置项：$b$
- 增广表示法
  - 添加虚拟输入 $x_0 = 1$
    $$\mathbf{x'} = [1, x_1, x_2, \dots, x_n]^T$$
  - 合并偏置到权重向量，作为 $w_0$
    $$\mathbf{w'} = [b, w_1, w_2, \dots, w_n]^T$$
  - 修正后的等价公式
    $$z = \mathbf{w'}^T \mathbf{x'} = \sum_{i=0}^{n} w'_i x'_i = \sum_{i=1}^{n} w_i x_i + b$$

#### 1.3 输出层激活函数

- 阶跃函数（ 二分类，单个输出神经元 ）
  $$
    f(z) = \begin{cases}
      1 & \text{if } z \geq 0 \\
      -1 & \text{otherwise} \\
    \end{cases}
  $$
- Softmax 函数（ 多分类，K 个输出神经元 ）
  - 每个神经元有独立权重
    $$z_k = \mathbf{w'}_k^T \mathbf{x'} \quad (k = 1,2,\dots,K)$$
  - Softmax 函数（ 概率归一化 ）
    $$f(z_k) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$
  - 满足：
    $$\sum_{k=1}^{K} f(z_k) = 1 \quad \text{且} \quad 0 < f(z_k) < 1$$
  - 预测类别：
    $$\hat{y} = \arg\max_{k} f(z_k)$$

#### 1.4 训练算法

- **线性决策边界**，即在 $n$ 维空间中的超平面
  - 二分类
    $$\mathbf{w}^T \mathbf{x} + b = 0$$
  - 多分类（ 类别 $k$ 和 $j$ 的边界 ）
    $$(\mathbf{w}_k - \mathbf{w}_j)^T \mathbf{x} + (b_k - b_j) = 0$$
- 损失函数：误分类点到决策边界的距离
  $$
  L(\mathbf{w}, b) = -\sum_{x_i \in M} y_i (\mathbf{w}^T \mathbf{x}_i + b)
  $$
  - 其中 $M$ 是误分类样本集
- 权重更新
  $$
  \mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i \\
  b \leftarrow b + \eta y_i
  $$
  其中 $\eta$ 是学习率，$(x_i, y_i)$ 是误分类样本

#### 1.5 收敛性

- 感知机收敛定理：若训练数据线性可分，算法在有限步内收敛
- 局限性：
  - 只能解决线性可分问题（ 如 AND, OR ）
  - 无法解决非线性问题（ 如 XOR ）

## 2. MLP

#### 2.1 基本概念

- **定义**：包含至少一个隐藏层的全连接前馈神经网络（ 全连接 即相邻层神经元完全互连 ）
- **结构**：
  - 输入层：n 个输入节点，对应 n 个特征
  - 隐藏层：1 到多层，每层 m 个神经元
  - 输出层：k 个神经元，对应 k 类分类

#### 2.2 数学模型

- **前向传播**：
  - 第 $l$ 层输出：
    $$
    \mathbf{a}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) \\
    \mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
    $$
  - 输入层：$\mathbf{a}^{(0)} = \mathbf{x}$
  - 输出层：$\hat{\mathbf{y}} = \mathbf{a}^{(L)}$

#### 2.3 激活函数

| 函数类型   | 公式                                                                  | 导数                                                     | 特点                     |
| ---------- | --------------------------------------------------------------------- | -------------------------------------------------------- | ------------------------ |
| Sigmoid    | $\sigma(z) = \frac{1}{1+e^{-z}}$                                      | $\sigma'(z) = \sigma(z)(1-\sigma(z))$                    | (0,1) 输出，梯度消失问题 |
| Tanh       | $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$                        | $1 - \tanh^2(z)$                                         | (-1,1) 输出，中心化      |
| ReLU       | $\text{ReLU}(z) = \max(0, z)$                                         | $\begin{cases} 1 & z>0 \\ 0 & z \leq 0 \end{cases}$      | 计算高效，缓解梯度消失   |
| Leaky ReLU | $\max(\alpha z, z)$ ($\alpha \approx 0.01$)                           | $\begin{cases} 1 & z>0 \\ \alpha & z \leq 0 \end{cases}$ | 解决 ReLU "死亡"问题     |
| Softmax    | $\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$ | $\frac{\partial}{\partial z_i} = p_i(1-p_i)$             | 多分类概率输出           |

#### 2.4 损失函数

| 任务类型 | 损失函数                    | 公式                                                                            |
| -------- | --------------------------- | ------------------------------------------------------------------------------- |
| 二分类   | Binary Cross-Entropy        | $L = -\frac{1}{N}\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$ |
| 多分类   | Categorical Cross-Entropy   | $L = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^K y_{i,c} \log(\hat{y}_{i,c})$         |
| 回归     | Mean Squared Error（ MSE ） | $L = \frac{1}{2N}\sum_{i=1}^N \|\mathbf{y}_i - \hat{\mathbf{y}}_i\|^2$          |

#### 2.5 反向传播算法

- **核心思想**：链式法则计算梯度
- **关键步骤**：
  1. 前向传播计算输出
  2. 计算输出层误差：
     $$
     \delta^{(L)} = \nabla_{\mathbf{a}} L \odot f'(\mathbf{z}^{(L)})
     $$
  3. 反向传播误差：
     $$
     \delta^{(l)} = (\mathbf{W}^{(l+1)T} \delta^{(l+1)}) \odot f'(\mathbf{z}^{(l)})
     $$
  4. 计算梯度：
     $$
     \nabla_{\mathbf{W}^{(l)}} L = \delta^{(l)} \mathbf{a}^{(l-1)T} \\
     \nabla_{\mathbf{b}^{(l)}} L = \delta^{(l)}
     $$
  5. 参数更新：
     $$
     \mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \nabla_{\mathbf{W}^{(l)}} L \\
     \mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \nabla_{\mathbf{b}^{(l)}} L
     $$

#### 2.6 优化技巧

- **梯度下降变体**：
  - SGD（随机梯度下降）
  - Momentum：$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$
  - Adam：结合 Momentum 和 RMSProp
- **正则化**：
  - L2 正则化：$L' = L + \frac{\lambda}{2} \|\mathbf{w}\|^2$
  - Dropout：训练时随机丢弃神经元（比例 $p$）
  - Batch Normalization：标准化层输入
- **初始化方法**：
  - Xavier 初始化：$\text{Var}(w) = \frac{2}{n_{\text{in}} + n_{\text{out}}}$
  - He 初始化：$\text{Var}(w) = \frac{2}{n_{\text{in}}}$（ReLU 适用）

#### 2.7 通用近似定理

- **关键结论**：一个包含至少一层隐藏层的 MLP，只要使用非线性激活函数且隐藏层足够宽，可以逼近任意连续函数
- **数学表述**：
  $$
  \forall f \in C(\mathbb{R}^n), \epsilon > 0, \exists g(x) = \sum_{j=1}^N \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + b_j)
  $$
  使得 $|f(x) - g(x)| < \epsilon$ 对所有 $x \in K$（紧集）成立

## 3. SLP vs MLP 对比

| 特性         | SLP                | MLP                       |
| ------------ | ------------------ | ------------------------- |
| 网络深度     | 单层（无隐藏层）   | 多层（≥1 隐藏层）         |
| 决策边界     | 线性               | 非线性                    |
| 解决问题类型 | 线性可分问题       | 线性/非线性问题           |
| 训练算法     | 感知机学习规则     | 反向传播                  |
| 激活函数     | 阶跃/Signum        | Sigmoid/ReLU/Softmax 等   |
| 参数更新     | 仅输出层           | 所有层                    |
| 逼近能力     | 有限（线性分类器） | 通用函数逼近器            |
| 典型应用     | 简单线性分类       | 图像识别/NLP/复杂模式识别 |

## 4. 实践注意事项

1. **数据预处理**：
   - 标准化/归一化：$\tilde{x} = \frac{x - \mu}{\sigma}$
   - 类别变量编码：One-Hot Encoding
2. **超参数调优**：
   - 学习率 $\eta$：使用学习率调度（ 如 Step Decay ）
   - 隐藏层大小：从 $[n, 2n]$ 开始（ $n$ 为输入特征数 ）
   - 层数：从 1-3 层开始，逐步增加
   - Batch Size：32/64/128（ 需平衡内存和收敛速度 ）
3. **防止过拟合**：
   - Early Stopping：验证损失不再下降时停止
   - Dropout（ 典型值 $p=0.5$ ）
   - L2 正则化（ $\lambda \in [0.0001, 0.01]$ ）
4. **梯度问题处理**：
   - 梯度消失：使用 ReLU/Leaky ReLU，残差连接
   - 梯度爆炸：梯度裁剪，权重约束
