## 1. 基本概念

- **定义**：包含至少一个隐藏层的前馈神经网络
- **结构**：
  - 输入层：n 个输入节点，对应 n 个特征
  - 隐藏层：1 到多层，每层 m 个神经元
  - 输出层：k 个神经元，对应 k 类分类
  - 全连接：相邻层神经元完全互连

## 2. 数学模型

- **前向传播**：
  - 第 $l$ 层输出：
    $$
    \mathbf{a}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) \\
    \mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
    $$
  - 输入层：$\mathbf{a}^{(0)} = \mathbf{x}$
  - 输出层：$\hat{\mathbf{y}} = \mathbf{a}^{(L)}$

## 3. 激活函数

| 函数类型   | 公式                                                                  | 导数                                                     | 特点                     |
| ---------- | --------------------------------------------------------------------- | -------------------------------------------------------- | ------------------------ |
| Sigmoid    | $\sigma(z) = \frac{1}{1+e^{-z}}$                                      | $\sigma'(z) = \sigma(z)(1-\sigma(z))$                    | (0,1) 输出，梯度消失问题 |
| Tanh       | $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$                        | $1 - \tanh^2(z)$                                         | (-1,1) 输出，中心化      |
| ReLU       | $\text{ReLU}(z) = \max(0, z)$                                         | $\begin{cases} 1 & z>0 \\ 0 & z \leq 0 \end{cases}$      | 计算高效，缓解梯度消失   |
| Leaky ReLU | $\max(\alpha z, z)$ ($\alpha \approx 0.01$)                           | $\begin{cases} 1 & z>0 \\ \alpha & z \leq 0 \end{cases}$ | 解决 ReLU "死亡"问题     |
| Softmax    | $\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$ | $\frac{\partial}{\partial z_i} = p_i(1-p_i)$             | 多分类概率输出           |

## 4. 损失函数

| 任务类型 | 损失函数                    | 公式                                                                            |
| -------- | --------------------------- | ------------------------------------------------------------------------------- |
| 二分类   | Binary Cross-Entropy        | $L = -\frac{1}{N}\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$ |
| 多分类   | Categorical Cross-Entropy   | $L = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^K y_{i,c} \log(\hat{y}_{i,c})$         |
| 回归     | Mean Squared Error（ MSE ） | $L = \frac{1}{2N}\sum_{i=1}^N \|\mathbf{y}_i - \hat{\mathbf{y}}_i\|^2$          |

## 5. 反向传播算法

- **核心思想**：链式法则计算梯度
- **关键步骤**：
  1. 前向传播计算输出
  2. 计算输出层误差：
     $$
     \delta^{(L)} = \nabla_{\mathbf{a}} L \odot f'(\mathbf{z}^{(L)})
     $$
  3. 反向传播误差：
     $$
     \delta^{(l)} = (\mathbf{W}^{(l+1)T} \delta^{(l+1)}) \odot f'(\mathbf{z}^{(l)})
     $$
  4. 计算梯度：
     $$
     \nabla_{\mathbf{W}^{(l)}} L = \delta^{(l)} \mathbf{a}^{(l-1)T} \\
     \nabla_{\mathbf{b}^{(l)}} L = \delta^{(l)}
     $$
  5. 参数更新：
     $$
     \mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \nabla_{\mathbf{W}^{(l)}} L \\
     \mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \nabla_{\mathbf{b}^{(l)}} L
     $$

## 6. 优化技巧

- **梯度下降变体**：
  - SGD（随机梯度下降）
  - Momentum：$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$
  - Adam：结合 Momentum 和 RMSProp
- **正则化**：
  - L2 正则化：$L' = L + \frac{\lambda}{2} \|\mathbf{w}\|^2$
  - Dropout：训练时随机丢弃神经元（比例 $p$）
  - Batch Normalization：标准化层输入
- **初始化方法**：
  - Xavier 初始化：$\text{Var}(w) = \frac{2}{n_{\text{in}} + n_{\text{out}}}$
  - He 初始化：$\text{Var}(w) = \frac{2}{n_{\text{in}}}$（ReLU 适用）

## 7. 通用近似定理

- **关键结论**：一个包含至少一层隐藏层的 MLP，只要使用非线性激活函数且隐藏层足够宽，可以逼近任意连续函数
- **数学表述**：
  $$
  \forall f \in C(\mathbb{R}^n), \epsilon > 0, \exists g(x) = \sum_{j=1}^N \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + b_j)
  $$
  使得 $|f(x) - g(x)| < \epsilon$ 对所有 $x \in K$（紧集）成立

## 8. SLP vs MLP 对比

| 特性         | SLP                | MLP                       |
| ------------ | ------------------ | ------------------------- |
| 网络深度     | 单层（无隐藏层）   | 多层（≥1 隐藏层）         |
| 决策边界     | 线性               | 非线性                    |
| 解决问题类型 | 线性可分问题       | 线性/非线性问题           |
| 训练算法     | 感知机学习规则     | 反向传播                  |
| 激活函数     | 阶跃/Signum        | Sigmoid/ReLU/Softmax 等   |
| 参数更新     | 仅输出层           | 所有层                    |
| 逼近能力     | 有限（线性分类器） | 通用函数逼近器            |
| 典型应用     | 简单线性分类       | 图像识别/NLP/复杂模式识别 |

## 9. 实践注意事项

1. **数据预处理**：
   - 标准化/归一化：$\tilde{x} = \frac{x - \mu}{\sigma}$
   - 类别变量编码：One-Hot Encoding
2. **超参数调优**：
   - 学习率 $\eta$：使用学习率调度（ 如 Step Decay ）
   - 隐藏层大小：从 $[n, 2n]$ 开始（ $n$ 为输入特征数 ）
   - 层数：从 1-3 层开始，逐步增加
   - Batch Size：32/64/128（ 需平衡内存和收敛速度 ）
3. **防止过拟合**：
   - Early Stopping：验证损失不再下降时停止
   - Dropout（ 典型值 $p=0.5$ ）
   - L2 正则化（ $\lambda \in [0.0001, 0.01]$ ）
4. **梯度问题处理**：
   - 梯度消失：使用 ReLU/Leaky ReLU，残差连接
   - 梯度爆炸：梯度裁剪，权重约束
