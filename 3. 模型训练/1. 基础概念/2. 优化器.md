## 1. 优化器

- 优化器的核心特点包括
  - **迭代性**
    - 从初始参数开始，通过逐步更新参数（ 如 $\theta$ ）来最小化目标函数 $J(\theta)$
  - **基于导数**
    - 通常依赖一阶导数（ 梯度 ）或二阶导数（ Hessian ）
- 常见的优化器包括
  - 梯度下降：一阶方法，基于梯度 $\nabla J(\theta)$），如 BGD，SGD，MBGD，Adam
  - 牛顿法：二阶方法，基于梯度和 Hessian 矩阵
  - 准牛顿法：近似二阶方法，如 BFGS、L-BFGS

## 2. 梯度下降（ Gradient Descent，GD ）

- 核心思想是 **利用梯度 $\nabla J(\theta)$ 调整方向，利用学习率调整步长**，逐步更新参数，直到收敛到（ 或接近 ）最小值点

#### 2.1 核心步骤

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例，J 表示代价函数 ）
  - 从一个初始 w，b 开始
  - 不断的修改 w，b，去减少 J(w, b)
  - 直到 J 到达最小值（ 很可能收敛到局部最小值，而不是全局最小值 ）
  - 但是线性回归的代价函数是严格的凸二次函数，所以只有全局一个最小值（ 碗底 ）
- 如图，会不断找一个方向往谷底走
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)

#### 2.2 计算公式

- 公式如下
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  w &= \text{tmp}_w \\
  b &= \text{tmp}_b
  \end{align*}
  $$
  - **α 为学习率**，用于控制修改 w，b 的幅度，剩下的部分为求偏导
- 注意，不能用修改后的 w 去计算，下面是错误的
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  w &= \text{tmp}_w \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  b &= \text{tmp}_b
  \end{align*}
  $$

#### 2.3 学习率

- 取值通常是 0 到 1 之间的较小正数，可以从低到高尝试，0.001，0.01，0.1
  - 过小时，收敛速度慢，像下山时步伐太小，耗时长
  - 过大时，可能导致代价函数振荡或发散，像下山时步伐太大，跳过谷底，因此当代价函数 J 不下降或振荡时，可以尝试降低学习率

## 3. 基础方法

#### 3.1 批量梯度下降（ Batch Gradient Descent, BGD ）

- 在每次迭代中使用 **整个训练数据集** 来计算损失函数的梯度，并更新模型参数（ 权重 w 和偏置 b ）
- 优点
  - 收敛平稳
    - 由于使用了所有样本的梯度，梯度方向更准确，收敛路径平滑，通常能稳定地趋向全局最小值（ 对于凸函数 ）或局部最小值（ 对于非凸函数 ）
  - 梯度估计精确
    - 梯度基于整个数据集，避免了单样本噪声的影响
- 缺点
  - 计算开销大
    - 每次迭代需要遍历整个数据集，计算复杂度为 $O(m)$，对于大规模数据集（ 如百万级样本 ），计算速度慢
  - 内存需求高
    - 需要同时加载所有样本到内存，内存占用大，特别是在数据集较大时可能不可行
- 适用场景：
  - 数据集较小，通常几百到几千条样本
  - 对收敛稳定性和精度要求较高时
  - 硬件资源充足，内存和计算能力可以支持全数据集的处理

#### 3.2 随机梯度下降（ Stochastic Gradient Descent, SGD ）

- 在每次迭代中 **随机选择一个训练样本**，计算该样本的损失函数梯度，并用此梯度更新模型参数
- 优点
  - 计算速度快
    - 每次迭代只处理一个样本，计算复杂度为 $O(1)$，适合大规模数据集
  - 内存需求小
    - 只需加载一个样本到内存，内存占用极低
  - 逃逸局部最小值
    - 由于梯度估计的随机性，SGD 在非凸优化问题中可能通过波动跳出局部最小值，增加找到全局最小值的可能性
- 缺点
  - 收敛波动大
    - 由于每次仅基于单个样本的梯度，梯度估计噪声较大，导致收敛路径不稳定，可能在最小值附近振荡
  - 可能需要更多迭代
    - 虽然单次迭代快，但由于梯度噪声，总体收敛可能需要更多步数
- 适用场景
  - 数据集非常大，计算资源有限
  - 模型训练需要快速迭代以支持在线学习或实时更新
  - 非凸优化问题，希望通过随机性探索更优解

#### 3.3 小批量梯度下降（ Mini-batch Gradient Descent, MBGD ）

- 是 BGD 和 SGD 的折中方案，每次迭代 **使用一小部分训练样本**（ 称为一个小批量 ）来计算梯度并更新参数，小批量的样本数量通常设置为 2 的幂次方（ 如 32、64、128、256 等 ）
- 优点
  - 平衡效率与稳定性
    - 小批量梯度结合了 BGD 的稳定性和 SGD 的计算效率，梯度估计比 SGD 更准确，同时计算开销比 BGD 小
  - 适合 GPU 加速
    - 小批量大小为 2 的幂次方时，矩阵运算可以高效并行化，显著提高训练速度
  - 内存需求适中
    - 只需加载一个批次的样本，内存占用远低于 BGD
- 缺点
  - 超参数敏感
    - 小批量大小和学习率需要仔细调整，不合适的值可能导致收敛缓慢或不稳定
  - 仍可能有噪声
    - 虽然比 SGD 稳定，但小批量梯度的随机性仍可能导致收敛路径波动
- 适用场景：
  - 大多数深度学习任务（ 如神经网络训练 ），因为 MBGD 能够平衡计算效率和收敛稳定性
  - 数据集规模较大，但硬件支持 GPU 加速
  - 需要在稳定性和速度之间取得折中时

## 4. Adam（ adaptive moment estimation ）

#### 4.1 核心思路

- Adam 算法对每个参数（ 包括 $w_1$，$w_2$ ... $w_n$ 和 b ）独立计算自适应学习率 α
- 在统计学中，"矩" 是描述概率分布形态的核心指标
  - 一阶矩：均值，描述分布的中心位置
  - 二阶矩：方差，描述分布的离散程度
- Adam 将梯度看作随机变量，用矩描述其分布特性
  - 梯度的一阶矩，根据历史梯度方向记忆，优化更新向量的 **方向特性**
  - 梯度的二阶矩，根据梯度历史幅度，优化更新向量的 **步长大小**
  - **更新向量，指的是参数更新步骤中的位移向量**，即 表示参数空间中从当前位置指向新位置的位移向量

#### 4.2 动量机制

- Adam 存在动量机制
  - 普通梯度下降，每一步只根据当前位置的坡度调整方向
  - **Adam 会 "记住" 之前的运动方向，像有惯性一样保持运动趋势**
- 动量机制的核心就是计算梯度的一阶矩
  $$m_t = \beta_1·m_{t-1} + (1-\beta_1)·g_t$$
  - $g_t$：当前时间步 t 的梯度向量（ 注意是向量，表示了所有参数 ）
  - $m_t$：当前时间步 t 的一阶矩估计（ 动量 ）
  - $\beta_1$
    - 控制历史动量衰减的指数衰减率，通常 0.9 左右
    - $\beta_1$=0.9 表示 当前更新方向 = 90% 历史方向 + 10% 新梯度方向
- 偏差矫正
  $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
  - 修正初始迭代时的零偏置问题
  - 当 $t$ 增大时，$\beta_1^t$ 指数衰减趋近 0，校正因子 $1/(1-\beta_1^t) \to 1$

#### 4.3 梯度幅度

- Adam 为每个参数独立计算自适应学习率，基于梯度的二阶矩（ 方差估计 ）
  - **根据每个参数的 梯度历史幅度 动态缩放学习率**
  - 梯度大的方向（ 对应参数变化剧烈 ）学习率变小
  - 梯度小的方向（ 对应参数变化缓慢 ）学习率变大
- 计算二阶矩，累积梯度平方的指数加权平均
  $$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$$
  - $g_t$：当前时间步 t 的梯度向量
  - $v_t$：当前时间步 t 的二阶矩估计
  - $\beta_2$：二阶矩衰减率（ 默认 0.999 ），提供长期记忆
- 偏差校正
  $$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
  - 修正初始估计偏低问题

#### 4.4 自适应缩放学习率

- 自适应缩放学习率
  $$\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
  - $\hat{m}_t$：校正后动量，方向平滑与加速
  - $\frac{1}{\sqrt{\hat{v}_t}}$：自适应缩放因子，步长动态调整
  - $\alpha$：基础学习率，全局步长控制
  - $\epsilon$：数值稳定项，防止除零错误
  - $\theta_t$：更新向量

## 5. 牛顿法

#### 5.1 基础介绍

- 牛顿法不属于梯度下降，与梯度下降仅使用一阶导数（ 梯度 ）不同，牛顿法通过二阶导数（ Hessian 矩阵 ）捕捉函数的曲率，使更新方向更精确，收敛速度更快，尤其在接近最优解时
- 核心思想是 **通过在当前参数点对目标函数进行二阶泰勒展开，将函数近似为二次函数，然后直接求解该二次函数的最优解，确定参数更新方向和步长**

#### 5.2 工作原理

- 对于目标函数 $J(\theta)$，牛顿法假设函数在当前参数 $\theta_t$ 附近可以用二阶泰勒展开近似
  $$
  J(\theta) \approx J(\theta_t) + \nabla J(\theta_t)^T (\theta - \theta_t) + \frac{1}{2} (\theta - \theta_t)^T H(\theta_t) (\theta - \theta_t)
  $$
  - $\nabla J(\theta_t)$：梯度向量，表示函数在 $\theta_t$ 处的一阶导数
  - $H(\theta_t)$：Hessian 矩阵，表示函数在 $\theta_t$ 处的二阶导数矩阵，定义为
    $$
    H(\theta_t) = \nabla^2 J(\theta_t) = \left[ \frac{\partial^2 J}{\partial \theta_i \partial \theta_j} \right]_{n \times n}
    $$
- 为了找到二次近似函数的最小值，对 $\theta$ 求导并令其为零
  $$
  \nabla J(\theta_t) + H(\theta_t) (\theta - \theta_t) = 0
  $$
- 解得参数更新公式
  $$
  \theta_{t+1} = \theta_t - H(\theta_t)^{-1} \nabla J(\theta_t)
  $$
  - $H(\theta_t)^{-1}$ 是 Hessian 矩阵的逆，用于调整梯度方向和步长
  - 如果 $H(\theta_t)$ 是正定的（对于凸函数），更新方向保证是下降方向

#### 5.3 优缺点

- 优点
  - 快速收敛
    - 在接近最优解时，牛顿法具有二次收敛性（ 误差平方级别减少 ），远快于梯度下降的线性收敛
  - 精确方向
    - 利用 Hessian 矩阵的曲率信息，更新方向比梯度下降更精确，尤其在函数形状复杂（如非均匀缩放的谷底）时
  - 适合凸问题
    - 对于严格凸函数（ 如线性回归的二次损失函数 ），牛顿法通常能快速收敛到全局最优解
- 缺点
  - 计算复杂度高
    - 计算 Hessian 矩阵需要 $O(n^2)$ 的复杂度，求其逆需要 $O(n^3)$，其中 $n$ 是参数数量，适合低维问题
  - 内存需求大
    - 存储 $n * n$ 的 Hessian 矩阵需要 $O(n^2)$ 的内存，高维问题（如深度学习）不可行
  - 对非凸问题不稳定
    - 如果 Hessian 矩阵非正定（ 如在鞍点或非凸区域 ），更新方向可能不合理，导致发散或收敛到非最优解
  - 对初始点敏感
    - 初始参数选择不当可能导致收敛到局部极值或发散
  - 要求函数二阶可微
    - 目标函数必须具有二阶导数，对于不可微或噪声大的函数效果不佳

## 6. 准牛顿法

#### 6.1 基础介绍

- 准牛顿法是牛顿法的一种改进，保留了牛顿法利用曲率信息的优点，**旨在解决牛顿法计算和存储完整 Hessian 矩阵，导致 计算 和 内存 成本过高的问题**
- 核心思想是 **准牛顿法基于牛顿法的二阶优化框架，但使用低秩更新或有限内存的方式近似 Hessian 矩阵（ 或其逆 ），利用历史梯度和参数变化信息来构建更新方向，从而无需直接计算目标函数的二阶导数 Hessian 矩阵**

#### 6.2 工作原理

- 准牛顿法假设目标函数 $J(\theta)$ 在当前参数 $\theta_t$ 附近的二阶信息可以通过历史迭代数据近似，而不是直接计算 Hessian 矩阵 $H(\theta_t)$
- 准牛顿法通过以下方式构建近似：
  - 使用参数变化向量 $\Delta \theta_t = \theta_t - \theta_{t-1}$ 和 梯度变化向量 $\Delta g_t = \nabla J(\theta_t) - \nabla J(\theta_{t-1})$ 来更新 Hessian 的近似（ 或其逆 ）
  - 常见的准牛顿法（如 BFGS）通过迭代更新公式构造 Hessian 逆的近似矩阵 $B_t \approx H(\theta_t)^{-1}$
- 更新公式类似于牛顿法
  $$\theta_{t+1} = \theta_t - \alpha_t B_t \nabla J(\theta_t)$$
  - $B_t$：Hessian 逆的近似矩阵
  - $\alpha_t$：步长，通常通过线搜索（ 如 Wolfe 条件 ）确定
- 准牛顿法需满足 secant 条件（ 割线条件 ），确保近似矩阵 $B_t$ 或 $H_t$ 与函数的二阶性质一致
  $$
  H_t (\theta_t - \theta_{t-1}) = \nabla J(\theta_t) - \nabla J(\theta_{t-1})$$ 或等价地： $$B_t (\nabla J(\theta_t) - \nabla J(\theta_{t-1})) = \theta_t - \theta_{t-1}
  $$

#### 6.3 常见准牛顿法

- BFGS（ Broyden-Fletcher-Goldfarb-Shanno ）
  - 最常用的准牛顿法，通过以下公式更新 Hessian 逆的近似 $B_t$
    $$
    B_{t+1} = B_t + \frac{(\Delta \theta_t)(\Delta \theta_t)^T}{(\Delta \theta_t)^T \Delta g_t} - \frac{(B_t \Delta g_t)(B_t \Delta g_t)^T}{\Delta g_t^T B_t \Delta g_t}
    $$
  - 保持 Hessian 近似的正定性，适合凸优化问题
- L-BFGS（ Limited-memory BFGS ）
  - BFGS 的有限内存版本，仅存储最近 $m$ 次（ 通常 3-20 ）的参数变化和梯度变化向量对，隐式构造 Hessian 逆
  - 通过两阶段循环计算更新方向，内存需求低，适合高维问题
- 其他变种：如 DFP、SR1 等，区别在于更新公式的形式和稳定性

#### 6.4 适用场景

- 中小规模优化问题
  - 数据集和参数规模适中，内存足以支持存储向量对
- 凸优化问题
  - 如逻辑回归、支持向量机等，准牛顿法能快速收敛到全局最优解
- 高精度优化
  - 需要较高精度的解，且计算资源允许时
- 传统机器学习任务
  - 适合非深度学习的模型训练，L-BFGS 在这些场景中表现优于梯度下降变种
