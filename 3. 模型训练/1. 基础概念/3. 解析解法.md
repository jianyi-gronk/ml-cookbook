## 1. 核心思想

- 与优化器迭代逼近最优解不同，解析解法 **通过数学公式直接计算目标函数 $J(\theta)$ 的全局最优解，无需迭代过程**
- 思路为 令目标函数的偏导数等于零，求解极值点，得到闭式解（ 闭式解即通过有限的数学运算直接计算得到的精确解 ）

## 2. 矩阵表示

- 定义设计矩阵 $\mathbf{X}$
  $$
  \mathbf{X} = \begin{bmatrix}
  1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
  1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
  \end{bmatrix}
  $$
  - 其中，每行表示一个样本（ $m$ 个样本 ），每列表示一个特征（ $n$ 个特征加偏置项 ）
- 目标向量 $\mathbf{y}$
  $$
  \mathbf{y} = \begin{bmatrix}
  y^{(1)} \\
  y^{(2)} \\
  \vdots \\
  y^{(m)}
  \end{bmatrix}
  $$
  - 表示 $m$ 个样本的真实输出值
- 参数向量 $\mathbf{w}$
  $$
  \mathbf{w} = \begin{bmatrix}
  b \\
  w_1 \\
  w_2 \\
  \vdots \\
  w_n
  \end{bmatrix}
  $$
  - 包含偏置 $b$ 和权重 $w_1, w_2, \ldots, w_n$

## 3. 正规方程

- 正规方程是线性回归中直接求解最优参数 w 的闭式解公式，通过矩阵运算直接计算全局最优解
  $$
  \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
  $$
- 推导过程
  - 代价函数：线性回归的目标是最小化均方误差，矩阵形式为
    $$
    J(\mathbf{w}) = \frac{1}{2m} (\mathbf{X}\mathbf{w} - \mathbf{y})^T(\mathbf{X}\mathbf{w} - \mathbf{y})
    $$
  - 求导：对 $\mathbf{w}$ 求偏导并令其为零，找到极值点
    $$
    \nabla_{\mathbf{w}} J(\mathbf{w}) = \frac{1}{m} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y}) = 0
    $$
  - 解方程：化简得
    $$
    \mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T \mathbf{y}
    $$
  - 若 $\mathbf{X}^T \mathbf{X}$ 可逆，解得
    $$
    \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
    $$

## 4. 优缺点

- 优点
  - 精确性：直接计算全局最优解，无近似误差
  - 无需迭代：一步到位，适合小规模问题或解析解可行的情况
- 缺点
  - 计算成本高
    - $O(n^3)$，主要源于矩阵 $\mathbf{X}^T\mathbf{X}$ 的求逆，特征维度 $n > 10000$ 时计算成本高
  - 内存需求大
    - 存储 $\mathbf{X}^T \mathbf{X}$ 需 $O(n^2)$ 空间
  - 局限性
    - 仅适用于 $\mathbf{X}^T \mathbf{X}$ 可逆的凸问题，不适合非凸或高维复杂问题（ 如深度学习 ）
    - 同时样本数需要大于特征数（ 不能等于 ），以确保 $\mathbf{X}^T\mathbf{X}$ 满秩

## 5. 适用场景

- 适合
  - 小规模数据集 和 凸优化问题（ 如线性回归 ）
  - 需要高精度解且计算资源充足的场景
- 不适合
  - 高维特征（ $n$ 过大 ）或大规模样本（ $m$ 过大 ），因计算和内存成本高
  - 非凸问题或无闭式解的复杂函数（ 如神经网络损失函数 ）
