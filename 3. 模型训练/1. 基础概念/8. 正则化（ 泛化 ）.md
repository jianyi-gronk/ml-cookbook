## 1. 正则化（ Regularization ）

- 模型可能在训练数据上表现很好，但在测试数据上表现差，因其 “记住” 训练数据的细节（ 包括噪声 ），即过拟合
- **正则化 在 原始损失函数 中 引入额外项**，也就是目标函数变成了 原始损失函数 + 额外项，以便防止过拟合和提高模型泛化性能
  $$\mathcal{L} = \mathcal{L}_{\text{original}} + \lambda \cdot \mathcal{R}(\theta)$$
  - $\mathcal{L}_{\text{original}}$：原始损失函数，如均方误差、交叉熵
  - $\mathcal{R}(\theta)$：正则化项，如权重范数
  - $\lambda$：正则化强度，通常是超参数，控制惩罚力度
- 常用的额外项一般有两种，L1 正则化和 L2 正则化

## 2. L1 正则化

- **L1 正则化在损失函数中添加权重的绝对值之和（ L1 范数 ）作为惩罚项**
  $$\mathcal{R}(\theta) = \sum_{i} |w_i|$$
- 目标函数变为
  $$\mathcal{L} = \mathcal{L}_{\text{original}} + \lambda \cdot \sum_{i} |w_i|$$
  - 其中 $w_i$ 是模型的权重参数，$\lambda$ 控制正则化强度
- L1 正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择
  - “稀疏性” 就是模型的很多参数是 0
  - 相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力

## 3. L2 正则化

- **L2 正则化在损失函数中添加权重的平方和（ L2 范数 ）作为惩罚项**
  $$\mathcal{R}(\theta) = \sum_{i} w_i^2$$
- 目标函数变为
  $$\mathcal{L} = \mathcal{L}_{\text{original}} + \lambda \cdot \sum_{i} w_i^2$$
- L2 通过减少模型参数的权值来控制过拟合的效果，L2 中模型参数 W 中每个元素都很小，接近于 0，一般不会等于 0
  - 拟合过程中通常倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型，因为通常抗扰动能力更优秀
  - 比如对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响
- 实际正则化中使用 L2 的会更多一些
  - 因为 L1 会趋向于提取少量的有效特征项，而 L2 会选择更多的特征
  - 不过所有特征中只有少量特征其重要作用的情况，可以选择 L1 来自动选择比较合适的特征属性
