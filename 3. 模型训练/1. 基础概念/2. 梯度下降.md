## 4. 梯度下降（ Gradient Descent，GD ）

#### 4.1 核心步骤

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例 ）
  1. 从一个初始 w，b 开始
  2. 不断的修改 w，b，去减少 J(w, b)
  3. 直到 J 到达最小值
  - 一般是局部最小值，不是全局最小值，因此可能有多个最小值
  - 但是线性回归的代价函数是严格的凸二次函数，所以只有全局一个最小值（ 碗底 ）

#### 4.2 梯度下降公式

- α 为学习率，通常是 0 到 1 之间的较小正数（ 例 0.01 ），控制修改 w，b 的幅度，剩下的部分为求偏导
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  w &= \text{tmp}_w \\
  b &= \text{tmp}_b
  \end{align*}
  $$
- 注意，不能用修改后的 w 去计算，下面是错误的
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  w &= \text{tmp}_w \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  b &= \text{tmp}_b
  \end{align*}
  $$
- 梯度下降公式中求导公式推导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/612b5384-2789-4186-9fc2-11609406f7e8)

#### 4.3 学习率

- 当随着迭代次数增加，代价函数结果变化异常，需判断是否因为代码异常或者学习率过大，可以通过将学习率设置成非常小的值，如果还不是下降趋势，那么就是代码问题
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a9b2b4e4-1e76-420d-9bd0-4e05b0dfca8e)
- 学习率的选择，可以从低到高尝试，0.001，0.01，0.1

#### 4.4 梯度下降方法细节

- **通过精确计算梯度确定当前唯一最优方向，步长由学习率调整**
- 梯度下降常用基础方法
  - 批量梯度下降（ Batch Gradient Descent, BGD ）
    - 每次迭代使用全部训练样本，收敛平稳但速度慢，内存需求大
  - 随机梯度下降（ Stochastic Gradient Descent, SGD ）
    - 每次迭代使用单个样本，收敛快但波动大，内存需求小
  - 小批量梯度下降（ Mini-batch Gradient Descent, MBGD ）
    - 代表每次迭代训练部分样本，通常样本数设置为 2 的幂次方，通常设置 2，4，8，16，32，64，128，256，512（ 设置成 2 的幂次方，更有利于 GPU 加速，并且很少设置大于 512 ）
    - 平衡 BGD 的稳定性和 SGD 的速度
- 梯度下降常用进阶方法
  - Adam 算法（ adaptive moment estimation ）
    - 核心思路是，如果 $w_j$ 持续向一个方向移动，那么就增加学习率 α，如果 $w_j$ 持续向另一个方向移动的话，就减小学习率 α
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4afcfcdc-2310-42f1-aafb-40003502cc6b)
    - Adam 算法对不同的 $w_1$，$w_2$ ... $w_n$ 和 b 都有着不同的学习率 α

#### 4.5 举例概括

- 如图
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)
- 但在线性回归中，有且只有一个最小值，成碗状图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/109b13d7-41fd-4f96-b6e2-f05fec3953c8)
- 当随着迭代次数增加，代价函数结果趋于稳定，则梯度下降收敛，可通过看曲线图或者判断代价函数结果是否小于阈值
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/896033c0-f761-4feb-8298-cb3a6e7fc598)
