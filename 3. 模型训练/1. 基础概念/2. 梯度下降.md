## 1. 梯度下降（ Gradient Descent，GD ）

- 核心是 **通过精确计算梯度确定当前唯一最优方向，步长由学习率调整**

#### 1.1 核心步骤

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例，J 表示代价函数 ）
  - 从一个初始 w，b 开始
  - 不断的修改 w，b，去减少 J(w, b)
  - 直到 J 到达最小值（ 很可能收敛到局部最小值，而不是全局最小值 ）
  - 但是线性回归的代价函数是严格的凸二次函数，所以只有全局一个最小值（ 碗底 ）
- 如图，会不断找一个方向往谷底走
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)

#### 1.2 计算公式

- 公式如下
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  w &= \text{tmp}_w \\
  b &= \text{tmp}_b
  \end{align*}
  $$
  - **α 为学习率**，用于控制修改 w，b 的幅度，剩下的部分为求偏导
- 注意，不能用修改后的 w 去计算，下面是错误的
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  w &= \text{tmp}_w \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  b &= \text{tmp}_b
  \end{align*}
  $$

#### 1.3 学习率

- 取值通常是 0 到 1 之间的较小正数，可以从低到高尝试，0.001，0.01，0.1
  - 过小时，收敛速度慢，像下山时步伐太小，耗时长
  - 过大时，可能导致代价函数振荡或发散，像下山时步伐太大，跳过谷底，因此当代价函数 J 不下降或振荡时，可以尝试降低学习率

## 2. 基础方法

#### 2.1 批量梯度下降（ Batch Gradient Descent, BGD ）

- 在每次迭代中使用 **整个训练数据集** 来计算损失函数的梯度，并更新模型参数（ 权重 w 和偏置 b ）
- 优点
  - 收敛平稳
    - 由于使用了所有样本的梯度，梯度方向更准确，收敛路径平滑，通常能稳定地趋向全局最小值（ 对于凸函数 ）或局部最小值（ 对于非凸函数 ）
  - 梯度估计精确
    - 梯度基于整个数据集，避免了单样本噪声的影响
- 缺点
  - 计算开销大
    - 每次迭代需要遍历整个数据集，计算复杂度为 $O(m)$，对于大规模数据集（ 如百万级样本 ），计算速度慢
  - 内存需求高
    - 需要同时加载所有样本到内存，内存占用大，特别是在数据集较大时可能不可行
- 适用场景：
  - 数据集较小，通常几百到几千条样本
  - 对收敛稳定性和精度要求较高时
  - 硬件资源充足，内存和计算能力可以支持全数据集的处理

#### 2.2 随机梯度下降（ Stochastic Gradient Descent, SGD ）

- 在每次迭代中 **随机选择一个训练样本**，计算该样本的损失函数梯度，并用此梯度更新模型参数
- 优点
  - 计算速度快
    - 每次迭代只处理一个样本，计算复杂度为 $O(1)$，适合大规模数据集
  - 内存需求小
    - 只需加载一个样本到内存，内存占用极低
  - 逃逸局部最小值
    - 由于梯度估计的随机性，SGD 在非凸优化问题中可能通过波动跳出局部最小值，增加找到全局最小值的可能性
- 缺点
  - 收敛波动大
    - 由于每次仅基于单个样本的梯度，梯度估计噪声较大，导致收敛路径不稳定，可能在最小值附近振荡
  - 可能需要更多迭代
    - 虽然单次迭代快，但由于梯度噪声，总体收敛可能需要更多步数
- 适用场景
  - 数据集非常大，计算资源有限
  - 模型训练需要快速迭代以支持在线学习或实时更新
  - 非凸优化问题，希望通过随机性探索更优解

#### 2.3 小批量梯度下降（ Mini-batch Gradient Descent, MBGD ）

- 是 BGD 和 SGD 的折中方案，每次迭代 **使用一小部分训练样本**（ 称为一个小批量 ）来计算梯度并更新参数，小批量的样本数量通常设置为 2 的幂次方（ 如 32、64、128、256 等 ）
- 优点
  - 平衡效率与稳定性
    - 小批量梯度结合了 BGD 的稳定性和 SGD 的计算效率，梯度估计比 SGD 更准确，同时计算开销比 BGD 小
  - 适合 GPU 加速
    - 小批量大小为 2 的幂次方时，矩阵运算可以高效并行化，显著提高训练速度
  - 内存需求适中
    - 只需加载一个批次的样本，内存占用远低于 BGD
- 缺点
  - 超参数敏感
    - 小批量大小和学习率需要仔细调整，不合适的值可能导致收敛缓慢或不稳定
  - 仍可能有噪声
    - 虽然比 SGD 稳定，但小批量梯度的随机性仍可能导致收敛路径波动
- 适用场景：
  - 大多数深度学习任务（ 如神经网络训练 ），因为 MBGD 能够平衡计算效率和收敛稳定性
  - 数据集规模较大，但硬件支持 GPU 加速
  - 需要在稳定性和速度之间取得折中时

## 3. Adam（ adaptive moment estimation ）

#### 3.1 核心思路

- Adam 算法对每个参数（ 包括 $w_1$，$w_2$ ... $w_n$ 和 b ）独立计算自适应学习率 α
- 在统计学中，"矩" 是描述概率分布形态的核心指标
  - 一阶矩：均值，描述分布的中心位置
  - 二阶矩：方差，描述分布的离散程度
- Adam 将梯度看作随机变量，用矩描述其分布特性
  - 梯度的一阶矩，根据历史梯度方向记忆，优化更新向量的 **方向特性**
  - 梯度的二阶矩，根据梯度历史幅度，优化更新向量的 **步长大小**
  - **更新向量，指的是参数更新步骤中的位移向量**，即 表示参数空间中从当前位置指向新位置的位移向量

#### 3.2 动量机制

- Adam 存在动量机制
  - 普通梯度下降，每一步只根据当前位置的坡度调整方向
  - Adam 会 "记住" 之前的运动方向，像有惯性一样保持运动趋势
- 动量机制的核心就是计算梯度的一阶矩
  $$m_t = \beta_1·m_{t-1} + (1-\beta_1)·g_t$$
  - $g_t$：当前时间步 t 的梯度向量（ 注意是向量，表示了所有参数 ）
  - $m_t$：当前时间步 t 的一阶矩估计（ 动量 ）
  - $\beta_1$
    - 控制历史动量衰减的指数衰减率，通常 0.9 左右
    - $\beta_1$=0.9 表示 当前更新方向 = 90% 历史方向 + 10% 新梯度方向
- 偏差矫正
  $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
  - 修正初始迭代时的零偏置问题
  - 当 $t$ 增大时，$\beta_1^t$ 指数衰减趋近 0，校正因子 $1/(1-\beta_1^t) \to 1$

#### 3.3 梯度幅度

- Adam 为每个参数独立计算自适应学习率，基于梯度的二阶矩（ 方差估计 ）
  - 根据每个参数的 梯度历史幅度 动态缩放学习率
  - 梯度大的方向（ 对应参数变化剧烈 ）学习率变小
  - 梯度小的方向（ 对应参数变化缓慢 ）学习率变大
- 计算二阶矩，累积梯度平方的指数加权平均
  $$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$$
  - $g_t$：当前时间步 t 的梯度向量
  - $v_t$：当前时间步 t 的二阶矩估计
  - $\beta_2$：二阶矩衰减率（ 默认 0.999 ），提供长期记忆
- 偏差校正
  $$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
  - 修正初始估计偏低问题

#### 3.4 自适应缩放学习率

- 自适应缩放学习率
  $$\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
  - $\hat{m}_t$：校正后动量，方向平滑与加速
  - $\frac{1}{\sqrt{\hat{v}_t}}$：自适应缩放因子，步长动态调整
  - $\alpha$：基础学习率，全局步长控制
  - $\epsilon$：数值稳定项，防止除零错误
  - $\theta_t$：更新向量

## 4. L-BFGS（ Limited-memory Broyden–Fletcher–Goldfarb–Shanno ）
