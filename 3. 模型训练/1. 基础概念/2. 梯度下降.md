## 1. 梯度下降（ Gradient Descent，GD ）

#### 1.1 核心步骤

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例，J 表示代价函数 ）
  - 从一个初始 w，b 开始
  - 不断的修改 w，b，去减少 J(w, b)
  - 直到 J 到达最小值（ 很可能收敛到局部最小值，而不是全局最小值 ）
  - 但是线性回归的代价函数是严格的凸二次函数，所以只有全局一个最小值（ 碗底 ）
- 如图，会不断找一个方向往谷底走
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)
- 核心是 **通过精确计算梯度确定当前唯一最优方向，步长由学习率调整**

#### 1.2 计算公式

- 公式如下
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  w &= \text{tmp}_w \\
  b &= \text{tmp}_b
  \end{align*}
  $$
  - **α 为学习率**，用于控制修改 w，b 的幅度，剩下的部分为求偏导
- 注意，不能用修改后的 w 去计算，下面是错误的
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  w &= \text{tmp}_w \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  b &= \text{tmp}_b
  \end{align*}
  $$

#### 1.3 学习率

- 取值通常是 0 到 1 之间的较小正数，可以从低到高尝试，0.001，0.01，0.1
  - 过小时，收敛速度慢，像下山时步伐太小，耗时长
  - 过大时，可能导致代价函数振荡或发散，像下山时步伐太大，跳过谷底，因此当代价函数 J 不下降或振荡时，可以尝试降低学习率

## 2. 基础方法

-
- 梯度下降常用基础方法
  - 批量梯度下降（ Batch Gradient Descent, BGD ）
    - 每次迭代使用全部训练样本，收敛平稳但速度慢，内存需求大
  - 随机梯度下降（ Stochastic Gradient Descent, SGD ）
    - 每次迭代使用单个样本，收敛快但波动大，内存需求小
  - 小批量梯度下降（ Mini-batch Gradient Descent, MBGD ）
    - 代表每次迭代训练部分样本，通常样本数设置为 2 的幂次方，通常设置 2，4，8，16，32，64，128，256，512（ 设置成 2 的幂次方，更有利于 GPU 加速，并且很少设置大于 512 ）
    - 平衡 BGD 的稳定性和 SGD 的速度

## 2. Adam

#### 4.4 梯度下降方法细节

- 梯度下降常用进阶方法
  - Adam 算法（ adaptive moment estimation ）
    - 核心思路是，如果 $w_j$ 持续向一个方向移动，那么就增加学习率 α，如果 $w_j$ 持续向另一个方向移动的话，就减小学习率 α
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4afcfcdc-2310-42f1-aafb-40003502cc6b)
    - Adam 算法对不同的 $w_1$，$w_2$ ... $w_n$ 和 b 都有着不同的学习率 α
