## 1. 归一化

#### 1.1 背景

- **在神经网络的训练中，各层输入（ 即上一层的输出 ）的分布会随着参数更新而不断变化**，这种现象被称为内部协变量偏移
- 这种分布变化可能导致以下问题：
  - 梯度消失或爆炸：当激活值分布偏离合理范围时（ 如过大或过小 ），梯度可能变得极小或极大，阻碍优化过程
  - 训练不稳定：不同层的激活值分布差异会导致模型对初始参数和学习率敏感，增加训练难度
  - 收敛速度慢：分布不稳定的激活值可能导致梯度下降路径曲折，延长收敛时间

#### 1.2 基础介绍

- 和数据预处理的归一化类似，目的是调整数据分布，优化模型训练
- 区别在于使用的场景不同
  - 数据预处理的归一化
    - 模型训练前，对输入数据进行的标准化处理，统一特征尺度
  - 神经网络的归一化
    - 模型训练中（ **通常在 线性变化之后，激活函数之前** ），对每一层线性变换的结果进行标准化，以稳定中间层激活值的分布
- 神经网络中常用的归一化方法包括 批归一化 和 层归一化

## 2. 批归一化（ BatchNormalization，BN ）

- BN 通过 **对每个 batch 中的数据进行标准化处理，使得每层输入的 均值 接近 0，方差 接近 1，并且将标准化后的数据通过 缩放 和 平移**，将数据重新映射到一个更合适的分布范围

#### 2.1 详细步骤

- 计算 batch 中数据的均值
  $$\mu = \frac{1}{m} \sum_{i=1}^m x_i$$
  - 其中，\( x_i \) 是 batch 中第 \( i \) 个样本的线性变换结果，\( m \) 是 batch size。
- 计算 batch 中数据的方差
  $$\sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2$$
- 标准化
  $$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
  - 其中，$\epsilon$ 是一个小常数（ 通常为 $10^{-5}$ ），用于防止除零错误
- 缩放和平移：
  $$y_i = \gamma \hat{x}_i + \beta$$
  - 其中，$\gamma$ 和 $\beta$ 是可学习的缩放和平移参数

#### 2.2 不同场景

- 卷积神经网络
  - BN 常用于深度卷积神经网络（ 如 ResNet ）的卷积层之后，因卷积层的特征图具有空间一致性，BN 能有效标准化这些特征，减少内部协变量偏移
  - 空间一致性指卷积层特征图上所有像素都由同一卷积核生成，捕捉同一种特征（ 如边缘或纹理 ），因此数值分布（ 亮度或强度 ）在统计上相似
- 全连接层
  - BN 较少用于全连接层，因其输出缺乏特征图的空间结构，内部协变量偏移问题不显著，归一化收益较低
- 较大 batch size 场景
  - BN 依赖 batch 内样本的均值和方差计算，所以当 batch size 过小时，统计量不稳定，会导致归一化效果下降
- 生命周期
  - 在推理阶段（ 即模型用于预测或测试时 ），BN 使用训练过程中通过滑动平均累积的全局均值和方差，以确保对任意输入（ 包括单样本或不同 batch size ）进行一致的标准化

## 3. 层归一化（ LayerNormalization，LN ）

#### 3.1 与 LN 对比

- **BN 是对全部样本的一个特征进行归一化，而 LN 是对一个样本的全部特征进行归一化**，如果对人身体特征进行归一化，比如对身高，体重和年龄一起求一个均值方差，那确实看上去毫无作用，但在 NLP 领域却非常有效果
- 并且与 BN 不同，LN 不依赖 batch 内的统计量，而是对单个样本内的特征进行归一化，因此适用于 batch size 小或序列长度不一致的场景
- 和 LN 一样，可以通过可学习的缩放参数 $\gamma$ 和平移参数 $\beta$，LN 调整标准化后的数据，保留网络的表达能力

#### 3.2 在 NLP 领域的优势

- 在 NLP 中，n 个特征都可能表示不同的词，如果仍然采用 BatchNormalization 的话，对第一个词进行操作，很显然意义就不是非常大了，因为任何一个词都可以放在第一个位置，而且很多时候词序对于我们对于句子的影响没那么大，而此时我们对 n 个词进行 LN 等操作可以很好地有效捕捉样本（ 如句子 ）的整体分布特性，而不受词序或 batch 大小的影响

#### 3.3 不同场景

- NLP 任务
  - LN 常用于自然语言处理中的序列模型（ 如 RNN、Transformer ），例如，在 Transformer 中，LN 应用于每个样本的隐藏状态（如注意力层的输出），稳定句子的特征分布，而不关心词序或 batch 中其他样本。
- 小 batch size 或单样本场景
  - LN 不依赖 batch 统计量，适合 batch size 为 1 或动态变化的场景
- 生命周期
  - LN 在训练和推理阶段都使用样本内统计量，无需存储全局均值和方差，计算更简单
