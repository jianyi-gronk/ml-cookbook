## 1. 线性回归

- 模型与数据拟合成一条直线
- 本质就是求解方程，w，b 是模型的参数（ 也可以叫系数，权重，是可以调整的模型变量 ）
  $$f_{w,b}(x) = wx + b$$

## 2. 代价函数

- 数据和直线不会完全拟合，因此需要 代价函数 J(w, b) 来衡量误差，**而线性回归的核心就是最小化代价函数**
- **线性回归的代价函数通常为 MSE**，为了方便求导，在公式里除以 2，相当于 $\frac{1}{2}$ MSE（ 但效果一样 ）
  $$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2$$

## 7. 多元线性回归

- 多维线性回归公式
  $$f_{w,b}(\mathbf{x}) = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b$$
- 常用符号
  $$
  \begin{align*}
  x_j &= j^{\text{th}} \text{ feature} \\
  n &= \text{number of features} \\
  \vec{x}^{(i)} &= \text{features of } i^{\text{th}} \text{ training example} \\
  x_j^{(i)} &= \text{value of feature } j \text{ in } i^{\text{th}} \text{ training example}
  \end{align*}
  $$
- 公式还可以写成（ 向量点积 ）
  $$f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b = w_1 x_1 + w_2 x_2 + w_3 x_3 + \cdots + w_n x_n + b$$
  - 转换成代码
    ```
    w = np.array([1.0, 2.5, -3.3])
    x = np.array([10, 20, 30])
    b = 4
    f = np.dot(w, x) + b;
    ```
  - 并且这种向量化计算，性能更好，无论是否使用 GPU。如果不使用向量计算，则是一个一个计算后累加，如果使用向量计算，计算机会在一步中并行计算每对 (w, x) 乘积，并将每对计算结果通过专门的硬件进行高效累加
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e7bc0c60-c536-4570-9999-08c89ab7d7a9)
- 梯度下降
  $$
  \begin{align*}
  \text{repeat } \{ & \\
  \quad w_j &= w_j - \alpha \frac{\partial}{\partial w_j} J(\vec{w}, b) \\
  \quad b &= b - \alpha \frac{\partial}{\partial b} J(\vec{w}, b) \\
  \} &
  \end{align*}
  $$
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c693468b-0d16-48af-810d-3d6160ac97bb)

## 8. 多项式回归

- 如果数据不是直线，而是曲线，那么可能多项式回归的曲线更能符合数据集
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2f52c6b7-1064-44f6-bba4-626aac6cbc97)
- 这个时候通常特征缩放非常重要，因为 x 的范围是 1-1000 的话，x 的三次方范围就是 1-1 亿

## 9. 实战

- 在 sklearn.linear_model 中，能找到对应的现成模型使用
  | 方法 | sklearn 类 | 适用场景 | 关键参数 |
  |--------------------|-------------------------|-----------------------------------|-------------------|
  | **普通线性回归** | `LinearRegression` | 小数据、无多重共线性 | `fit_intercept` |
  | **岭回归** | `Ridge` | 多重共线性数据 | `alpha` (λ) |
  | **Lasso 回归** | `Lasso` | 特征选择、高维数据 | `alpha` (λ) |
  | **随机梯度下降** | `SGDRegressor` | 大规模数据 | `loss`, `penalty` |
  | **多项式回归** | `PolynomialFeatures` + 线性模型 | 非线性关系 | `degree`
