## 1. 逻辑回归

- 虽然名字里带有回归，但是是为了解决分类问题

#### 1.1 分类问题

- 对于分类问题，线性回归通常表现都不好
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e2163523-8350-44bb-a233-d547baae8124)
- 而逻辑回归就是在线性回归的基础上，加上了一层映射，把线性回归的结果映射到 0 到 1 之间，通常用于二分类问题

#### 1.2 映射函数

- 常用函数 Sigmoid 函数，也叫 Logistic 函数，取值范围为 (0,1)，它可以将一个实数映射到 (0,1) 的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好，优点是平滑，易于求导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/923d6f1c-158a-4da6-a265-e6f9f18656e2)
- Sigmoid 函数定义为
  $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
  - z 就是线性回归的结果

## 2. 决策边界

- z = 0 且划分两个集合，分别对应两个分类的线，被称为决策边界
- 比如，决策边界右边是 y = 1 的集合，左边是 y = 0 的集合
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/1e7c9b42-0002-4ff2-9133-c74082785c65)
- 也有可能是曲线
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/37bb2ac6-d0f8-47e8-8dec-6a186b2cf923)
- 也有可能是更复杂的不规则曲线
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/10ae28bb-a268-4c49-b4a4-6c83be9e8d73)

## 3. 代价函数

#### 3.1 代价函数公式

- 逻辑回归预测的是概率（ 如 $P(y=1∣x)$ ），因此不能和线性回归使用相同的代价函数，得到的会是非凸函数，有很多局部最小值影响
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/31cae5eb-ca20-4348-bbc8-1ee4f7115e80)
- 逻辑回归使用**对数损失函数**（ Log Loss ），也称为**交叉熵损失**，其数学表达式为：
  $$
  J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
  $$
  - $m$：样本数量
  - $y^{(i)}$：第 i 个样本的真实标签（ 0 或 1 ）
  - $\hat{y}^{(i)}$：模型预测 $y^{(i)} = 1$ 的概率（ Sigmoid 函数的输出 ）
  - $\mathbf{w}$：权重向量
  - $b$：偏置项

#### 3.2 公式推理过程

- 因此需要将计算误差的公式进行替换，根据真实标签的值不同，Cost 公式也不同
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/47b30024-306f-436a-81d0-754394b7b83a)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3de79591-7591-4737-bae9-cface535b4c8)
- 最终函数的曲线为（ 注意 f 的取值在 0 - 1 区间，且为 -log ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4283f93d-63d5-4a7f-ac8a-4d391d6590b3)
- 真实标签为 1 时，f = 1 时，接近真实标签，那么损失很小，f 接近 0 时，离真实标签相差大，那么损失大
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/7ee51183-4510-4be8-8146-baba5bf70f39)
- 真实标签为 0 时同理
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/f1f5be35-f517-4a13-9e08-dbe9720734a0)
- 简化误差函数（ 即缩略成一行，方便计算 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/fb7efbb6-d706-4ee2-adf9-e12dce085742)
- 最终逻辑回归的代价函数为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ae3ec934-554f-450e-9339-5bdec2d2959a)

## 4. 最小化代价函数

- 可以发现和线性回归的梯度下降公式相同
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e0bab2ca-415b-49d2-956c-d4da184affa9)
- 但是需注意，f 代表的公式不同
  - 线性回归
    $$f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b \\$$
  - 逻辑回归
    $$f_{\vec{w}, b}(\vec{x}) = \frac{1}{1 + e^{-(\vec{w} \cdot \vec{x} + b)}}$$

## 5. 多分类问题

#### 5.1 输出层

- 在多分类问题场景下，通常把 sigmoid 输出层改成 softmax 输出层
- 当 N = 2 的时候，最终结果和逻辑回归的结果基本相同，因此叫逻辑回归的泛化
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/93a92d0a-2df0-4e0d-a999-c638139551ef)
- 比如当 N = 4 的时候，输出公式为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d6eb8b72-f896-438c-a140-66fbf0e94ac1)

#### 5.2 代价函数

- 对于多分类问题（ 假设有 $K$ 个类别 ），通常代价函数使用 **分类交叉熵**
  $$
  J(\mathbf{W}) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log(\hat{y}_k^{(i)})
  $$
  - 其中 $\hat{y}_k^{(i)} = \text{softmax}(\mathbf{w}_k \cdot \mathbf{x}^{(i)} + b_k)$

## 6. 实战

- 在 sklearn.linear_model 中，能找到对应的现成模型使用
  | 方法 | sklearn 类 | 适用场景 | 关键参数 |
  | ------------------------ | ------------------------------------------- | ------------------------ | ---------------------------------- |
  | **二分类逻辑回归** | `LogisticRegression` | 标准二分类问题 | `penalty`, `C` |
  | **多分类逻辑回归** | `LogisticRegression` | 多类别分类问题 | `multi_class`, `solver` |
  | **L1 正则化逻辑回归** | `LogisticRegression` | 特征选择、稀疏解 | `penalty='l1'`, `solver='saga'` |
  | **L2 正则化逻辑回归** | `LogisticRegression` | 防止过拟合、提高泛化能力 | `penalty='l2'`, `C` |
  | **随机梯度下降逻辑回归** | `SGDClassifier` | 大规模数据、在线学习 | `loss='log_loss'`, `penalty` |
  | **多项式逻辑回归** | `PolynomialFeatures` + `LogisticRegression` | 非线性决策边界 | `degree`, `interaction_only` |
  | **不平衡数据逻辑回归** | `LogisticRegression` | 类别分布不均衡的分类问题 | `class_weight` |
