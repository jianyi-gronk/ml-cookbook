## 1. 基础介绍

#### 1.1 核心作用

- 激活函数是神经网络中的非线性变换单元，核心作用如下
  - **引入非线性能力**
    - 对神经元的加权输入进行非线性映射，使神经网络能够逼近任意复杂函数（ 万能逼近定理 ）
    - **如果所有层都没使用非线性激活函数，那么无论网络有多少层，整个网络等效于一个单层线性模型**，因为每层都相当于矩阵相乘，就算叠加了非常多层，最终还是个矩阵相乘罢了
    - 除了 Identity，其他激活函数都是非线性的
  - **决定神经元激活状态**
    - 控制信息是否传递（ 如 ReLU 的 0 抑制 ）
    - 实现特征选择和网络稀疏化
  - **规范化输出范围**
    - 将输入压缩到特定范围（ 如 (0,1)，(-1,1) ），保持数值稳定性，防止梯度爆炸
  - **构建决策边界**
    - 通过非线性组合形成复杂分类边界，解决线性不可分问题（ 如 XOR ）
  - **梯度传播控制**
    - 通过导数影响反向传播的梯度值，决定网络能否有效训练（ 梯度消失/爆炸问题 ）

#### 1.2 使用时机

- 必须使用场景
  - **任何深度 ≥2 的神经网络隐藏层**
  - **所有分类/回归任务的输出层**
- 激活函数在神经网络中的使用时机主要分为三个关键位置：
  - 在 前向传播 阶段，**隐藏层输出** 位置
    - 在线性变换后引入非线性，使网络具备复杂模式学习能力，同时需要考虑 梯度消失 和 收敛速度
    - 例如卷积层/全连接层后接 ReLU 或 其变体
  - 在 预测输出 阶段，**输出层处理** 位置
    - 将原始输出转换为任务所需格式（ 概率/回归值 ）
    - 例如二分类用 Sigmoid，多分类用 Softmax
  - 在 注意力机制 的 特征融合（ 特征相关性权重计算 ）阶段
    - 计算特征间的相关性权重 Transformer 中的 softmax 注意力权重

#### 1.3 饱和 和 非饱和

- 激活函数能分成两类，**饱和激活函数** 和 **非饱和激活函数**
- 饱和激活函数
  - 当 **输入值达到一定范围时，函数的梯度趋于 0，输出趋于某个固定值（ 通常是上下限 ）**，导致导数（ 梯度 ）接近于零
  - 这种情况会使梯度在反向传播中变得非常小，容易引发梯度消失问题
- 非饱和激活函数
  - 函数的输出不会在某个范围内趋于固定值，导数通常不会趋于零
  - 因此 非饱和激活函数 的优势在于两点
    - **能解决所谓的 梯度消失 问题**
    - **能加快收敛速度**

## 2. 饱和激活函数

#### 2.1 Sigmoid（ 也叫 logistic，逻辑激活函数 ）

- 公式
  $$f(x) = \frac{1}{1 + e^{-x}}$$
  - 输出范围：(0, 1)
- 导数
  $$f'(x) = f(x) \cdot (1 - f(x))$$
- 特性
  - 饱和函数
    - 当输入 x 的绝对值较大时，输出趋于 0（ 当 $x \to -\infty$ ）或 1（ 当 $x \to \infty$ ），导数接近 0，导致梯度消失问题
  - 平滑性
    - 是平滑的非线性函数，导数连续
- 优点
  - 输出可以解释为概率，适合二分类问题的输出层
  - 平滑的梯度有助于某些优化算法的稳定性
- 缺点
  - 梯度消失，在输入绝对值较大时，导数接近 0，导致深层网络训练困难
  - 非零中心化，输出始终为正，可能会使梯度更新方向不一致，降低收敛速度
  - 计算复杂度，指数运算相对较慢
- 适用场景
  - 二分类问题的输出层，输出概率值
  - 较少用于隐藏层，由于梯度消失问题，现代深度网络中通常被 ReLU 等非饱和激活函数替代

#### 2.2 Tanh

- 公式
  $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2x}} - 1$$
  - 输出范围：(-1, 1)
- 导数
  $$f'(x) = 1 - [f(x)]^2$$
- 特性
  - 饱和函数
    - 当输入 x 的绝对值较大时，输出趋于 -1（ 当 $x \to -\infty$ ）或 1（ 当 $x \to \infty$ ），导数接近 0，导致梯度消失
  - 零中心化
    - 输出以 0 为中心（ 均值为 0 ），梯度更新更稳定
  - 平滑性
    - 连续可导，曲线平滑
- 优点
  - 比 Sigmoid 收敛更快（ 零中心化缓解了梯度振荡 ）
  - 输出范围对称，适合需要正负输出的场景
- 缺点
  - 梯度消失，输入绝对值较大时梯度接近 0（ 与 Sigmoid 类似 ）
  - 计算复杂，涉及指数运算，计算成本较高
- 适用场景
  - 输出范围为 (-1,1) 的回归任务，如归一化目标预测

#### 2.3 Softmax

- 公式
  $$\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$
  - 输出范围：(0, 1) 且和为 1
- 导数
  $$
  \frac{\partial \sigma_i}{\partial z_j} =
  \begin{cases}
    \sigma_i(1 - \sigma_i) & \text{if } i = j \\
    -\sigma_i \sigma_j       & \text{if } i \neq j
  \end{cases}
  $$
- 特性
  - 饱和函数
    - 当某个 $z_i \gg z_j$（$j \neq i$）时，$\sigma_i \to 1$，其他输出 $\to 0$，导数趋近 0
  - 概率归一化
    - 将实数向量压缩为概率分布（ 所有输出和为 1 ）
  - 平移不变性
    - $\text{Softmax}(\mathbf{z}) = \text{Softmax}(\mathbf{z} + c)$（$c$ 为常数）
- 优点
  - 天然适配多分类问题，输出可直接解释为类别概率
  - 在分类任务中与交叉熵损失配合效果最佳
- 缺点
  - 梯度消失，当某个类别概率接近 1 时，梯度趋于 0
  - 数值不稳定，指数运算易导致溢出（ 需配合 $\text{LogSoftmax}$ 或数值稳定技巧 ）
- 适用场景
  - 多分类神经网络的输出层（ 如图像分类、自然语言处理 ）
  - 注意力机制中的权重分配（ 如 Transformer ）

## 3. 非饱和激活函数

#### 3.1 Identity（ 恒等激活函数 ）

- 公式
  $$f(x) = x$$
- 导数
  $$f'(x) = 1$$
- 特性
  - 线性变换
    - 输出等于输入，不进行任何非线性处理
  - 非饱和函数
    - 梯度恒为 1，永远不会消失
  - 无激活效果
    - 本质上只是线性传递
- 优点
  - 完全避免梯度消失问题
  - 计算效率极高（ 无复杂运算 ）
  - 适用于需要保持原始数值的场景
- 缺点
  - 不能解决复杂模式识别问题
  - 无法引入非线性能力，多层网络等效于单层线性模型
- 适用场景
  - 回归问题的输出层
  - 神经网络中的跳跃连接，如 ResNet
  - 需要绕过激活函数的特殊结构
  - 非常简单的神经网络模型，如 MLP

#### 3.2 ReLU

- 公式
  $$
  f(x) = \max(0, x) =
  \begin{cases}
  x & \text{if } x > 0 \
  0 & \text{if } x \leq 0
  \end{cases}
  $$
  - 输出范围：$[0, +\infty)$
- 导数
  $$
  f'(x) =
  \begin{cases}
  1 & \text{if } x > 0 \
  0 & \text{if } x \leq 0
  \end{cases}
  $$
- 特性
  - 非饱和函数
    - 正区间梯度恒为 1，无梯度消失问题
  - 单侧抑制
    - 负输入输出为 0，产生稀疏激活
- 优点
  - 解决梯度消失，正区间梯度恒为 1，确保深层网络有效训练
  - 加速收敛，相比 Sigmoid/Tanh，收敛速度可快 6 倍以上
  - 稀疏激活，约 50% 神经元被抑制，提升表征效率
  - 计算高效，简单阈值操作，计算复杂度 O(1)
- 缺点
  - Dying ReLU 问题，即输入恒为负时，梯度永久为 0，神经元 "死亡"
  - 非零中心化，即输出恒为正时，影响梯度更新效率
  - 无上界，输出范围 $[0, +\infty)$，这可能导致某些层的激活值过大，增加梯度爆炸或数值溢出的风险

#### 3.3 ReLU 常见变体

- Leaky ReLU
  $$
  f(x) =
  \begin{cases}
  x & \text{if } x > 0 \\
  \alpha x & \text{if } x \leq 0
  \end{cases} \quad (\alpha \text{ 通常取 } 0.01)
  $$
  - 特点：负区间引入小斜率 $\alpha$，缓解神经元死亡问题
- PReLU（ Parametric ReLU，参数化 ReLU ）
  $$
  f
  (x) =
  \begin{cases}
  x & \text{if } x > 0 \\
  \alpha x & \text{if } x \leq 0
  \end{cases} \quad (\alpha \text{ 为可学习参数})
  $$
  - 特点：负斜率 $\alpha$ 是可学习的模型参数，在训练过程中根据数据自动调整
- ELU（ Exponential Linear Unit，指数线性单元 ）
  $$
  f(x) =
  \begin{cases}
  x & \text{if } x > 0 \\
  \alpha(e^x - 1) & \text{if } x \leq 0
  \end{cases} \quad (\alpha \geq 0)
  $$
  - 特点：负区间使用指数曲线，输出趋近 $-\alpha$
- SELU（ Scaled Exponential Linear Unit，缩放指数线性单元 ）
  $$
  f(x) = \lambda
  \begin{cases}
  x & \text{if } x > 0 \\
  \alpha(e^x - 1) & \text{if } x \leq 0
  \end{cases} \quad \lambda \approx 1.0507, \alpha \approx 1.6733
  $$
  - 特点：自归一化激活函数，帮助维持网络各层输出的 均值 和 方差 分别接近 0 和 1
- GELU（ Gaussian Error Linear Unit，高斯误差线性单元 ）
  $$
  \begin{array}{c}
  f(x) = x \cdot \Phi(x) = x \cdot \dfrac{1}{2}\left[1 + \operatorname{erf}\left(\dfrac{x}{\sqrt{2}}\right)\right] \\
  \\
  \text{\footnotesize 其中 $\Phi(x)$ 是标准正态分布的累积分布函数}
  \end{array}
  $$
  - 基于概率建模，模拟神经元的随机正则化效果
- Swish
  $$
  f(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}} \quad (\beta \text{ 可学习或固定 } )
  $$
  - 特点：平滑非单调，在负区间保留小幅激活
- Mish
  $$f(x) = x \cdot \tanh(\operatorname{softplus}(x)) = x \cdot \tanh(\ln(1 + e^x))$$
  - 特点：连续可微，下界约 -0.31，无上界

## 4. 总结

#### 4.1 输出层场景

| 激活函数 | 任务类型             | 输出转换                    |
| -------- | -------------------- | --------------------------- |
| Sigmoid  | 二分类               | 压缩到 (0,1) 概率           |
| Tanh     | 回归（ 归一化目标 ） | 压缩到 (-1,1) 区间          |
| Identity | 回归                 | 保持原始数值                |
| Softmax  | 多分类               | 压缩到 (0,1) 概率且求和为 1 |

#### 4.2 隐藏层场景

| 激活函数   | 核心改进                           | 解决的主要问题               | 典型应用场景            |
| ---------- | ---------------------------------- | ---------------------------- | ----------------------- |
| ReLU       | 正区间线性激活，负区间输出零       | 缓解梯度消失问题（ 正区间 ） | 通用网络架构            |
| Leaky ReLU | 负区间固定斜率（ $\alpha=0.01$ ）  | 缓解神经元死亡               | 通用网络架构            |
| PReLU      | 负斜率 $\alpha$ 可学习             | 自适应优化激活强度           | 大规模图像识别          |
| ELU        | 负区间指数变化，输出趋近 $-\alpha$ | 改善梯度流，接近零均值       | 噪声敏感任务            |
| SELU       | 自带归一化缩放参数                 | 自归一化网络                 | 全连接密集型网络        |
| GELU       | 基于概率建模的平滑激活             | 替代 ReLU 的先进选择         | BERT/GPT 等 Transformer |
| Swish      | Sigmoid 加权门控                   | 平衡线性和非线性             | 移动端高效模型          |
| Mish       | Self-regularized 非单调激活        | 优化梯度信息流动             | 计算机视觉前沿模型      |

#### 4.3 建议

- **不要使用 ReLU** ，它太旧了。虽然它是非常有用的非线性函数，可以解决很多问题。但是，你可以试试用它微调一个新模型，由于 ReLU 阻碍反向传播，初始化不好，你没法得到任何微调效果。但是你应该用 PreLU 以及一个非常小的乘数，通常是 0.1。使用 PreLU 的话收敛更快，而且不会像 ReLU 那样在初始阶段被卡住。ELU 也很好，但成本高
- 在执行最大池化操作之前，不要在卷积层输出上直接应用 ReLU 或 PreLU 等激活函数，而是在**完成卷积操作并操作完最大池化得到特征图后再进行激活函数的应用**
- 避免使用 Sigmoid/tanh 激活函数，它们代价昂贵，容易饱和，而且可能会停止反向传播。实际上，你的网络越深，就越应该避免使用 Sigmoid 和 TanH。可以使用更便宜而且更有效的 ReLU 和 PReLU 的门，这两者能够促进稀疏性，而且它们的反向传播更加鲁棒
