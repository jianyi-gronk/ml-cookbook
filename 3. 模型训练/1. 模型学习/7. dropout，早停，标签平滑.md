## 1. Dropout 层（ 随机失活 ）

- Dropout 是一种神经网络正则化技术，**通过在训练阶段中随机将部分神经元的输出置为 0（ 即 “丢弃” ），减少过拟合，提高模型泛化能力**，但在推理阶段（ 即模型用于预测或测试时 ），Dropout 会关闭，所有神经元都参与计算

#### 1.1 详细原理

- 在每次训练迭代中，Dropout 层为每个神经元生成一个二进制掩码（ 0 或 1 ）
  - 以概率 p（ 丢弃率，通常为 0.2~0.5 ）随机决定哪些神经元的输出保留（ 1 ）或丢弃（ 0 ）
  - 这种 随机丢弃 **防止神经元形成共适应**，即某些神经元过于依赖彼此，像是 “抱团合作” 的队员，忽略其他神经元，Dropout **逼迫模型用不同的神经元组合完成任务，从而避免过拟合**
- 除了掩码，还需要考虑 缩放因子
  - 假如训练时 30% 的神经元输出被置为 0，那么总输出大约只有之前的 70%，但在推理阶段，Dropout 会关闭，总输出又变为 100%，会导致模型训练和推理时的表现不同
  - **因此训练时，需要通过缩放因子将总输出放大至 100%**，公式为 $\frac{1}{1 - p}$
- 数学上，对于输入 $x_i$，Dropout 操作为
  $$y_i = \frac{x_i \cdot m_i}{1 - p}$$
  - $m_i$ 是二进制掩码（ 0 或 1 ）
  - p 是丢弃率
  - $\frac{1}{1 - p}$ 是缩放因子

#### 1.2 不同场景

- 全连接层
  - 常用于全连接层之后，尤其在隐藏层神经元较多（ 如 > 256 ）时，通过随机丢弃减少神经元间的共适应，降低过拟合风险
- 卷积神经网络
  - 在卷积层后通常不使用 Dropout，因卷积层的共享权重和空间一致性已提供一定正则化，过拟合风险较低
- 循环神经网络
  - 可用于 RNN（ 如 LSTM、GRU ）的隐藏状态，但需谨慎设置丢弃率，因序列数据对连续性敏感
- 生命周期
  - Dropout 仅在训练时使用，推理时关闭，所有神经元输出按比例缩放，以保持输出期望一致

## 2. 早停法（ Early Stopping ）

- 早停法用于控制神经网络的训练轮数，以平衡欠拟合和过拟合
  - 欠拟合：训练轮数不足（ epochs 过少 ），模型未充分学习数据特征，导致性能较差
  - 过拟合：训练轮数过多（ epochs 过多 ），模型过度拟合训练数据，泛化能力下降
- 工作原理
  - 在训练过程中，定期计算验证集上的性能指标
  - 若连续若干轮（ 由耐心参数控制 ）验证集上的损失未低于历史最低值，则停止训练

## 3. 标签平滑（ Label smoothing ）

- 核心思想
  - 通过修改训练数据的标签分布，避免模型在训练时生成过于尖锐的概率分布，如目标类别概率为 1，非目标类别为 0
  - 模型被鼓励预测更平滑的概率分布，防止模型对目标类别过于自信，从而提升泛化能力

#### 3.1 工作原理

- 对于分类问题，通常认为训练数据中标签向量的目标类别概率应为 1，非目标类别概率应为 0，传统的标签向量 $y_i$ 为
  $$
  y_i = \begin{cases}
  1, & i = \text{target} \\
  0, & i \neq \text{target}
  \end{cases}
  $$
- 而标签平滑结合了均匀分布，修改了 $y_i$ 公式，其中 K 为多分类的类别总个数，α 是一个较小的超参数（ 一般取 0.1 ），即
  $$
  y_i = \begin{cases}
  1 - \alpha, & i = \text{target} \\
  \alpha / K, & i \neq \text{target}
  \end{cases}
  $$

#### 3.2 使用场景

- 主要用于深度学习中的分类任务，尤其在数据集存在标签噪声或模型易过拟合（ 如大型神经网络 ）时，但在数据量较小或标签噪声较少的场景中，标签平滑可能效果有限，甚至略微降低模型对正确类别的区分能力
- 标签平滑仅用于训练过程中的损失计算，不影响训练前的标签预处理或训练后的模型评估
