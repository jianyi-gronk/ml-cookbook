- 机器学习最重要的核心分为两点 优化 和 泛化

## 1. 优化（ 最小化代价函数 ）

- **让模型在训练数据上表现得尽可能好**
- 核心思想
  - 模型从数据中学习到有效的模式，通过调整内部参数（ 如权重 ），最小化一个预定义的损失函数
- 常见方法
  - 优化器，如 梯度下降及其变体，牛顿法，伪牛顿法 等
  - 解析解法

## 2. 泛化（ 避免过拟合训练集 ）

- **让模型在新的数据（ 测试数据或真实世界数据 ）上表现得尽可能好**
- 背景
  - 模型容易陷入“过拟合”——在训练数据上表现极好（优化得很好），但在新数据上表现糟糕（泛化能力差）。反之，“欠拟合”则是模型在训练数据上都表现不好（优化不足），泛化能力自然也很差。
- 核心思想
  - 模型学到的不是训练数据的噪声或特定细节，而是数据背后的普遍规律
- 常见方法
  - 归一化，如 批归一化，层归一化 等
  - 激活函数，如 Sigmoid，Softmax，ReLU 及变体 等
  - 正则化，如 L1，L2 等
  - drop 随机失活
  - 早停
  - 标签平滑

## 3. 举例

- 以 全连接层 的 神经网络 的操作步骤举例
- 前向传播阶段
  - 输入层
    - 接收数据 x
  - 加权求和 + 偏置
    $$z = \mathbf{w}^T \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b$$
  - **归一化（ 可选 ）**
    - 批归一化 或 层归一化
  - **激活函数**
    - 最核心是为了非线性能力，但也能带来泛化能力
  - 重复 2 ～ 4 步骤
  - 输出层
    - 分类问题可以用 Sigmoid（ 二分类 ）或 Softmax（ 多分类 ）**激活函数**，可以引入 **标签平滑（ 可选 ）**
- 损失计算阶段
  - 代价函数
    - 根据代价函数，计算预测值与真实标签的误差，可以引入 **正则化（ 可选 ）**
  - **早停**
    - 监控验证集损失，若连续多轮不下降则终止训练
- 反向传播阶段
  - 计算梯度
    - 从输出层反向传播误差
    - 如果引入了归一化，还需要计算 **归一化** 的梯度，因为归一化层引入了额外的可学习参数 γ 和 β
- 参数更新阶段
  - **优化器**
    - 通常使用 adam 更新参数，小型神经网络 L-BFGS 可能效果更好
