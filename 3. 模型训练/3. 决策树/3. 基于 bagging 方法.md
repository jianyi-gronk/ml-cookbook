## 1. 随机森林

- scikit-learn 中的 sklearn.ensemble.RandomForestClassifer（ 分类 ）和 sklearn.ensemble.RandomForestRegressor（ 回归 ）

#### 1.1 基础介绍

- 随机森林会 **同时对 样本 和 特征 进行有放回的采样**（ 被采样的叫袋内，未被采样的叫袋外 ），从而确保各决策树之间的相关性较低，然后用采样的样本和特征 **构建多棵决策树**，最后 **将多棵决策树的结果进行平均（ 回归任务 ）或投票（ 分类任务 ）等操作**

#### 1.2 优缺点

- 优点
  - 都是对整体进行采样，因此训练可以 **高度并行化**，可以有效运行在大数据集上
  - 由于对决策树候选划分属性的采样，这样在样本特征维度较高的时候，仍然可以 **高效的训练模型**
  - 由于有了样本和属性的采样，最终训练出来的模型 **泛化能力强**
  - 可以输出并评估 **各特征子集对预测目标的重要性**
  - 采样之外的数据可用作验证集来检验模型的有效性，**不用额外划分数据集**
- 缺点
  - 对噪声敏感，若训练数据中存在大量噪声样本或冗余特征，随机森林可能通过投票机制 **放大噪声的影响**，导致模型偏差增大
  - **解释性较差**，随机森林通过多棵树的集成结果进行预测，难以像单棵决策树一样直观展示决策路径
  - 单棵决策树擅长捕捉特征间的交互，但 **集成后可能因随机采样削弱某些关键交互的影响**

#### 1.3 参数

```python
(
  n_estimators=100,
  criterion='gini',
  max_depth=None,
  min_samples_split=2,
  min_samples_leaf=1,
  min_weight_fraction_leaf=0.0,
  max_features='sqrt',
  max_leaf_nodes=None,
  min_impurity_decrease=0.0,
  bootstrap=True,
  oob_score=False,
  n_jobs=None,
  random_state=None,
  verbose=0,
  warm_start=False,
  class_weight=None,
  ccp_alpha=0.0,
  max_samples=None,
  monotonic_cst=None
)
```

## 2. 极端随机森林

- Scikit-learn 中的 sklearn.ensemble.ExtraTreesClassifer（ 分类 ）和 sklearn.ensemble.ExtraTreesRegressor（ 回归 ）

#### 2.1 和随机森林对比

- 和随机森林的关键区别有两点
  - 极端随机森林训练每棵决策树都是 **使用全部样本（ 但还是对特征抽样 ）**
  - 在每个节点进行划分时，不仅随机选择一部分特征，而且对于每个特征，**会随机选择一个划分阈值**，而不是像随机森林那样寻找最优的划分阈值，这样 **由于随机选择划分阈值，对噪声和异常值的敏感性相对较低**，同时 **避免了寻找最优阈值的计算过程，训练速度更快**
- 极端随机森林相比于随机森林，**适用于对计算效率要求较高，或者数据中噪声和异常值较多的场景**

#### 2.2 参数

- 和随机森林的唯一参数默认值不同的是把 bootstrap 从 True 改成 False

```python
(
  n_estimators=100,
  criterion='gini',
  max_depth=None,
  min_samples_split=2,
  min_samples_leaf=1,
  min_weight_fraction_leaf=0.0,
  max_features='sqrt',
  max_leaf_nodes=None,
  min_impurity_decrease=0.0,
  bootstrap=False,
  oob_score=False,
  n_jobs=None,
  random_state=None,
  verbose=0,
  warm_start=False,
  class_weight=None,
  ccp_alpha=0.0,
  max_samples=None,
  monotonic_cst=None
)
```

## 3. 孤立森林

- Scikit-learn 中的 sklearn.ensemble.IsolationForest（ 回归，返回每个样本的异常分数 ）
- 孤立森林 和 随机森林与极端随机森林 不同，不是用于分类和回归，而是用于离群点检测（ 异常检测 ），所以也不需要目标变量

#### 3.1 核心思路

- 核心思路是，**利用孤立树（ 二叉搜索树结构 ）来孤立样本，由于异常值的数量较少且与大部分样本较疏离，因此异常值会被更早孤立出来，即异常值会距离树的根节点更近，而正常值则会距离根节点有更远的距离**
- 如何孤立样本，可以理解为 **随机切割样本集，直到每个子空间（ 叶子节点 ）里面只有一个样本为止**，这样密度很高的簇需要被切割多次才停止，而密度低的簇很容易就切割停止
  ![image](https://github.com/user-attachments/assets/8943d0f4-eb3c-4c3a-bb62-8d7b7c4f21ab)

#### 3.2 具体步骤

1. 从训练数据集中抽取 n 个样本放入树的根节点
2. 随机指定一个维度（ 特征 ），在当前节点数据中随机产生一个切割点 p，切割点产生于当前节点数据中指定维度的最大值和最小值之间
3. 以此切割点生成了一个超平面，然后将当前节点数据空间划分为 2 个子空间，然后把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点
4. 在子节点中递归步骤 2 和 3，不断构造新的子节点，直到子节点中只有一个数据或子节点已到达限定高度
5. 循环 1 至 4，直至生成 T 个孤立树
6. 根据各个值在 T 个孤立树节点的平均高度，计算异常值分数（ 具体公式不提 ）

#### 3.3 参数

- 和随机森林和极端随机森林不同的是
  - 采用全部特征
  - 同时多了 contamination 参数，表示数据集中异常值的比例，为 auto 时和原始论文取值相同，如果为浮点数，则取值应该在 (0, 0.5] 范围内
  - max_samples 值为 'auto'，表示 max_samples = min(256, n_samples)

```python
(
  n_estimators=100,
  max_samples='auto',
  contamination='auto',
  max_features=1.0,
  bootstrap=False,
  n_jobs=None,
  random_state=None,
  verbose=0,
  warm_start=False
)
```
