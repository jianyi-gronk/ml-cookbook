[参考资料](https://leovan.me/cn/2018/12/ensemble-learning/)

## 1. 集成学习（ Ensemble Learning ）

- 假如使用比较小的模型，可以试试 ensemble，通常 ensemble 5 个网络能够提升准确度约 3%
- 集成学习也称为模型融合，不同于传统的机器学习方法在训练集上构建一个模型，集成学习通过构建并融合多个模型来完成学习任务
- 集成学习的主要方法可归类为三大类： 堆叠（ Stacking ）、提升（ Boosting ） 和 装袋（ Bootstrap aggregating，即 Bagging ）

## 2. Bagging（ Boostrap Aggregating ）

- 算法步骤如下
  - 每次采用有放回的抽样从训练集中取出 n 个训练样本组成新的训练集
  - 利用新的训练集，训练得到 M 个子模型 { $H_1$, $H_2$, ..., $H_m$ }
  - 对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值
- 通过二分类问题举例
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b1d01459-8ae1-410d-a613-6defb1d76094)
  - a、b、c 分别是三个不同模型在测试集上的分类结果
  - 上图中每个模型的错误分类的样本都用圆圈标了出来，可以看到每个模型大体都是准确的，但是错误的情况也会存在，这种情况下，可以考虑用集成的方法，将 3 个不同模型的记过进行综合考虑。上图 d 模型就是进行集成学习的结果，可以看到效果变好了
  - 这里采用的方法是投票方法（ majority vote ），对 3 个模型分类结果中投票多数的结果作为最终结果。除了多数投票，在实际应用中常见的方法还有取均值，但是基本思想都是一样的

## 3. Boosting

#### 3.1 基础概念

- Bagging 是可以不同模型并行得到结果再处理，而 Boosting 是把多个弱学习器用自定义顺序串联起来，该序列中每个模型在拟合的过程中，会更加重视那些序列中之前的模型处理地很糟糕的观测数据（ 通过增加数据的权重 ），即每个模型都把注意力集中在目前最难拟合的观测数据上
- 基本思想
  - 先赋予每个训练样本相同的权重
  - 然后进行 N 次迭代，每次迭代后，对分类错误的样本加大权重，并重采样，使得在下一次的迭代中更加关注这些样本
- 如图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ab48d1ed-0039-4a9f-915e-40c2dde9b52a)
- 其中分为两类
  - Adaboost：每轮训练后调整样本权重，使后续模型更关注被误分类的样本
  - GBDT：每轮训练拟合前序模型的残差

#### 3.2 Adaboost（ 自适应增强 ）

- 基本思想是通过训练数据的分布构造一个分类器，然后通过误差率求出这个若弱分类器的权重，通过更新训练数据的分布，迭代进行，直到达到迭代次数或者损失函数小于某一阈值
- 将 Adaboost 集成模型定义为 L 个弱学习器的加权和，其中 $c_l$ 是系数，而 $w_l$ 是弱学习器
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c72cd303-6ee4-48bb-8a54-3192a7f61327)
- 由于寻找整体最优解非常复杂，因此使用贪心算法，在每次迭代中寻找当前的局部最优解（ 可能整体不是最优解 ），即循环地将 $s_l$ 定义如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/6eabe4a2-1850-4792-b5d9-cffe7dcce2b6)

#### 3.3 GBDT（ Gradient Boosting Decision Tree，梯度提升决策树 ）

- 是一种迭代的决策树算法，又叫 MART（ Multiple Additive Regression Tree ），它通过构造一组弱的学习器（ 树 ），并把多颗决策树的结果 **累加** 起来作为最终的预测输出
- 基本思想为每次学习一点
  - 先用一个初始值来学习一棵决策树，叶子处可以得到预测的值，以及预测之后的残差
  - 不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数 f(x)，去拟合上次预测的残差，直到预测值和真实值的残差为 0
  - 最后对于测试样本的预测值，就是前面许多棵决策树预测值的累加
- 可以用一个通俗的例子解释，
  - 假如有个人 30 岁，我们首先用 20 岁去拟合，发现损失有 10 岁，这时我们用 6 岁去拟合剩下的损失，发现差距还有 4 岁，第三轮我们用 3 岁拟合剩下的差距，差距就只有 1 岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小
- GBDT 也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用 CART 回归树模型，GBDT 的会累加所有树的结果，而这种累加是无法通过分类完成的，因此 GBDT 的树都是 CART 回归树，而不是分类树，尽管 GBDT 调整后也可以用于分类但不代表 GBDT 的树为分类树
- 它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型。所以说，**在 GBDT 中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法**，与传统的 Boosting 中关注正确错误的样本加权有着很大的区别
- GBDT 通过多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度
- 通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已

## 4. Stacking

#### 4.1 基础 Stacking 算法

- Stacking 是一种嵌套组合型的模型融合方法，其基本思路就是在第一层训练多个不同的基学习器，然后把第一层训练的各个基学习器的输出作为输入来训练第二层的学习器，从而得到一个最终的输出
- Stacking 与 bagging 和 boosting 主要存在两方面的差异
  - 首先，Stacking 通常考虑的是异质弱学习器（ 不同的学习算法被组合在一起 ），而 bagging 和 boosting 主要考虑的是同质弱学习器
  - 其次，stacking 学习用元模型组合基础模型，而 bagging 和 boosting 则根据确定性算法组合弱学习器

#### 4.2 举例

- 用一个基础模型进行 5 折交叉验证，对于训练集先拿出 4 折作为训练数据，另外一折作为测试数据，每一次交叉验证我们都会基于训练数据训练生成的模型对测试数据进行预测，这部分预测值最后拼接起来就是第二层模型的训练集。同时每次交叉验证我们还要对数据集原来的整个测试集进行预测，最后将各部分预测值取算术平均，作为第二层模型测试集，在此之后，我们把第一层模型的训练集预测值并列合并得到的矩阵作为训练集，第一层模型的预测集预测值并列合并得到的矩阵作为测试集，带入第二层的模型，再基于它们进一步训练，从而得到最终预测结果
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/155e1ffc-52f1-4315-93af-abb8b595a72e)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/bba8927f-807e-45ec-bb26-96f8ff50122f)

## 5. 迁移学习

- 迁移学习专注于存储已有问题的解决模型，并将其利用在其他不同但相关问题上。比如说，用来辨识汽车的模型也可以被用来提升识别卡车的能力
- 一般图片识别问题的模型可以用在其他图片识别上，但不适合用在音频识别问题上
- 迁移学习步骤
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/744fedee-b1ce-4d08-85fc-03af2eb239e1)
  - 假如现在想训练一个识别手写数字 0，1，2 ... 9 的模型，但是标记数据集非常小，可以先用有 100 万张猫，狗，车，人等一千个类别的图片去训练神经网络（ 也叫监督预训练 ）
  - 然后将最后的输出层进行替换，因为之前是 1000 中类别的输出，现在是 10 中类别的输出
  - 接下来对模型进行微调，有两种策略
    - 如果手写数字数据集非常小，则只训练输出层参数，隐藏层参数不变
    - 如果手写数字数据集一般小，则训练输出层和隐藏层的所有参数

## 6. 增量学习（ Incremental Learning ）

- 增量学习亦称为持续学习（ Continual Learning ）或终身学习（ Life-Long Learning ）
- 增量学习是一种动态的机器学习方法，它允许模型通过对新数据进行持续学习而不是重头训练整个模型。这种方法允许模型不断地学习新的知识，并在不断实际复杂多变的环境变化
- 增量学习的优点是可以随时训练新数据，不需要保留大量训练数据，因此存储和计算开销都比较小，同时还能够有效避免用户的隐私泄露问题，这在移动边缘计算的场景下是非常有价值有意义的
- 对于满足以下条件的学习方法可以定义为增量学习方法：
  - 可以学习新的信息中的有用信息
  - **不使用旧数据或者少使用原始数据**
  - 对已经学习的知识具有记忆功能
  - 在面对新数据中包含的新类别时，可以有效地进行处理
- 增量学习方法的种类有很多种划分方式，大体可以将其划分为以下三种范式：
  - 正则化（ regularization ）：**不使用旧数据**
  - 回放（ replay ）：**少使用旧数据**（ 选一部分作为旧数据的代表 ）
  - 参数隔离（ parameter isolation ）
  - 其中基于正则化和回放的增量学习范式受到的关注更多，也更接近增量学习的真实目标，参数隔离范式需要引入较多的参数和计算量，因此通常只能用于较简单的任务增量学习

## 7. 基于正则化的增量学习

- 基本思想就是：**不但在学习新的知识，而且一定程度上保护旧知识**
- 模型是通过旧数据训练得到，所以模型经过新数据后会得到损失，因此希望在损失中再加一些项，希望其可以反应旧数据在当前模型上的表现
- 举例
  - 以多任务学习为例，任务就是要学习一个图片分类器，比如之前学的都是猫的图片，现在给了狗的图片，也要学
  - 通常会这么做，前面那些几十层是用来提取图片通用特征的（ 猫狗都共享 ），后面那些一两层是任务特定的（ 猫，狗各自都有自己的层 ），有点像预训练模型
  - 这个时候我们有两种选择：
    - 根据新数据微调，那么会改变共享参数，这个时候旧数据的表现可能会变得很差
    - 特征抽取，保持共享参数不变，根据新数据只训练新任务的那几个特定层，但这样也不好，因为之前的共享参数未必学得适用于所有任务，所以单靠后面几层难以学习一个很好的分类器

#### 7.1 LwF（ Learning Without Forgetting ）

- LwF 算法是基于深度学习的增量学习的里程碑之作，这个有点像微调，但是思想上比微调更加丰富，它通过知识蒸馏防止灾难性遗忘
- 算法如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/8a850f1f-d398-44bf-9d89-114c9a13b9cb)
- 计算步骤为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ffe2a674-083c-4cb7-9615-f8b392fbb0b7)
- 相当于引入了新数据在旧模型上的结果 $Yo$，作为旧模型的回忆，希望新数据训练完毕之后，这个 $Yo$ 在新模型上也没有怎么变化，从而希望旧数据输入新模型得到的结果也没有怎么变化

#### 8. 基于回放的增量学习

- 基于回放的增量学习的基本思想就是 "温故而知新"，**在训练新任务时，一部分具有代表性的旧数据会被保留并用于模型复习曾经学到的旧知识**，因此要保留旧任务的哪部分数据，以及如何利用旧数据与新数据一起训练模型，就是这类方法需要考虑的主要问题
- iCaRL 是最经典的基于回放的增量学习模型，iCaRL 假设越靠近旧数据均值的样本越有代表性
