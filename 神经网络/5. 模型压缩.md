## 1. 模型压缩

- 深度神经网络需要巨大的计算开销和内存开销，因此受限制于资源容量，深度神经模型很难部署在资源受限制的设备上。如嵌入式设备和移动设备
- 模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中
- 模型压缩的方法主要有：模型压缩，模型量化 和 模型蒸馏

## 2. 需求背景

- 随着 AI 技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入 AI 能力，特别对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要
- 首先是资源受限，其次在许多网络结构中，如 VGG-16 网络，参数数量 1 亿 3 千多万，占用 500MB 空间，需要进行 309 亿次浮点运算才能完成一次图像识别任务

## 3. 模型剪枝

- 剪枝（ Pruning ）的核心思想是在尽量保持模型精度不受影响的前提下减少网络的参数量，例如减少网络中连接或神经元的数量，如下图所示
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/743096f7-8a02-4249-b93f-9b2252b90e20)
- 剪裁最常用的步骤如下：
  - 训练：在整个剪裁过程中，该步骤主要为预训练过程，同时为后续的剪裁工作做准备
  - 修剪：通过具体的方法对网络进行剪裁，并对网络重新进行评估以确定是否符合要求
  - 微调：通过微调恢复由于剪裁对模型带来的性能损耗
- 对网络进行剪裁的具体方法可以分为非结构化剪裁和结构化剪裁
- 非结构化剪裁
  - 非结构化剪裁是细粒度的剪裁方法，主要关注权重上，一般通过设定一个阈值，高于该阈值的权重得以保留，低于该阈值的权重则被去除
  - 非结构化剪裁虽然方法简单、模型压缩比高，但也存在诸多问题。例如：
    - 全局阈值设定未考虑不同层级的差异性
    - 剪裁信息过多有损模型精度且无法还原
    - 剪裁后的稀疏权重矩阵需要硬件层支持方可实现压缩和加速的效果
- 结构化剪裁
  - 结构化剪裁是粗粒度的剪裁方法，例如对网络层、通道、滤波器等进行剪裁。在滤波器剪裁中，通过评估每个滤波器的重要性（ 例如：Lp 范数 ）确定是否保留
  - 结构化剪裁算法相对复杂、控制精度较低，但剪裁策略更为有效且不需要硬件层的支持，可以在现有深度学习框架上直接应用

## 4. 模型量化

- 在深度学习中，模型量化是一种通过减少模型参数的表示位数来降低计算和存储开销的技术。由于浮点计算涉及较多的位数，如单精度 32 位和双精度 64 位，因而产生了较大的计算开销。通过将浮点表示转换为定点表示，可以有效减少位数，从而提高模型的运行效率
- 数值的量化可以看做一个近似过程，主要可以分为两类：
  - 定点近似：通过缩小浮点数表示中指数部分和小数部分的位宽实现。映射过程不需要额外的参数，实现相对简单，但针对较大数值的精度损失较大
  - 范围近似：通过统计分析，经过缩放和平移映射浮点数。映射过程需要存储额外的参数，计算时需要先反量化，计算相对复杂，但精度更高

## 5. 知识蒸馏

- 知识蒸馏（ Knowledge Distillation，KD ）是一种教师-学生（ Teacher-Student ）训练结构，通常是已训练好的教师模型提供知识，学生模型通过蒸馏训练来获取教师的知识。它能够以轻微的性能损失为代价将复杂教师模型的知识迁移到简单的学生模型中

#### 5.1 在线蒸馏（ Online Distillation ）

- 在线蒸馏是一种端到端的训练方式，其中教师模型和学生模型是同时更新的。学生模型在训练的每一步，都受到教师模型的影响，从而能够实时调整和优化自己的参数。由于学生模型可以直接从教师模型中学习，这种方式可以更好地利用教师模型的知识，尤其是在教师模型具有复杂结构和丰富表示能力时
- 在这种模式下，教师模型和学生模型共享相同的训练数据集，学生模型在训练过程中直接从教师模型中学习。在线蒸馏的关键在于，**学生模型不仅学习来自训练数据的标签信息，还学习教师模型对数据的软预测**
- 软硬预测的区别
  - 在传统的监督学习中，模型的预测通常是硬预测（ hard predictions ），即模型对每个类别给出一个确定的标签，这个标签，通常是概率分布中，概率最高的那个类别
  - 与硬预测相对的软预测（ soft predictions ），是指模型输出的是概率分布，这个概率分布包含了模型对于每个可能类别的预测信心。在 softmax 函数的帮助下，深度学习模型的输出层通常会生成一个这样的类别概率分布。每个类别的概率反映了模型认为输入数据属于该类别的可能性

#### 5.2 离线蒸馏（ Offline Distillation ）

- 离线蒸馏是静态的学习过程，学生模型使用教师模型预先学习到的知识，进行训练，而教师模型在学生模型的训练过程中保持不变。离线蒸馏的优点在于它的简单性和易于实现。此外，由于教师模型是预先训练好的，学生模型可以从大量的数据中提取知识，而不需要直接访问这些数据
- 在这种模式下，教师模型首先在大型数据集上进行训练，直到收敛。一旦教师模型训练完成，**它的知识（ 通常是输出层的软预测 ）被用来指导学生模型的训练，学生模型无法直接接触到原始训练数据**，而是通过模仿教师模型的行为来进行学习

#### 5.3 自我蒸馏（Self-Distillation）

- 自我蒸馏，**网络在不同的训练阶段扮演教师和学生的角色**，通过自我学习来提升自身的性能。自我蒸馏的优点在于它不需要额外的教师模型，可以减少对计算资源的需求。此外，由于学生模型直接从自身的早期预测中学习，这种方式可以更好地捕捉到模型内部的知识
- 自我蒸馏是一种特殊的情况，其中教师模型和学生模型是同一个网络，只是处于不同的训练阶段。在自我蒸馏中，网络在早期的训练阶段充当教师，生成软预测，然后在后续的训练阶段，网络使用这些软预测来进一步训练自己，从而提高性能
