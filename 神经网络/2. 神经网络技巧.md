## 1. 早停法（ Early Stopping ）

- 神经网络训练多少轮是一个很关键的问题，如果 epoch 数量太少，网络有可能发生欠拟合，即对于定型数据的学习不够充分；如果 epoch 数量太多，则有可能发生过拟合
- 根本原因就是因为继续训练会导致测试集上的准确率下降，早停法的本质是在平均验证损失值不再改善时，要等待多少个 epoch 才停止训练，即连续多少轮次的平均验证损失值都小于最低验证损失就停止训练

## 2. 标签平滑（ Label smoothing ）

- 标签平滑通常用于分类问题，目的是防止模型在训练时过于自信地预测标签，改善泛化能力差的问题
- 对于分类问题，我们通常认为训练数据中标签向量的目标类别概率应为 1，非目标类别概率应为 0，传统的标签向量 $y_i$ 为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/95300e1e-2dd0-41a5-916a-695480610062)
- 标签平滑结合了均匀分布，修改了 $y_i$ 公式，其中 K 为多分类的类别总个数，α 是一个较小的超参数（ 一般取 0.1 ），即
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0109b202-9c35-4cd1-9fe6-87a6fe086795)

## 3. 注意 CPU 与 GPU 之间的数据传输

- 通过 tensor.cpu() 可以将张量从 GPU 传输到 CPU，反之使用 tensor.cuda()，但这样的数据转化代价较高。.item() 和 .numpy() 的使用也是如此，建议使用 .detach()
- 如果要创建新的张量，使用关键字参数 device=torch.device（'cuda：0'）将其直接分配给 GPU
- 最好使用 .to(non_blocking=True) 传输数据，确保传输后没有任何同步点即可

## 4. 垃圾回收

- python 也是使用分代回收策略，总共分了 3 代，默认触发垃圾回收时机如下

  ```python
  import gc

  print "Garbage collection thresholds: %r" % gc.get_threshold()
  # Garbage collection thresholds: (700, 10, 10)
  # 700：表示当分配对象的个数达到 700 时，进行一次 0 代回收
  # 10：当进行 10 次 0 代回收以后触发一次 1 代回收
  # 10：当进行 10 次 1 代回收以后触发一次 2 代回收

  # 上面这个是默认的回收策略的阈值
  # 也可以自己设置回收策略的阈值
  gc.set_threshold(500, 5, 5)
  ```

- 由于自动垃圾收集高度重视空闲对象的数量，而不是它们的大小，因此当代码中释放占据大量内存的对象时，是运行手动垃圾收集的良好时机（ 注意不要频繁调用，会浪费性能 ）

  ```python
  del train_val

  gc.collect()
  ```

## 5. 其他建议

- 在整个数据集上训练之前，先在非常小的子数据集上训练进行过拟合，这样可以知道你的网络是否可以收敛
- 避免 LRN 池化，MAX 池化会更快
- GPU 上报错时尽量放在 CPU 上重跑，错误信息更友好
- 一般来说，越大的 batch-size 使用越大的学习率。原理很简单，越大的 batch-size 意味着我们学习的时候，收敛方向的 confidence 越大，我们前进的方向更加坚定，而小的 batch-size 则显得比较杂乱，毫无规律性，因为相比批次大的时候，批次小的情况下无法照顾到更多的情况，所以需要小的学习率来保证不至于出错
- 卷积层通常使用金字塔状特征提取，比如 [64, 128, 256, 128, 64]，而全连接层，和 RNN 层通常从大到小的形状，比如 [256, 128, 64]
- 如果保存下来的模型，在新的训练过程中，其他代码没动，但发现 r2-score 相比于上次训练下降了不少，这种通常是因为学习率的问题，因为在上一轮的训练中，学习率已经被训练成一个适合的值，而重新训练的时候，又从默认值开始调，因此应该记住上次训练后的学习率的值，并在下一轮训练前改动学习率
- 一般顺序是，全连接后先 BN 层，再激活函数，再 dropout 层
