## 1. 代码样例

```python
import gc  # 垃圾回收模块，用于手动执行垃圾回收
import random  # 随机数模块，用于生成随机数
import time  # 时间模块，用于处理时间相关的操作
import torch  # PyTorch 深度学习库
import datetime  # 日期时间模块，用于处理日期和时间
import numpy as np  # NumPy 科学计算库，用于数组和矩阵运算
import pandas as pd  # Pandas 数据分析库，用于数据处理和分析
import polars as pl  # Polars 数据处理库，用于数据操作和分析
import torch.nn as nn  # PyTorch 神经网络模块，用于构建神经网络模型
import torch.optim as optim  # PyTorch优化模块，用于定义优化器
from torch.utils.data import Dataset, DataLoader  # PyTorch 数据处理模块，用于构建数据集和数据加载器
from torchmetrics.regression import R2Score  # PyTorch 指标模块，用于回归模型评估


DATA_PATH = "/kaggle/input/"  # 定义数据路径常量
BATCH_SIZE = 8192  # 定义批处理大小
MIN_STD = 1e-8  # 定义最小标准差
SCHEDULER_PATIENCE = 3  # 定义学习率调度器的耐心
SCHEDULER_FACTOR = 10**(-0.5)  # 定义学习率调度器的因子
EPOCHS = 70  # 定义训练轮数
PATIENCE = 6  # 定义耐心
PRINT_FREQ = 50  # 定义打印频率
TRAIN_RATIO = 0.9  # 定义训练数据占所有数据的比率


def format_time(elapsed):
    """
    将时间（秒）转换为 hh:mm:ss 格式的字符串
    :param elapsed: 要转换的时间，以秒为单位
    :return: 转换后的时间字符串，格式为 hh:mm:ss
    """
    # 将经过的时间四舍五入为整数秒
    elapsed_rounded = int(round(elapsed))
    # 使用 datetime 模块将秒数转换为 hh:mm:ss 格式的字符串
    return str(datetime.timedelta(seconds=elapsed_rounded))


def seed_everything(seed_val=1218):
    """
    设置所有可能影响随机性的种子，以确保实验的可重复性
    :param seed_val: 要设置的种子值，默认为 1218
    """
    # 设置 Python 内置 random 模块的随机种子
    random.seed(seed_val)
    # 设置 NumPy 随机数生成器的随机种子
    np.random.seed(seed_val)
    # 设置 PyTorch 随机数生成器的随机种子
    torch.manual_seed(seed_val)
    # 设置所有 GPU 上的 PyTorch 随机数生成器的随机种子
    torch.cuda.manual_seed_all(seed_val)


# 记录开始时间
# 接下来在想标记的位置进行打印记录耗时时长，通常打印的时间点有
#   打印读取数据所花费的时间
#   打印数据处理后所花费的时间
#   打印所有准备工作完成后的时间
#   打印总运行时间
ts = time.time()

# 读取样本权重数据，只读取第一行
weights = pd.read_csv(DATA_PATH + "leap-atmospheric-physics-ai-climsim/sample_submission.csv", nrows=1)
# 删除 'sample_id' 列
del weights['sample_id']
# 转置并转换为字典格式
weights = weights.T
weights = weights.to_dict()[0]

# 读取训练数据，最多读取 2,500,000 行
df_train = pl.read_csv(DATA_PATH + "leap-atmospheric-physics-ai-climsim/train.csv", n_rows=2_500_000)

# 对每个目标列进行加权处理
for target in weights:
    df_train = df_train.with_columns(pl.col(target).mul(weights[target]))

# 打印读取数据所花费的时间
print("Time to read dataset:", format_time(time.time()-ts), flush=True)

# 获取特征列和目标列的列名
FEAT_COLS = df_train.columns[1:557]
TARGET_COLS = df_train.columns[557:]

# 将特征列的数据类型转换为 Float32
for col in FEAT_COLS:
    df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))

# 将目标列的数据类型转换为 Float32
for col in TARGET_COLS:
    df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))

# 提取特征列和目标列的数据并转换为 NumPy 数组
x_train = df_train.select(FEAT_COLS).to_numpy()
y_train = df_train.select(TARGET_COLS).to_numpy()

# 删除 df_train 以释放内存
del df_train
gc.collect()

# Z-score 归一化训练数据
# 计算特征列的均值和标准差，最小标准差为 MIN_STD
mx = x_train.mean(axis=0)
sx = np.maximum(x_train.std(axis=0), MIN_STD)
# 对特征数据进行归一化处理
x_train = (x_train - mx.reshape(1, -1)) / sx.reshape(1, -1)

# 计算目标列的均值和标准差，最小标准差为 MIN_STD
my = y_train.mean(axis=0)
sy = np.maximum(np.sqrt((y_train*y_train).mean(axis=0)), MIN_STD)
# 对目标数据进行归一化处理
y_train = (y_train - my.reshape(1, -1)) / sy.reshape(1, -1)

# 打印数据处理后所花费的时间
print("Time after processing data:", format_time(time.time()-ts), flush=True)

# 调用设置随机种子的函数
seed_everything()
# 检查是否有可用的 GPU，如果有则使用 GPU，否则使用 CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class NumpyDataset(Dataset):
    def __init__(self, x, y):
        """
        使用 NumPy 数组进行初始化
        :param x: 特征数组，NumPy 格式
        :param y: 标签数组，NumPy 格式
        """
        # 确保特征和标签具有相同数量的样本
        assert x.shape[0] == y.shape[0], "特征和标签必须具有相同的样本数量"
        self.x = x  # 储存特征数组
        self.y = y  # 储存标签数组

    def __len__(self):
        """
        返回数据集的总样本数
        :return: 样本数量
        """
        return self.x.shape[0]  # 返回特征数组的第一维度大小，即样本数量

    def __getitem__(self, index):
        """
        生成一个数据样本
        :param index: 样本索引
        :return: 一个样本的特征和标签，都是 PyTorch 张量格式
        """
        # 根据索引获取一个样本，并将其从 NumPy 数组转换为 PyTorch 张量，
        # 转换为浮点型，并移动到指定设备（如 GPU）
        return torch.from_numpy(self.x[index]).float().to(device), torch.from_numpy(self.y[index]).float().to(device)


# 创建 NumpyDataset 实例，使用 x_train 和 y_train 初始化
dataset = NumpyDataset(x_train, y_train)

# 计算训练集大小和验证集大小
train_size = int(TRAIN_RATIO * len(dataset))  # 训练集大小为总数据集大小的 90%
val_size = len(dataset) - train_size   # 验证集大小为总数据集大小减去训练集大小

# 使用 torch.utils.data.random_split 方法将数据集随机拆分为训练集和验证集
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# 创建训练集和验证集的数据加载器
# DataLoader 用于从数据集中按批次加载数据
# batch_size 指定每个批次的样本数量
# shuffle 参数表示是否在每个 epoch 前随机打乱数据
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)


class FFNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(FFNN, self).__init__()

        layers = []  # 存储神经网络的层
        previous_size = input_size  # 记录前一层的输出大小
        for hidden_size in hidden_sizes:  # 遍历隐藏层的大小
            layers.append(nn.Linear(previous_size, hidden_size))  # 添加线性变换层，将前一层的输出映射到当前隐藏层的大小
            layers.append(nn.LayerNorm(hidden_size))  # 添加层归一化层，用于规范化隐藏层的输出
            layers.append(nn.LeakyReLU(inplace=True))  # 添加 LeakyReLU 激活函数层，引入非线性特性
            layers.append(nn.Dropout(p=0.1))  # 添加 Dropout 层，以防止过拟合，随机地将某些输入设置为 0
            previous_size = hidden_size  # 更新前一层的输出大小为当前隐藏层的大小

        layers.append(nn.Linear(previous_size, output_size))  # 添加最后一层线性变换层，将最后一个隐藏层的输出映射到输出层的大小

        self.layers = nn.Sequential(*layers)  # 使用 nn.Sequential 将所有层组合在一起，形成神经网络模型

    def forward(self, x):
        return self.layers(x)  # 前向传播函数，通过神经网络模型进行前向计算并返回结果


# 获取输入数据的特征数量，用于确定模型的输入大小
input_size = x_train.shape[1]

# 获取输出数据的特征数量，用于确定模型的输出大小
output_size = y_train.shape[1]

# 设置隐藏层的大小，这里选择为输入大小和输出大小之和
hidden_size = input_size + output_size

# 初始化前馈神经网络模型
# 隐藏层大小依次为 3 倍隐藏层大小、2 倍隐藏层大小、2 倍隐藏层大小、2 倍隐藏层大小和 3 倍隐藏层大小
model = FFNN(input_size, [3*hidden_size, 2*hidden_size, 2*hidden_size, 2*hidden_size, 3*hidden_size], output_size).to(device)

# 设置损失函数为均方误差(MSELoss)
criterion = nn.MSELoss()

# 使用AdamW优化器，指定学习率和权重衰减
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# 设置学习率调度器，使用ReduceLROnPlateau方法
# 当验证集损失在一段时间内不再下降时，缩减学习率
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)

# 打印所有准备工作完成后的时间
print("Time after all preparations:", format_time(time.time()-ts), flush=True)

best_val_loss = float('inf')  # 初始化最佳验证损失为正无穷大
best_model_state = None  # 初始化最佳模型的参数状态为空
patience_count = 0  # 初始化耐心计数器为 0，用于跟踪没有改进的训练周期数量
r2score = R2Score(num_outputs=len(TARGET_COLS)).to(device)  # 创建一个 R2Score 对象，用于计算 R2 分数（ 确定预测值与目标值之间的拟合优度 ）
for epoch in range(EPOCHS):  # 在每个训练周期中进行迭代
    print("")  # 打印空行
    model.train()  # 将模型设置为训练模式
    total_loss = 0  # 初始化总损失为0
    steps = 0  # 初始化步骤计数为0
    for batch_idx, (inputs, labels) in enumerate(train_loader):  # 在训练数据加载器中进行迭代，获取输入数据和标签
        optimizer.zero_grad()  # 将优化器的梯度置零
        outputs = model(inputs)  # 通过模型前向传播获得输出值
        loss = criterion(outputs, labels)  # 计算模型预测值与标签之间的损失
        loss.backward()  # 反向传播并更新模型参数
        optimizer.step()  # 更新模型参数

        total_loss += loss.item()  # 累积总损失
        steps += 1  # 增加步骤计数

        if (batch_idx + 1) % PRINT_FREQ == 0:  # 如果达到了打印频率
            current_lr = optimizer.param_groups[0]["lr"]  # 获取当前学习率
            elapsed_time = format_time(time.time() - ts)  # 计算已经过的时间
            print(f'  Epoch: {epoch+1}',\
                  f'  Batch: {batch_idx + 1}/{len(train_loader)}',\
                  f'  Train Loss: {total_loss / steps:.4f}',\
                  f'  LR: {current_lr:.1e}',\
                  f'  Time: {elapsed_time}', flush=True)  # 打印当前训练周期、批次索引、训练损失、学习率和已经过的时间
            total_loss = 0  # 重置总损失
            steps = 0  # 重置步骤计数

    model.eval()  # 将模型设置为评估模式
    val_loss = 0  # 初始化验证损失为0
    y_true = torch.tensor([], device=device)  # 创建空的张量用于存储真实标签
    all_outputs = torch.tensor([], device=device)  # 创建空的张量用于存储模型的所有输出
    with torch.no_grad():  # 禁用梯度计算，避免在验证阶段进行不必要的内存消耗
        for inputs, labels in val_loader:  # 在验证数据加载器中进行迭代，获取输入数据和标签
            outputs = model(inputs)  # 通过模型前向传播获得输出值
            val_loss += criterion(outputs, labels).item()  # 累积验证损失
            y_true = torch.cat((y_true, labels), 0)  # 将真实标签添加到张量中
            all_outputs = torch.cat((all_outputs, outputs), 0)  # 将模型输出添加到张量中
    r2 = 0  # 初始化 R2 分数为0
    r2_broken = []  # 创建空列表用于存储被排除的目标索引
    r2_broken_names = []  # 创建空列表用于存储被排除的目标特征名称
    for i in range(368):  # 针对每个目标进行 R2 分数的计算
        r2_i = r2score(all_outputs[:, i], y_true[:, i])  # 计算第 i 个目标的 R2 分数
        if r2_i > 1e-6:  # 如果 R2 分数大于1e-6
            r2 += r2_i  # 累积 R2 分数
        else:
            r2_broken.append(i)  # 将目标索引添加到被排除列表中
            r2_broken_names.append(FEAT_COLS[i])  # 将目标特征名称添加到被排除列表中
    r2 /= 368  # 计算平均 R2 分数
    avg_val_loss = val_loss / len(val_loader)  # 计算平均验证损失
    print(f'\nEpoch: {epoch + 1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')  # 打印当前训练周期的验证损失和平均 R2 分数
    print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')  # 打印有多少目标在计算 R2 分数时被排除掉

    scheduler.step(avg_val_loss)  # 根据平均验证损失调整学习率

    if avg_val_loss < best_val_loss:  # 如果平均验证损失小于最佳验证损失
        best_val_loss = avg_val_loss  # 更新最佳验证损失
        best_model_state = model.state_dict()  # 保存当前模型参数状态
        patience_count = 0  # 重置耐心计数器
        print("Validation loss decreased, saving new best model and resetting patience counter.")  # 打印验证损失减少的消息
    else:
        patience_count += 1  # 增加耐心计数器
        print(f"No improvement in validation loss for {patience_count} epochs.")  # 打印验证损失没有改进的消息

    if patience_count >= PATIENCE:  # 如果耐心计数器超过指定的耐心阈值
        print("Stopping early due to no improvement in validation loss.")  # 打印提前结束训练的消息
        break  # 停止训练循环

del x_train, y_train  # 删除训练数据和标签的张量
gc.collect()  # 执行内存清理

model.load_state_dict(best_model_state)  # 加载最佳模型的参数状态
model.eval()  # 将模型设置为评估模式

df_test = pl.read_csv(DATA_PATH + "leap-atmospheric-physics-ai-climsim/test.csv")  # 读取测试数据集

for col in FEAT_COLS:  # 将特征列的数据类型转换为Float32
    df_test = df_test.with_columns(pl.col(col).cast(pl.Float32))

x_test = df_test.select(FEAT_COLS).to_numpy()  # 选择特征列并转换为NumPy数组

x_test = (x_test - mx.reshape(1, -1)) / sx.reshape(1, -1)  # 对测试数据进行归一化

predt = np.zeros([x_test.shape[0], output_size], dtype=np.float32)  # 初始化存储预测结果的数组

i1 = 0
for i in range(10000):  # 对测试数据进行批量预测
    i2 = np.minimum(i1 + BATCH_SIZE, x_test.shape[0])
    if i1 == i2:  # 如果范围不变，退出循环
        break

    inputs = torch.from_numpy(x_test[i1:i2, :]).float().to(device)  # 将当前切片的测试数据转换为 PyTorch 张量

    with torch.no_grad():  # 在推理阶段不需要跟踪梯度
        outputs = model(inputs)  # 获取模型的预测值
        predt[i1:i2, :] = outputs.cpu().numpy()  # 将预测结果存储在 predt 中

    i1 = i2  # 更新i1到当前批次的末尾

    if i2 >= x_test.shape[0]:  # 如果到达测试数据末尾，退出循环
        break

for i in range(sy.shape[0]):  # 将预测值按照归一化前的标准差和均值逆转换
    if sy[i] < MIN_STD * 1.1:  # 如果标准差过小
        predt[:, i] = 0  # 将预测值设为0

predt = predt * sy.reshape(1, -1) + my.reshape(1, -1)  # 反向归一化预测值

ss = pd.read_csv(DATA_PATH + "leap-atmospheric-physics-ai-climsim/sample_submission.csv")  # 读取提交文件样本

ss.iloc[:, 1:] = predt  # 将预测结果存储到提交文件中

use_cols = []
for i in range(27):  # 选择一部分目标列用于后续的处理
    use_cols.append(f"ptend_q0002_{i}")

ss2 = pd.read_csv(DATA_PATH + "leap-atmospheric-physics-ai-climsim/sample_submission.csv")  # 读取第二份提交文件样本

df_test = df_test.to_pandas()  # 将测试数据转换为Pandas DataFrame
for col in use_cols:  # 对目标列进行处理
    ss[col] = -df_test[col.replace("ptend", "state")] * ss2[col] / 1200.  # 根据给定公式计算新的目标值

test_polars = pl.from_pandas(ss[["sample_id"]+TARGET_COLS])  # 从DataFrame创建Pandas表格
test_polars.write_csv("submission.csv")  # 将结果写入提交文件

print("Total time:", format_time(time.time()-ts))  # 打印总运行时间

```

## 2. 数据加载器

- 通常需要使用到 DataLoader，虽然数据集已经读取在内存中，但直接使用整个数据集进行训练也会导致整个数据集被一次性加载到显存中，这可能会超出显存容量，尤其是在使用大型模型时
- 训练集和验证集通常都需要数据加载器
  ```python
  # 训练集通常需要被打乱
  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
  # 验证集通常不需要被打乱
  val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
  ```

## 3. 训练过程

- 首先确定需要训练的轮次
  ```python
  for epoch in range(EPOCHS):
  ```
- 进行当前轮的计算
  - 先在训练数据加载器中进行迭代，拿到数据，喂给模型，前向传播，然后反向传播，更新模型参数
    ```python
    for batch_idx, (inputs, labels) in enumerate(train_loader):
    ```
  - 然后在验证数据加载器中，注意，需要禁用梯度计算，验证阶段只进行前向传播，不进行反向传播，因为验证数据只需要累积验证损失，而不需要去优化模型参数
    ```python
     with torch.no_grad():
          for inputs, labels in val_loader:
    ```
  - 计算平均验证损失，如果是最优，则记录当前模型，并刷新耐心计数器为 0，如果不是最优，则耐心计数器 + 1，如果高于阀值，就提前结束训练
  - 计算 r2score，并打印出来，方便实时观察模型情况。平均验证损失更侧重于描述模型的误差大小，而 r2score 更侧重于描述模型对数据的拟合程度和解释能力
- 进行下一轮计算
