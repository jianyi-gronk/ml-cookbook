## 1. Dropout 层（ 随机失活 ）

- **在大小 > 256 的全连接层之后就应该始终使用 dropout 将过拟合的几率最小化，在卷积层则通常不使用**
- Dropout 层是一种用于神经网络中的正则化技术，旨在减少过拟合现象的发生。它的原理是在训练过程中，随机地将一些神经元的输出设置为零，从而 “丢弃” 它们。在下一次训练迭代中，不同的神经元被随机选择被 “丢弃”，这样可以防止神经元之间的过拟合
- 具体来说，Dropout 层会对每个输入进行一次掩码操作，以随机生成一个二进制掩码向量，掩码向量中的元素可能是 0 或 1。在掩码向量中，0 表示对应的神经元被 “丢弃”，而 1 表示保留该神经元的输出。因此，在每个训练迭代中，不同的神经元将被随机地 “丢弃”，这种随机性有助于避免神经元之间出现过拟合
- 使用 Dropout 层可以带来多方面的好处。首先，它可以减少模型中神经元之间的共适应（ co-adaptation ），避免过拟合现象的发生，提高模型的泛化能力。其次，它可以增加模型的复杂性，提高模型的表达能力。最后，Dropout 层可以用于任何类型的神经网络，包括卷积神经网络和循环神经网络等
- 然而，使用 Dropout 层也有一些缺点。首先，它会降低网络的有效性，因为一些神经元的输出被随机丢弃，会导致一些信息的丢失。其次，Dropout 层会增加训练时间和计算成本，因为每个训练迭代需要重新生成掩码向量。最后，Dropout 层可能不适合所有的任务，特别是在训练数据量较小的情况下

## 2. BN 和 LN

#### 2.1 批归一化（ BatchNormalization，BN ）

- BatchNormalization 的算法原理基于对神经元输入的标准化处理。在深度神经网络中，由于每层神经元的输入都会受到前一层输出的影响，而前一层的参数更新过程是随着训练的进行逐步进行的，所以每层神经元的输入分布也会随着训练过程发生变化。这样的变化可能导致神经网络训练的不稳定，使得模型性能下降
- BatchNormalization 通过**对每个 batch 中的数据进行标准化处理，使得每层输入的均值接近 0，方差接近 1**，并且将标准化后的数据通过缩放和平移，将数据重新映射到一个更合适的分布范围。这样可以使得每层神经元的输入保持稳定，减少了训练过程中的内部协变量偏移（ Internal Covariate Shift ），从而提高了模型的训练速度和性能
- **BN 层往往用在深度神经网络的卷积层之后，但是并不一般使用在全连接层之后**，因为全连接层本身不像卷积层那样有明显的特征图结构，因此其输出可能不像卷积层那样容易出现内部协变量偏移的问题
- **BN 是按照样本数计算归一化统计量的，当样本数很少时，效果通常不理想**
- 数学公式如下：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/83d06bcc-dad0-47d6-9ef7-697bdd70dc71)
  - 其中，𝑥 代表输入数据样本，𝜇 代表输入数据样本的均值，𝜎2 代表输入数据样本的方差，𝜖 是一个很小的值，用于防止除零错误。𝑥̂ 是标准化之后的数据，𝑦 是经过缩放和平移后的数据，𝛾 是缩放参数，𝛽 是平移参数，它们都是可学习的参数
- 计算步骤
  - 对每个 batch 中的输入数据进行标准化处理，计算均值 𝜇 和方差 𝜎2
  - 对标准化后的数据进行缩放和平移，得到最终的输出结果

#### 2.2 层归一化（ LayerNormalization，LN ）

- **BN 是对全部样本的一个特征进行归一化，而 LN 是对一个样本的全部特征进行归一化**，如果对人身体特征进行归一化，比如对身高，体重和年龄一起求一个均值方差，那确实看上去毫无作用，但在 NLP 领域却非常有效果
- 在 NLP 中，n 个特征都可能表示不同的词，这个时候我们仍然采用 BatchNormalization 的话，对第一个词进行操作，很显然意义就不是非常大了，因为任何一个词都可以放在第一个位置，而且很多时候词序对于我们对于句子的影响没那么大，而此时我们对 n 个词进行 LN 等操作可以很好地反映句子的分布

#### 2.3 BN 和 LN

- BN 和 LN 都可以比较好的抑制梯度消失和梯度爆炸的情况
- BN 适合于 CV 中的 CNN 等网络，不适合 ​​batchsize​​ 较小的情况
- 而 LN 适合用于 NLP 中的 RNN、transformer 等网络，因为 sequence 的长度可能是不一致的

## 3. 正则化（ Regularization ）

- 正则化是机器学习中对原始损失函数引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称，也就是目标函数变成了原始损失函数 + 额外项，常用的额外项一般有两种，L1 正则化和 L2 正则化
- L1 正则化和 L2 正则化可以看做是损失函数的惩罚项。所谓 “惩罚” 是指对损失函数中的某些参数做一些限制

#### 3.1 L1 正则化

- L1 正则化是指权值向量 w 中各个元素的绝对值之和，通常表示为 $||w||_1$
- L1 正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择
- “稀疏性” 说白了就是模型的很多参数是 0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组作为一个特征，那么特征数量会达到上万个。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是 0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只关注系数是非零值的特征。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能

#### 3.2 L2 正则化

- L2 正则化是指权值向量 w 中各个元素的平方和然后再求平方根，通常表示为 $||w||_2^2$
- L2 通过减少模型参数的权值来控制过拟合的效果，L2 中模型参数 W 中每个元素都很小，接近于 0，一般不会等于 0
- 拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是抗扰动能力强

#### 3.3 真实使用

- 一般都会在正则化项之前添加一个系数（ 正则化参数 ） λ（ λ$||w||_1$，λ$||w||_2^2$ ），这个系数需要用户指定（ 也就是超参数之一 ），一般从 0 开始尝试，逐渐增大 λ
- 实际中正则化中使用 L2 的会更多一些，因为 L1 会趋向于提取少量的有效特征项，而 L2 会选择更多的特征。在所有特征中只有少量特征其重要作用的情况，可以选择 L1 来自动选择比较合适的特征属性。而如果所有的特征中，大部分的特征都能起到一定的作用，还是使用 L2 会比较合适

## 4. 激活函数

- 激活函数能分成两类，饱和激活函数 和 非饱和激活函数
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a1a5a7a4-ba57-4c78-98ee-27a6be20f987)
- sigmoid 和 tanh 是 饱和激活函数，而 ReLU 及其变体则是 非饱和激活函数
- 使用 非饱和激活函数 的优势在于两点
  - 非饱和激活函数 能解决所谓的 梯度消失 问题
  - 非饱和激活函数 能加快收敛速度

#### 4.1 饱和激活函数

- Sigmoid 函数需要一个实值输入压缩至 [0, 1] 的范围
  - σ(x) = 1 / (1 + exp(−x))
- tanh 函数需要讲一个实值输入压缩至 [-1, 1] 的范围
  - tanh(x) = 2σ(2x) − 1

#### 4.2 非饱和激活函数

- 修正线性单元（ ReLU ）
  - ReLU 函数是带有卷积图像的输入 x 的最大函数 (x,o)。ReLU 函数将矩阵 x 内所有负值都设为零，其余的值不变。ReLU 函数的计算是在卷积之后进行的，因此它与 tanh 函数和 sigmoid 函数一样，同属于“非线性激活函数”
- Leaky ReLU
  - ReLU 是将所有的负值都设为零，相反，Leaky ReLU 是给所有负值赋予一个非零斜率。Leaky ReLU 激活函数以数学的方式我们可以表示为：
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0b10b4ff-2ecf-4938-895f-e6d3ba049472)
- PReLU（ Parametric ReLU ）
  - PReLU 可以看作是 Leaky ReLU 的一个变体。在 PReLU 中，负值部分的斜率是根据数据来定的，而非预先定义的。在 ImageNet 分类上，PReLU 是超越人类分类水平的关键所在
- RReLU（ Randomized ReLU ）
  - RReLU 也是 Leaky ReLU 的一个变体。在 RReLU 中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU 的亮点在于，在训练环节中，aji 是从一个均匀的分布 U(I,u)中随机抽取的数值。形式上来说，我们能得到以下结果：
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/70334ac2-f7a6-41cc-b812-51f584ed714c)
- 下图是 ReLU、Leaky ReLU、PReLU 和 RReLU 的比较：
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/49d2f556-3a32-4ab2-815b-e118e463266d)
  - Leaky ReLU 中的 ai 是固定的
  - PReLU 中的 ai 是根据数据变化的
  - RReLU 中的 aji 是一个在一个给定的范围内随机抽取的值，这个值在测试环节就会固定下来
- ELU 激活函数
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/cd907bd3-70a5-4ade-8e4f-5b9d1b0c23ca)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4b3ae7f7-a19d-4b4f-b384-ffaa5fa974ab)
  - 融合了 sigmoid（ 多了一个 α 参数 ）和 ReLU，左侧具有软饱和性，右侧无饱和性。
  - 右侧线性部分使得 ELU 能够缓解梯度消失，而左侧的软饱和性能够让 ELU 对输入变化或噪声更鲁棒
  - ELU 的输出均值接近于零，所以收敛速度更快
  - 在 ImageNet 上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU 网络在 MSRA 的 Fan-in （caffe ）初始化下会发散，而 ELU 网络在 Fan-in/Fan-out 下都能收敛

#### 4.3 建议

- **不要使用 ReLU** ，它太旧了。虽然它是非常有用的非线性函数，可以解决很多问题。但是，你可以试试用它微调一个新模型，由于 ReLU 阻碍反向传播，初始化不好，你没法得到任何微调效果。但是你应该用 PreLU 以及一个非常小的乘数，通常是 0.1。使用 PreLU 的话收敛更快，而且不会像 ReLU 那样在初始阶段被卡住。ELU 也很好，但成本高
- 在执行最大池化操作之前，不要在卷积层输出上直接应用 ReLU 或 PreLU 等激活函数，而是在**完成卷积操作并操作完最大池化得到特征图后再进行激活函数的应用**
- 避免使用 Sigmoid/tanh 激活函数，它们代价昂贵，容易饱和，而且可能会停止反向传播。实际上，你的网络越深，就越应该避免使用 Sigmoid 和 TanH。可以使用更便宜而且更有效的 ReLU 和 PReLU 的门，这两者能够促进稀疏性，而且它们的反向传播更加鲁棒
