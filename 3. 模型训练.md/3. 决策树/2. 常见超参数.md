- 以 scikit-learn 框架举例，其他框架的参数含义类似，且 scikit-learn 内各个集成树的参数类似，大都只是其中部分参数的默认值不同（ 只需要大致了解，存在印象，方便以后碰到场景能有调参思路 ）

## 1. 通用参数

- max_leaf_nodes
  - 表示最大叶子节点数，为 None 表示对叶子节点数没有限制，取值为整数设置节点数
- max_depth
  - 决策树最大深度，默认为 None，表示决策树在构建最优模型的时候不会限制子树的深度
  - 如果模型样本量多，特征也多的情况下，推荐限制最大深度；若样本量少或者特征少，则可以不限制最大深度
- min_samples_leaf
  - 默认值是 1，表示叶子节点含有的最少样本
  - 整数表示个数，浮点数表示取大于等于 (样本数 \* min_samples_leaf) 的最小整数
  - 若叶子节点样本数小于 min_samples_leaf，则对该叶子节点和兄弟叶子节点进行剪枝，只留下该叶子节点的父节点
- max_features
  - 表示构建决策树时考虑的最大特征数，一般默认是 None，表示最大特征数是所有特征
  - 其他取值有
    - "log2"，表示最大特征数是 $log_2N$
    - "sqrt"，表示最大特征数是 $\sqrt{N}$
    - 整数，表示考虑的最大特征数
    - 浮点数，表示考虑 (N \* max_features) 取整后的特征数，其中 N 表示样本的特征数
- class_weight
  - 表示每个特征的权重，主要用于处理分类问题中的类别不平衡情况，模型可能会偏向于样本数量较多的类别，通过设置 class_weight 参数，可以使得模型在训练过程中更加关注样本数量较少的类别
  - 若等于 None，意味着模型会将所有类别的样本视为同等重要，也就是每个类别的权重都为 1
  - 可以用字典的形式传入 {class_label: weight}
  - 如果选择了 "balanced”，则 $weight_{样本} = n_{samples} / (n_{classes} * n_i)$，其中 $n_{samples}$ 是训练数据的总样本数，$n_{classes}$ 是类别数量，$n_i$ 是第 i 个类别的样本数
- warm_start
  - 表示模型是否从上次训练的结果上继续训练，默认值为 False
  - 取值为 True 时，假如已经有训练好 100 轮，那么再次训练 200 轮决策树的时候，会直接在之前 100 轮的基础上训练
- random_state
  - 控制构建树的时候，样本和特征采样的随机性，默认为 None，表示构建中的随机性时不固定的
  - 取值可为 int 类型和 RandomState 实例，类似于随机种子
- verbose
  - 控制记录模型拟合和预测时的详细程度，默认为 0，为不在标准输出流输出日志信息。
  - 值为 1 时输出进度条记录；值为 2 时为每个 epoch 输出一行记录
- monotonic_cst
  - 用于对决策树模型中每个特征的单调性进行约束，因为特征的取值变化与目标变量之间可能存在一定的关系
    - 单调增加：比如预测房屋价格时，房屋面积这个特征通常与价格是单调增加关系
    - 单调减少，比如预测商品销量时，商品价格这个特征与销量通常是单调减少关系
  - 默认取值为 None，表示特征和目标值没有相关性
  - 取值为长度为 n_features 的 int 数组，映射各个特征与目标值的关系，数组元素取值为
    - 1: 表示 特征 和 目标变量 存在单调增加的关系
    - 0：表示 特征 和 目标变量 不存在单调关系

## 2. Bagging 参数

- n_estimators
  - 森林中的决策树数量，默认 100
  - 若 n_estimators 太小容易欠拟合，太大不能显著的提升模型，同时需要更大的计算量和内存，所以 n_estimators 需要选择适中的数值
- criterion
  - 表示节点的划分标准，默认为 "gini"，取值范围为 "gini"（ 基尼不纯度 ），"entropy"（ 信息熵 ），"log_loss"（ 对数损失 ）
  - "entropy" 和 "log_loss" 基本相同，都是基于信息增益，而 "gini" 是基于基尼不纯度
- bootstrap
  - 表示是否对样本集进行有放回抽样来构建树，大部分模型中默认值是 True，但是极端随机森林和孤立森林中是 False
- max_samples
  - 如果 bootstrap 为 True，则从样本中抽取样本，默认为 None，抽取全部样本
  - 如果是 int 则用该数量样本构建树，如果是 float，则用 (max_samples \* 样本数) 个样本构建树
- min_samples_split
  - 默认值是 2，表示节点可分的最小样本数，整数型和浮点型的含义与 min_samples_leaf 类似
- min_impurity_decrease
  - 取值为浮点数，默认为 0.0，表示节点划分的最小 “不纯度”
  - 假设 “不纯度” 用信息增益表示，若某节点划分时的信息增益大于等于 min_impurity_decrease，那么该节点还可以再划分；反之，则不能划分
- min_weight_fraction_leaf
  - 表示叶子节点最小的样本权重和，叶子节点如果小于这个值，则会和兄弟节点一起被剪枝，只保留叶子节点的父节点
  - 默认是 0，表示不考虑样本权重问题。一般来说，如果有较多样本的缺失值或偏差很大，则尝试设置该参数值
- n_jobs
  - 并行计算数，默认是 None，表示 1
  - 取值为整数，如果为 -1 时，表示使用所有处理器
- oob_score
  - 是否采用袋外样本（ 抽样外的样本 ）来评估模型的好坏，True 代表是，默认值 False
  - 袋外样本误差是测试数据集误差的无偏估计，所以推荐设置 True
- ccp_alpha
  - 用于控制 最小代价复杂度剪枝（ CART 的 Minimal Cost-Complexity Pruning ），是后剪枝的核心参数，其作用是通过调整剪枝强度来平衡模型的复杂度和泛化能力
  - 默认值是 0.0，表示不进行剪枝，生成完全生长的树，取值为非负浮点数，将剪掉代价复杂度小于 ccp_alpha 的枝，所以该值越大，剪枝越多，树越简单

## 3. Boosting 参数

- loss
  - 用于控制在 boosting 过程中使用的 loss 函数，默认为 "log_loss"
- learning_rate
  - 学习率，也称为收缩率，默认值为 0.1，通过减少每一步的权重，可以提高模型的鲁棒性
- max_iter
  - Boosting 过程的最大迭代次数
- l2_regularization
  - 设置 L2 正则化参数，默认为 0，表示无正则化
- max_bins
  - max_bins 定义了在对非缺失值的特征进行分箱时，所能使用的最大 bins（ 箱子 ）数量。通常默认值是 255，表示算法会尝试将每个特征的非缺失值划分到最多 255 个不同的 bins 中
  - 除了 max_bins 个用于非缺失值的 bins 之外，算法还会额外保留一个 bin 用于处理缺失值。也就是说，最终每个特征的总 bins 数量为 max_bins + 1（ 包含缺失值的 bin ）
- early_stopping
  - 默认为 "auto"，表示如果样本量大于 10000，则启用提前停止
  - 如果为 True，则启用提前停止，为 False 则禁用提前停止
- n_iter_no_change
  - 用于确定何时 early_stopping，仅在开启 early_stopping 时使用
  - 设置容忍度，即当最后 n_iter_no_change 个分数均不优于倒数第 n_iter_no_change + 1 的分数时，拟合过程将停止
- tol
  - 在提前停止期间比较分数时能容忍的绝对差值，容差越高，说明越有可能提前停止，默认为 1E-7
- scoring
  - 设置用于提前停止的模型得分计算模型，默认为 "loss"
- validation_fraction
  - 设置从训练数据中划分出的验证集大小，取值为 float 时，表示为总数据的占比，取值为 int 时，表示训练集具体大小，默认为 0.1
- interaction_cst
  - 用于控制特征在子节点分裂时的交互方式，默认为 None，表示不限制特征交互
  - 取值为 "pairwise" 表示只允许特征之间进行两两交互。也就是说，在树的每个节点分裂时，每次只考虑两个特征之间的组合来寻找最优的分裂点。例如，若有特征 A、B 和 C，模型在分裂节点时会考虑 (A, B)、(A, C) 和 (B, C) 这些两两组合的特征交互情况。这种设置可以一定程度上限制模型的复杂度，避免过度复杂的特征交互导致过拟合
  - 取值为 "no_interactions" 表示不允许特征之间进行交互，每次只基于单个特征来寻找最优分裂点，通常用于希望模型保持简单结构，或者数据中特征之间本身不存在明显交互关系的场景
  - 取值为 序列 类型，则序列中的每个元素指定了一组可以相互交互的特征索引，假设总共有 5 个特征，则 interaction_cst = [{0, 1}, {2, 3, 4}] 表示在树的每个分支中，要么只基于特征 0 和 1 进行分裂，要么只基于特征 2、3 和 4 进行分裂
