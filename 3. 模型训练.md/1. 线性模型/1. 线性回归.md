## 1. 线性回归

- 模型与数据拟合成一条直线
- 本质就是求解方程，w，b 是模型的参数（ 也可以叫系数，权重，是可以调整的模型变量 ）
  $$f_{w,b}(x) = wx + b$$

## 2. 代价函数

- 数据和直线不会完全拟合，因此需要 代价函数 J(w, b) 来衡量误差，**而线性回归的核心就是最小化代价函数**
- **线性回归的代价函数通常为 MSE**，为了方便求导，在公式里除以 2，相当于 $\frac{1}{2}$ MSE（ 但效果一样 ）
  $$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2$$

## 3. 最小化代价函数

- 线性回归中的最小化代价函数通常有以下几种
  - 梯度下降
    - 迭代优化算法，从初始点出发，沿函数梯度方向逐步迭代更新参数，逼近最优解
  - 解析解法
    - 闭式解，直接通过数学公式（ 如矩阵求逆、求导为零 ）一步计算出全局最优解
  - 正则化技术
    - 间接优化，修改代价函数，添加约束项（ 如权重惩罚 ），使优化过程更稳定且解更泛化

## 4. 梯度下降（ Gradient Descent，GD ）

#### 4.1 核心步骤

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例 ）
  1. 从一个初始 w，b 开始
  2. 不断的修改 w，b，去减少 J(w, b)
  3. 直到 J 到达最小值
  - 一般是局部最小值，不是全局最小值，因此可能有多个最小值
  - 但是线性回归的代价函数是严格的凸二次函数，所以只有全局一个最小值（ 碗底 ）

#### 4.2 梯度下降公式

- α 为学习率，通常是 0 到 1 之间的较小正数（ 例 0.01 ），控制修改 w，b 的幅度，剩下的部分为求偏导
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  w &= \text{tmp}_w \\
  b &= \text{tmp}_b
  \end{align*}
  $$
- 注意，不能用修改后的 w 去计算，下面是错误的
  $$
  \begin{align*}
  \text{tmp}_w &= w - \alpha \frac{\partial}{\partial w} J(w, b) \\
  w &= \text{tmp}_w \\
  \text{tmp}_b &= b - \alpha \frac{\partial}{\partial b} J(w, b) \\
  b &= \text{tmp}_b
  \end{align*}
  $$
- 梯度下降公式中求导公式推导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/612b5384-2789-4186-9fc2-11609406f7e8)

#### 4.3 学习率

- 当随着迭代次数增加，代价函数结果变化异常，需判断是否因为代码异常或者学习率过大，可以通过将学习率设置成非常小的值，如果还不是下降趋势，那么就是代码问题
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a9b2b4e4-1e76-420d-9bd0-4e05b0dfca8e)
- 学习率的选择，可以从低到高尝试，0.001，0.01，0.1

#### 4.4 梯度下降方法细节

- **通过精确计算梯度确定当前唯一最优方向，步长由学习率调整**
- 梯度下降常用基础方法
  - 批量梯度下降（ Batch Gradient Descent, BGD ）
    - 每次迭代使用全部训练样本，收敛平稳但速度慢，内存需求大
  - 随机梯度下降（ Stochastic Gradient Descent, SGD ）
    - 每次迭代使用单个样本，收敛快但波动大，内存需求小
  - 小批量梯度下降（ Mini-batch Gradient Descent, MBGD ）
    - 代表每次迭代训练部分样本，通常样本数设置为 2 的幂次方，通常设置 2，4，8，16，32，64，128，256，512（ 设置成 2 的幂次方，更有利于 GPU 加速，并且很少设置大于 512 ）
    - 平衡 BGD 的稳定性和 SGD 的速度
- 梯度下降常用进阶方法
  - Adam 算法（ adaptive moment estimation ）
    - 核心思路是，如果 $w_j$ 持续向一个方向移动，那么就增加学习率 α，如果 $w_j$ 持续向另一个方向移动的话，就减小学习率 α
      ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4afcfcdc-2310-42f1-aafb-40003502cc6b)
    - Adam 算法对不同的 $w_1$，$w_2$ ... $w_n$ 和 b 都有着不同的学习率 α

#### 4.5 举例概括

- 如图
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)
- 但在线性回归中，有且只有一个最小值，成碗状图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/109b13d7-41fd-4f96-b6e2-f05fec3953c8)
- 当随着迭代次数增加，代价函数结果趋于稳定，则梯度下降收敛，可通过看曲线图或者判断代价函数结果是否小于阈值
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/896033c0-f761-4feb-8298-cb3a6e7fc598)

## 5. 解析解法（ OLS，普通最小二乘法 ）

#### 5.1 核心思想

- **直接通过数学公式一步计算全局最优解，无需迭代过程**
- 数学基础：令代价函数的偏导数等于零求解极值点
- 适用于线性回归的正规方程（ Normal Equation ）方法
- 算法特性
  - 时间复杂度 $O(n^3)$：矩阵求逆的代价高，特征维度 > 10000 时效率低
  - 空间复杂度 $O(n^2)$：需存储 $\mathbf{X}^T\mathbf{X}$
  - 适用条件 $\mathbf{X}^T\mathbf{X}$：必须可逆，否则需正则化或伪逆
  - 样本要求 $m \geq n+1$：样本数 ≥ 特征数 + 1

#### 5.2 矩阵表示法

- 定义设计矩阵 $\mathbf{X}$ 和目标向量 $\mathbf{y}$：
  $$
  \mathbf{X} = \begin{bmatrix}
  1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
  1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
  \end{bmatrix}, \quad
  \mathbf{y} = \begin{bmatrix}
  y^{(1)} \\
  y^{(2)} \\
  \vdots \\
  y^{(m)}
  \end{bmatrix}, \quad
  \mathbf{w} = \begin{bmatrix}
  b \\
  w_1 \\
  w_2 \\
  \vdots \\
  w_n
  \end{bmatrix}
  $$

#### 5.3 正规方程

- 正规方程是线性回归中直接求解最优参数的闭式解公式，它通过矩阵运算一步计算出全局最优解
  $$
  \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
  $$
- 推导过程从代价函数出发，通过数学变换得到正规方程的过程
  - 代价函数矩阵形式
    $$
    J(\mathbf{w}) = \frac{1}{2m} (\mathbf{X}\mathbf{w} - \mathbf{y})^T(\mathbf{X}\mathbf{w} - \mathbf{y})
    $$
  - 对 $\mathbf{w}$ 求导并置零
    $$
    \begin{align*}
    \nabla_{\mathbf{w}} J(\mathbf{w}) &= \frac{1}{m} \mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y}) = 0 \\
    \Rightarrow \mathbf{X}^T\mathbf{X}\mathbf{w} &= \mathbf{X}^T\mathbf{y}
    \end{align*}
    $$

## 6. 正则化（ Regularization ）

#### 6.1 核心目的

- **解决过拟合问题**：当模型过度拟合训练数据噪声时，正则化通过约束参数大小提升泛化能力
- **控制模型复杂度**：在代价函数中添加惩罚项，防止权重过大
- **提高数值稳定性**：尤其对解析解法中矩阵不可逆的情况提供解决方案

#### 6.2 正则化方法

- 正则化在线性回归代价函数中添加权重惩罚项，常用的正则化有 L1 和 L2
- **L1 正则化（ Lasso 回归 ）**
  $$J(\mathbf{w}, b) = \frac{1}{2m} \left[ \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n} |w_j| \right]$$
- **L2 正则化（ Ridge 回归，也叫 岭回归 ）**
  $$J(\mathbf{w}, b) = \frac{1}{2m} \left[ \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n} w_j^2 \right]$$
- $\lambda$：**正则化系数**，是可设置的超参数，控制惩罚强度
  - $\lambda = 0$：退化为标准线性回归
  - $\lambda \to \infty$：所有权重趋近于零
- 注意：**偏置项 $b$ 通常不被正则化**

#### 6.3 正则化对解法的影响

- 梯度下降的修改
  - L1 正则化 的权重更新：
    $$w_j = w_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \cdot \text{sign}(w_j) \right]$$
    - 其中 $\text{sign}(w_j)$ 为符号函数：
      $$
      \text{sign}(w_j) =
      \begin{cases}
      1 & \text{if } w_j > 0 \\
      0 & \text{if } w_j = 0 \\
      -1 & \text{if } w_j < 0
      \end{cases}
      $$
    - 即每次更新时向零方向施加固定大小的收缩（ 与权重大小无关 ）
  - L2 正则化 的权重更新：
    $$w_j = w_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} w_j \right]$$
    - 即每次更新时额外缩小权重（ 权重衰减 ）
- 解析解法的修改
  - L1 正则化 没有闭式解析解，需使用迭代优化方法（ 如坐标下降、近端梯度法 ）
    $$\min_{\mathbf{w}} \left( \frac{1}{2m} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2_2 + \lambda \|\mathbf{w}\|_1 \right)$$
    - 原因：L1 范数的绝对值项在零点不可导
    - 常用解法：最小角回归（ LARS ）、坐标下降法
  - l2 正则化 的正规方程
    $$\mathbf{w} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}$$
    - 其中 $\mathbf{I}$ 是 $(n+1) \times (n+1)$ 单位矩阵
    - **确保 $\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}$ 恒可逆**（ 解决多重共线性问题 ）
- 总结 L1/L2 更新公式差异
  | 正则化类型 | 更新行为 | 数学特性 | 结果特征 |
  |------------|----------------------------|-------------------|--------------|
  | **L2** | 按比例缩小权重 | 处处可导 | 权重接近零 |
  | **L1** | 固定步长向零移动 | 零点不可导 | 权重精确为零 |

#### 6.4 正则化效果对比

| 特性             | L1 正则化 (Lasso)          | L2 正则化 (Ridge)      |
| ---------------- | -------------------------- | ---------------------- |
| **惩罚项**       | $\sum \|w_j\|$             | $\sum w_j^2$           |
| **解的特性**     | 稀疏解（ 部分权重=0 ）     | 稠密解（ 权重接近 0 ） |
| **特征选择能力** | 较强                       | 较弱                   |
| **抗噪声能力**   | 较弱                       | 较强                   |
| **计算复杂度**   | 需特殊优化（ 如坐标下降 ） | 可直接求解析解         |

#### 6.5 正则化系数选择

- 通常尝试 $\lambda \in [0.001, 0.01, 0.1, 1, 10, 100]$
- 典型现象：
  - $\lambda$ 太小 → 过拟合（ 训练误差 << 验证误差 ）
  - $\lambda$ 太大 → 欠拟合（ 训练和验证误差均大 ）

## 7. 多元线性回归

- 多维线性回归公式
  $$f_{w,b}(\mathbf{x}) = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b$$
- 常用符号
  $$
  \begin{align*}
  x_j &= j^{\text{th}} \text{ feature} \\
  n &= \text{number of features} \\
  \vec{x}^{(i)} &= \text{features of } i^{\text{th}} \text{ training example} \\
  x_j^{(i)} &= \text{value of feature } j \text{ in } i^{\text{th}} \text{ training example}
  \end{align*}
  $$
- 公式还可以写成（ 向量点积 ）
  $$f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b = w_1 x_1 + w_2 x_2 + w_3 x_3 + \cdots + w_n x_n + b$$
  - 转换成代码
    ```
    w = np.array([1.0, 2.5, -3.3])
    x = np.array([10, 20, 30])
    b = 4
    f = np.dot(w, x) + b;
    ```
  - 并且这种向量化计算，性能更好，无论是否使用 GPU。如果不使用向量计算，则是一个一个计算后累加，如果使用向量计算，计算机会在一步中并行计算每对 (w, x) 乘积，并将每对计算结果通过专门的硬件进行高效累加
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e7bc0c60-c536-4570-9999-08c89ab7d7a9)
- 梯度下降
  $$
  \begin{align*}
  \text{repeat } \{ & \\
  \quad w_j &= w_j - \alpha \frac{\partial}{\partial w_j} J(\vec{w}, b) \\
  \quad b &= b - \alpha \frac{\partial}{\partial b} J(\vec{w}, b) \\
  \} &
  \end{align*}
  $$
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c693468b-0d16-48af-810d-3d6160ac97bb)

## 8. 多项式回归

- 如果数据不是直线，而是曲线，那么可能多项式回归的曲线更能符合数据集
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2f52c6b7-1064-44f6-bba4-626aac6cbc97)
- 这个时候通常特征缩放非常重要，因为 x 的范围是 1-1000 的话，x 的三次方范围就是 1-1 亿
