## 1. SVM

- SVM（ Support Vector Machine ），即 支持向量机，是算法框架
- 最初是纯分类模型，其论文、理论推导均围绕分类任务展开，SVC 是其具体实现，后来扩展到回归任务，提出了 SVR 实现
- 因此后面只介绍 SVC，不再介绍 SVM，因为 SVC 的概念和 SVM 相同

## 2. SVC

- SVC（ Support Vector Classification ），即 支持向量分类，从名字上能够看出，是用于 **分类问题** 的算法，核心思想是 **在特征空间中找到一个最优的决策边界（ 超平面 ），最大化两类样本之间的间隔**
  ![image](https://i.postimg.cc/7ZfMxN77/6b35fb4b-8005-4d41-99f7-1e20c5ee8e37.png)
- 核心概念：决策边界（ 超平面 ）、支持向量、间隔

#### 2.1 决策边界（ 超平面 ）

- 在二维空间中，决策边界是一条直线；在三维空间中，它是一个平面；在更高维的特征空间中，它是一个超平面（ w·x + b = 0，其中 w 是法向量，b 是偏置项 ）
- 这个超平面的目标是完美（ 或尽可能好地 ）区分开属于不同类别的数据点
- 预测时计算也十分高效，只需要计算新样本点与支持向量的关系

#### 2.2 支持向量

- 这是 SVC 名称的由来，也是其关键所在。支持向量是指那些距离决策边界最近的数据点。它们就像 “支撑” 着这个边界一样，决定了边界的位置和方向
- 事实只有这些支持向量对最终模型的构建起到重要影响，远离边界的样本点不会影响决策边界

#### 2.3 间隔

- 间隔指的是 决策边界 到 最近的正类样本点 和 最近的负类样本点 之间的 垂直距离 之和
- 最大化间隔是 SVC 的优化目标，能提升 鲁棒性，因为更宽的间隔意味着决策边界对训练数据中的微小扰动或噪声不那么敏感，模型泛化能力更强，对未见过的数据预测更稳定

## 3. 超平面不可分问题

- 在 SVC 中，并不能保证在原始特征空间中一定存在一个线性超平面（ 比如二维空间中的一条直线 ）可以完美区分两个类别的数据
- 面对这种超平面不可分的情况时，SVC 主要依靠两大核心策略来解决：软间隔 和 核技巧

#### 3.1 软间隔

- 目的：
  - **允许一些样本点被错误分类或落在间隔区域内**，以换取更宽的间隔和更好的泛化能力，避免模型对噪声或异常点过于敏感（ 过拟合 ）
- 实现方式：在优化目标中引入 松弛变量 $ξ_i ≥ 0$ 和 正则化参数 C
  - 松弛变量 $ξ_i$
    - **量化了第 i 个样本违反间隔约束的程度**
    - $ξ_i = 0$：样本点完全满足约束，在间隔边界之外或之上
    - $0 < ξ_i < 1$：样本点在间隔区域内部，但在决策边界的正确一侧（ 即分类正确 ）
    - $ξ_i ≥ 1$：样本点被错误分类（ 在决策边界的错误一侧 ）
  - 正则化参数 C
    - 是大于 0 的超参数，由用户设定，它**控制着对误分类（ 或进入间隔区域 ）的惩罚力度**
    - C 值很大：
      - 模型会强烈惩罚任何违反间隔约束的点（ $ξ_i > 0$ ）
      - 这迫使优化过程尽量少犯错误，导致间隔变窄
      - 极端情况下（ $C → ∞$ ），接近硬间隔，但可能对噪声过拟合
    - C 值很小：
      - 模型对违反间隔约束的点惩罚较轻
      - 这意味着模型更容忍一些误分类或点落在间隔内，从而获得一个更宽的间隔
      - 这通常能提高模型的鲁棒性和泛化能力，但可能导致一些点被错误分类（ 欠拟合 ）
- 优化目标修改：
  - 从最小化 $||w||² / 2$（ 最大化间隔 ）变成了最小化 $||w||² / 2 + C * Σ(ξ_i)$
  - 这是一个权衡：既要让间隔尽可能宽（$||w||² / 2$ 小），又要让违反约束的程度尽可能小（ $Σ(ξ_i)$ 小），而 C 决定了这个权衡的侧重点

#### 3.2 核技巧

- 核心思想
  - Cover 定理表明，将复杂的非线性可分模式数据投射到足够高维的空间后，它们线性可分的概率会大大增加
  - 因此我们的目的是，**将数据从原始的低维非线性可分空间，映射到一个更高维（ 甚至无限维 ）的特征空间**，使得在这个新空间中，数据变得线性可分（ 或近似线性可分 ），然后在新空间中应用线性 SVC
- 映射过程分为显式映射和隐式映射（ SVM 使用隐式映射 ）
  - 显式映射
    - 映射函数比如 $φ：φ(x, y) = (x, y, x+y)$ 将原始二维空间映射到三维空间
    - 但是对于高维映射（ 可能还是无限维 ），计算 和 存储 成本会急剧上升
  - 隐式映射
    - **在 SVM 的优化过程和最终决策中，并不需要直接知道映射后的高维向量，只需要计算映射后空间中任意两个向量 $φ(x_i)$ 和 $φ(x_j)$ 的内积 $<φ(x_i), φ(x_j)>$**
    - 核技巧的核心在于定义了一个函数 K（ 核函数 ），这个函数直接在原始的低维输入空间中进行计算，但其计算结果恰好等于在高维特征空间中计算的内积 $K(x_i, x_j) = <φ(x_i), φ(x_j)>$
    - 因此 SVM 不需要计算并保存映射后的高维向量，直接在原始空间计算即可
- 常用核函数
  - 多项式核
    - $K(x_i, x_j) = (γ * <x_i, x_j> + r)^d$，能学习多项式决策边界
    - d 控制新空间的维度（ 多项式阶数 ），γ（ gamma ）是缩放参数，r 是常数项
  - 径向基函数核
    - $K(x_i, x_j) = exp(-γ * ||x_i - x_j||²)$，这是最常用的非线性核
    - γ（ gamma ）控制单个样本的影响范围，本质上衡量的是样本之间的相似度（ 距离越近，相似度/K 值越高 ）
      - γ 大：高斯钟形曲线变窄，影响范围小，决策边界更复杂，可能过拟合
      - γ 小：高斯钟形曲线变宽，影响范围大，决策边界更平滑，可能欠拟合
  - Sigmoid 核
    - $K(x_i, x_j) = tanh(γ * <x_i, x_j> + r)$，在特定参数下类似于神经网络激活函数
- 总结：**将原始空间映射到高维空间，找到一个线性超平面，然后再将其投影回原始空间，这时候决策边界可能看起来弯曲复杂（ 如二维空间中的圆或复杂曲线 ），形成非线性决策边界**

#### 3.3 软间隔 + 核技巧

- SVC 通常使用的方法：使用核技巧 + 软间隔
  - 选择非线性核（ 通常是 RBF 核 kernel='rbf'，因其普适性好 ）
  - 同时调整关键超参数：
    - C：控制间隔宽度与分类错误/间隔违反之间的权衡
    - γ ( gamma，仅用于 RBF、Poly、Sigmoid 核 )：控制核函数的 “宽度” 或样本影响力范围，影响决策边界的形状复杂度

## 4. 概率预测

- SVC 原始输出只根据相对于决策边界的位置，来决定分类结果，但实际需求中，分类任务通常需要输出概率，这个时候通常用到 Platt Scaling
  - **Platt Scaling 本质上是在 SVM 的原始决策值输出 $f(x)$ 上训练一个简化版的逻辑回归模型，通过 Sigmoid 函数将距离值映射为概率**
  - 此逻辑回归不直接使用原始特征，而是以 SVM 的输出 $f(x)$ 作为输入，因此是基于模型输出的校准层，而非独立模型
- 校准公式
  $$P(y=1 \mid x) = \frac{1}{1 + \exp(A \cdot f(x) + B)}$$
  - $f(x)$：SVC 对样本 $x$ 的原始决策值
  - $A, B$：$A$ 缩放距离，$B$ 调整偏置，都是通过极大似然估计学习的参数
- 参数学习（ 极大似然估计 ）
  - 训练数据：使用交叉验证生成决策值 $f(x_i)$ 和标签 $y_i$
  - 优化目标：最小化负对数似然（ 带 L2 正则化 ）
    $$\min_{A,B} -\sum_i \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right] + \lambda (A^2 + B^2)$$
    - $p_i = \frac{1}{1 + \exp(A \cdot f(x_i) + B)}$
    - $\lambda$：正则化强度（ 防过拟合 ）

## 5. 多分类问题

#### 5.1 两种策略

- SVC 本质上是二分类器，处理多分类问题需要策略，假设有 K 个类
  - One-vs-Rest（ OvR，一对其余 ）
    - 训练 K 个二分类器（ 每个类对应一个 “正类”，其余为 “负类” ），选择分类器置信度最高的类
  - One-vs-One（ OvO，一对一 ）
    - 为每对类训练一个二分类器，共 K(K-1)/2 个，通过投票决定最终类别。通常更准确但训练开销更大
- Scikit-Learn 的 SVC 类默认使用 OvO，LinearSVC 默认使用 OvR

#### 5.2 策略选择

- 选择 OvR 的场景
  - 类别数较大 (>50)：因为模型数量少
- 选择 OvO 的场景
  - 类别数较小 (≤10)：避免 OvR 的类别不平衡问题，精度通常更高
  - 需要概率预测：预测更稳定，因为数据分布，OvR 是一类对多类，数据分布不均衡
  - 高度不平衡数据：每个二分类器仅用两类数据，天然缓解不平衡
- 总结：**通常除了类别较大，且同时需要考虑模型数量问题的场景，否则优先尝试 OvO**
