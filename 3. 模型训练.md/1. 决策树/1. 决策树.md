## 1. 基础概念

- 决策树（ Decision Tree ）是机器学习中一种经典的分类与回归算法，**该多叉树中每个非叶子节点表示某个特征，而每个分叉路径则代表的某个可能的属性值，每个叶子节点代表预测结果**
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/dd30cf3d-de49-477d-965f-ca25f8cdd9a3)

#### 1.1 算法过程

- 决策树学习的算法 **通常是递归地选择最优特征，并根据该特征对训练数据进行分割**，使得对各个子数据集有一个最好的分类的过程
- 这一过程对应着对特征空间的划分，也对应着决策树的构建
  - 开始，构建根结点，将所有训练数据都放在根结点
  - 选择一个最优特征，按照该特征将训练数据集分割成子集，使得各个子集有着在当前条件下最好的分类
    - 如果这些子集己经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去
    - 如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点
    - 如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止
  - 最后每个子集都被分到叶结点上，即都有了明确的类，这就生成了一棵决策树
- 特征选择在于选取对训练数据具有分类能力的特征，这样可以提髙决策树学习效率，**如何选取 “最优特征” 也就是决策树的关键**

#### 1.2 剪枝算法

- 决策树的问题
  - 决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象
  - **过拟合的原因在于学习时多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树**
- 解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化，通常从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，这个过程称为剪枝（ Pruning ）
- 剪枝主要分为两种情况
  - **预剪枝**
    - 指在决策树的构造过程中，对每个节点在划分前需要根据不同的指标进行估计，如果已经满足对应指标（ 比如树的深度，叶子节点个树，叶子节点样本数 等等 ），则不再进行划分，否则继续划分
    - 由于预剪枝是在构建决策树的同时进行剪枝处理，所以其训练时间开销较少，同时可以有效的降低过拟合的风险。但是，预剪枝有一个问题，会给决策树带来欠拟合的风险
  - **后剪枝**
    - 先根据训练集生成一颗完整的决策树，然后根据相关方法进行剪枝，比如通过验证集判断节点划分前后是否有精度提升
    - **一般后剪枝的决策树泛化能力要优化预剪枝的决策树**，但由于要对非叶子节点逐一计算，所以训练时间要比未剪枝和预剪枝决策树多很多

## 2. 决策树生成算法

- 有 3 中典型的生成算法：ID3，C4.5，CART。其中 ID3 和 C4.5 不是二叉树，而 CART 是二叉树

#### 2.1 ID3

- **核心思想：以信息增益来度量特征选择，选择信息增益最大的特征进行分裂**
- 假如是二分类问题
  - 当 A 类和 B 类各占 50% 的时候，是分类效果最差的状态
  - 当只有 A 类，或只有 B 类的时候，是完全分类的状态
  - 实际情况下，一般介于上面两种情况之间，通常不会最差和最好

#### 2.2 ID3 缺陷

- ID3 过于在乎信息增益，更倾向与取值较多的特征，因为越细小的分割分类错误率越小，比如类似 “编号” 的特征，将每个编号值都单独分类，信息增益直接最大，但是会过度学习训练集，对于新的数据没有意义
- ID3 没有剪枝策略
- ID3 只能处理离散分布特征，不能处理连续分布特征
- ID3 算法无法处理有缺失值的数据

#### 2.3 C4.5

- **以信息增益率作为分裂标准**
  - 信息增益率 = 信息增益 / 该特征的信息熵，一般特征对应取值越多，则该特征的信息熵越大
  - 这样可以解决 ID3 偏向特征取值多的问题，然而增益率又会对取值数目较少的属性有偏好，因此还有一个 “启发式” 的规则：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的
- **进行后剪枝**
  - 采用的悲观剪枝法，从下往上，针对每一个非叶子节点，用 **验证集的数据（ 注意不是训练集 ）** 评估，看该节点划分前和划分后，精度是否有提高，如果划分后精度没有提高，则剪掉此子树，将其替换为叶子节点
- **连续值离散化**
  - 假设 n 个样本的连续特征 A 有 m 个取值，则将其排序并取每每相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点
- **添加缺失值处理**
  - 进行节点分裂时，对于具有缺失值特征，用没有缺失的样本子集所占比重来折算
  - 划分特征属于哪个节点时，将样本同时划分到所有子节点，不过要调整样本的权重值，本质也就是以不同概率划分到不同节点中

#### 2.4 C4.5 缺陷

- 剪枝策略可以优化
- C4.5 仍是多叉树，效率不够高
- C4.5 的信息增益率计算复杂
- C4.5 离散化数值特征时，需要提前排序，需要先放在内存中再排序，训练集大时，内存压力大
- C4.5 仍只能用于分类问题，不能用于回归问题

#### 2.5 CART（ Classification and Regression Tree，分类回归树 ）

- **采用 最小代价复杂度剪枝 替代 悲观剪枝**
  - 从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树
- **分裂过程只能二分叉**
  - 分裂过程是一个二叉递归划分过程，二分叉的运算速度快于多叉树，且 CART 没有停止准则，会一直生长下去
- **基于 基尼指数 进行分裂**
  - 熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点
  - 基尼指数反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此基尼指数越小，则数据集纯度越高，特征越好
- **连续值离散化优化**
  - 对于连续值的处理，CART 分类树采用基尼系数的大小来度量特征的各个划分点
- **可以用于回归问题**
  - CART 区分分类树和回归树，回归树的建立算法和分类树相似，主要是决策树建立后做预测的方式
  - CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而 CART 回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果

## 3. 集成决策树

- **通常单个决策树是弱学习器，容易出现欠拟合和过拟合问题**，因为它们倾向于严格拟合训练数据中的所有样本，但是当我们将一组决策树组成集成为一个整体时，预测结果就会更加准确
- 常见的集成方法有 bagging，boosting 和 stacking，其中 bagging，boosting 通常考虑的是同质弱学习器，而 stacking 通常考虑的是异质弱学习器，所以 **bagging 与 boosting 都非常适合集成多个决策树**

#### 3.1 bagging 方法

- bagging 方法的衍生有：随机森林，极端随机森林，孤立森林
- **bagging 核心思路非常简单，通过训练集和特征（ 可能采样 ）生成多个决策树，最后将多棵决策树的结果进行 平均 或 投票 等操作**

#### 3.2 boosting 方法

- boosting 方法的衍生有：XGBoost，CatBoost，LightBGM，HistGradientBoosting
- boosting 分为两类
  - Adaboost：每轮训练后调整样本权重，使后续模型更关注被误分类的样本
  - GBDT：**每轮训练拟合前序模型的残差**
- 上面举例的四种算法都属于 GBDT 框架的实现
