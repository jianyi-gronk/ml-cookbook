- 在 kaggle 比赛，三大 GBDT（ XGB、LGBM、CATB ）似乎比其他 GBDT 表现得更好

## 1. XGBoost（ eXtreme Gradient Boosting ）

#### 1.1 核心原理

- 基于梯度提升算法，其核心思想是 **在每一轮迭代中，XGBoost 都会训练一个新的决策树来拟合前一轮迭代的预测结果与真实值之间的残差**（ 所以 n 轮训练就有 n 棵决策树 ），并 **将这些弱学习器组合（ 通过累加 ）成一个强学习器**
- 决策树累加的时候 **通过学习率 η 控制每一轮迭代中每棵决策树的权重**，是一个小于 1 的正数，较小的学习率可以使模型更加稳定，但会增加训练时间
- 用数学公式表示累加：
  ![image](https://github.com/user-attachments/assets/f70fa577-a0e2-45d8-a592-7e73cb6760ee)

#### 1.2 决策树生长策略

- XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合
  ![image](https://github.com/user-attachments/assets/38d299c3-c9d2-44ec-bdc6-33428f766c06)
- 但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销

#### 1.3 具体步骤

- 举例有 3 个样本（ x1, y1 ），（ x2, y2 ），（ x3, y3 ），要进行 3 轮迭代训练 3 棵决策树
- 第 0 轮：
  ![image](https://github.com/user-attachments/assets/cb4d543a-8fbf-455c-9e1f-911087fa2d1d)
- 第 1 轮：
  ![image](https://github.com/user-attachments/assets/5d27161e-5166-4f7b-a3ae-8bb24fe67e6d)
- 第 2 轮：
  ![image](https://github.com/user-attachments/assets/b8ebef4b-5c72-4aff-8f02-2cbbaa4edd20)
- 第 3 轮：
  ![image](https://github.com/user-attachments/assets/54546215-0dbb-4057-b1d0-b9073f8e0285)

#### 1.4 参数

- XGBoost 并不在 scikit-learn 库中，需要安装 xgboost 库，xgboost 的参数分成三部分
  - General Parameters，即与整个模型属基调相关的参数
  - Booster Parameters，即与单颗树生成有关的参数
  - Learning Task Parameters，即与模型调优相关的参数
- General Parameters
  - booster：基学习器类型，有两种选择，分别是 gbtree（ 树模型 ）和 linear models（ 线性模型 ），默认是 gbtree
  - silent：控制迭代日志的是否输出，0 指输出，1 指不输出，默认是 0
  - nthread：默认值为最大可能的线程数，与 sklearn 中 n_jobs 的含义相似，默认使用 CPU 全部的核
- Booster Parameters
  - eta：学习率，与 sklearn 中的 learning_rate 相似，默认是 0.3
  - min_child_weight：与 sklearn 的 min_samples_leaf 相似，只不过这里不是样本数，而是权重值，如果样本的权重都是 1，这两个参数是等同的，默认是 1
  - max_depth：与 sklearn 中的 max_depth 相似
  - max_leaf_nodes：与 sklearn 中的 max_leaf_nodes 相似
  - gamma：用来比较每次节点分裂带来的收益，有效控制节点的过度分裂，默认为 0
  - subsample：样本的采样，与 sklearn 中的 max_samples 相似，默认为 1
  - colsample_bytree：特征的采样，与 sklearn 中的 max_features 相似，默认为 1
  - colsample_bylevel：用来控制树的每一节点的分裂，对列数的采样的占比，和 colsample_bytree 类似
  - alpha：权重的 L1 正则化项
  - lambda：权重的 L2 正则化项
  - scale_pos_weight：用于调节正负样本的权重，通常设置为负样本量与正样本量之比
- Learning Task Parameters
  - objective：定义需要被最小化的损失函数，默认为 reg:linear
  - eval_metric：对于有效数据的度量方法，默认值取决于 objective 参数的取值
  - seed：随机种子，与 sklearn 中的 random_state 相似，默认为 0

## 2. CatBoost

- 核心思路和 XGBoost 一样，也是基于梯度提升，通过迭代地训练一系列弱学习器来构建一个强学习器

#### 2.1 相比于 XGBoost 的优化

- 相比于 XGBoost 优化点主要在 分类特征处理，过拟合控制，特征组合，数据噪声处理
- **分类特征处理**
  - XGBoost 不能直接处理分类特征，需要手动对分类特征进行编码，如独热编码、标签编码等。当分类特征的类别数量较多时，独热编码会导致特征维度大幅增加，增加计算复杂度和内存消耗
  - CatBoost 可以直接处理分类特征，它使用了一种名为 “有序目标统计” 的方法，能自动将分类特征转换为数值特征，无需额外的特征工程步骤
- **过拟合控制**
  - XGBoost 在计算梯度时会使用所有样本的信息，这可能会导致模型对训练数据过拟合
  - CatBoost 引入了有序提升技术，在训练每棵树时，随机打乱样本顺序，并使用样本的前缀子集来计算梯度和构建树。这种方法可以有效减少模型对训练数据的过拟合，提高模型的泛化能力
- **特征组合**
  - XGBoost 虽然也能捕捉特征之间的交互，但通常需要手动进行特征工程来实现
  - CatBoost 会自动考虑特征之间的组合，能够发现一些手动难以发现的特征交互关系。这种自动特征组合的方式可以在不增加太多计算成本的情况下，提高模型的表达能力和预测性能
- **数据噪声处理**
  - CatBoost 对数据中的噪声和异常值具有更好的鲁棒性。其有序提升和目标统计方法使得模型在训练过程中对异常样本的敏感度降低，能够更好地学习到数据的内在规律，减少噪声对模型性能的影响

#### 2.2 参数

- CatBoost 也并不在 scikit-learn 库中，需要安装 catboost 库
- CatBoost 的参数太多，需要自己去找官方参数介绍

## 3. LightGBM

- 和 XGBoost 和 CatBoost 的主要不同点有两个
  - **LightGBM 对连续值进行划分节点的时候，并不是通过排序后遍历，也不是通过基尼系数大小，而是基于 Histogram（ 直方图 ）算法**
  - **LightGBM 抛弃了大多数 GBDT 工具使用的 level-wise 生长策略，而使用了 leaf-wise 算法**
  - **采样阶段 使用单边梯度采样（ Gradient-based One-Side Sampling，GOSS ）算法**

#### 3.1 Histogram 算法

- 直方图算法的基本思想是，先把 N 个样本的连续的浮点特征值离散化成 k 个整数，同时构造一个宽度为 k 的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点
  ![image](https://github.com/user-attachments/assets/d5efbd8f-d7d8-48df-aeb1-95c6ea0ba51e)
- 这样相比于排序后遍历计算分割点，计算分裂增益的次数从 O(N) 变成了 O(k)，同时避免了 O(logn) 的排序，内存消耗也从 N 变成了 k，但选取的分割点可能不是最精确的
- 在 准确度，内存消耗，效率 上在 排序后遍历 和 基尼系数 之间，相当于做出了取舍

#### 3.2 leaf-wise 算法

- LightGBM 采用 Leaf-wise 的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环
  ![image](https://github.com/user-attachments/assets/e5a8b384-8506-4559-b58f-18211e405e79)
- 同 Level-wise 相比
  - Leaf-wise 的优点是：在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度
  - Leaf-wise 的缺点是：可能会长出比较深的决策树，产生过拟合。因此 LightGBM 会在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合

#### 3.3 单边梯度采样算法

- 在 GBDT 中没有原始样本权重，不能应用权重采样，但样本存在梯度的高低，可以帮助采样样本，根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响
- 因此 GOSS 在采样的时候，排除大部分小梯度的样本，仅用剩下的高梯度的样本计算信息增益，精度会略有损耗，但是减少了内存消耗和提升了模型训练速度

#### 3.4 参数

- LightGBM 也并不在 scikit-learn 库中，需要安装 lightgbm 库
- LightGBM 的参数也太多，需要自己去找官方参数介绍

## 4. HistGradientBoosting

- scikit-learn 库有提供，但是没有特殊优势，且存在不自动支持类别特征，决策树生长策略是 Level-wise，不支持 GPU 等等问题
- 存在 categorical_features 参数，是基于直方图的梯度提升模型的重要参数，用于指定数据集中哪些是类别特征
  - 默认为 None，表示模型不会将数据集中的任何特征视为类别特征，所有特征都会被当作连续数值特征进行处理
  - 但假设数据有 5 个特征，第 1 和第 3 个特征是类别特征
    - 可以通过布尔类型数组设置，categorical_features = [True, False, True, False, False]
    - 可以通过整数类型数组，categorical_features = [0, 2]
    - 可以通过字符串类型数组，categorical_features = ['feature0'，'feature2']
    - 可以设置成 "from_dtype"，模型会自动识别输入数据中数据类型为 category 的列作为类别特征。不过，使用这个选项时，输入数据必须是支持 dataframe 方法的对象，如 Pandas 或 Polars 的 DataFrame

```python
(
  loss='log_loss',
  learning_rate=0.1,
  max_iter=100,
  max_leaf_nodes=31,
  max_depth=None,
  min_samples_leaf=20,
  l2_regularization=0.0,
  max_features=1.0,
  max_bins=255,
  categorical_features='from_dtype',
  monotonic_cst=None,
  interaction_cst=None,
  warm_start=False,
  early_stopping='auto',
  scoring='loss',
  validation_fraction=0.1,
  n_iter_no_change=10,
  tol=1e-07,
  verbose=0,
  random_state=None,
  class_weight=None
)
```

## 5. 四个模型对比

| **特性**         | **XGBoost**                  | **CatBoost**                 | **LightGBM**                   | **HistGradientBoosting** |
| ---------------- | ---------------------------- | ---------------------------- | ------------------------------ | ------------------------ |
| **算法基础**     | 预排序                       | 对称树 + 有序目标编码        | 直方图 + GOSS 采样             | 直方图优化               |
| **类别特征支持** | 需手动编码                   | 直接支持                     | 直接支持                       | 需手动编码               |
| **缺失值处理**   | 自动学习分裂方向             | 自动处理                     | 自动处理                       | 自动处理                 |
| **树生长策略**   | Level-wise                   | Symmetric Tree（对称结构）   | Leaf-wise                      | Level-wise               |
| **并行化支持**   | CPU/GPU                      | CPU/GPU                      | CPU/GPU                        | CPU 多线程               |
| **内存效率**     | 较低（ 预排序 ）             | 中等（ 对称树结构 ）         | 极高（ 直方图 + 并行优化 ）    | 高（ 分箱压缩数据 ）     |
| **训练速度**     | 较慢（ 大数据时资源消耗高 ） | 快（ 类别特征多时优势明显 ） | 极快（ 支持百万级数据 ）       | 快（ 中小数据 ）         |
| **正则化方法**   | L1/L2 正则化 + 树复杂度惩罚  | L2 正则化 + 有序提升         | L1/L2 正则化 + GOSS 采样       | L2 正则化 + 早停         |
| **主要优势**     | 高精度，高灵活性             | 类别特征鲁棒性               | 有效处理大规模数据和高维度特征 | Scikit-Learn 生态集成    |
| **缺点**         | 内存消耗大，大数据处理速度慢 | 训练速度相对较慢（ 对称树 ） | Leaf-wise 可能过拟合           | 不支持 GPU               |

- 总结：**大部分情况下 LightGBM 更适用**
