## 1. 早停法（ Early Stopping ）

- 神经网络训练多少轮是一个很关键的问题
  - 如果 epoch 数量太少，则有可能发生欠拟合
  - 如果 epoch 数量太多，则有可能发生过拟合
- 早停法的本质是在平均验证损失值不再改善时，要等待多少个 epoch 才停止训练，即连续多少轮次的平均验证损失值都小于最低验证损失就停止训练

## 2. 标签平滑（ Label smoothing ）

#### 2.1 使用场景和时机

- 使用场景
  - 标签平滑通常用于 **分类问题**，目的是 **防止模型在训练时过于自信地预测标签，改善泛化能力差的问题**
- 使用时机
  - 标签平滑 **只用在训练过程中**（ 训练前预处理 和 训练后评估模型 的时候都不会用到 ），具体发生在模型前向传播之后、损失计算阶段，属于模型训练过程中的正则化技术（ 或损失函数的改进方法 ）
- 对于分类问题，通常认为训练数据中标签向量的目标类别概率应为 1，非目标类别概率应为 0，传统的标签向量 $y_i$ 为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/95300e1e-2dd0-41a5-916a-695480610062)
- 而标签平滑结合了均匀分布，修改了 $y_i$ 公式，其中 K 为多分类的类别总个数，α 是一个较小的超参数（ 一般取 0.1 ），即
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0109b202-9c35-4cd1-9fe6-87a6fe086795)

## 3. 注意 CPU 与 GPU 之间的数据传输

- 通过 tensor.cpu() 可以将张量从 GPU 传输到 CPU，反之使用 tensor.cuda()，但这样的数据转化代价较高。.item() 和 .numpy() 的使用也是如此，建议使用 .detach()
- 如果要创建新的张量，使用关键字参数 device=torch.device（'cuda：0'）将其直接分配给 GPU
- 最好使用 .to(non_blocking=True) 传输数据，确保传输后没有任何同步点即可

## 4. 垃圾回收

- python 也是使用分代回收策略，总共分了 3 代，默认触发垃圾回收时机如下

  ```python
  import gc

  print "Garbage collection thresholds: %r" % gc.get_threshold()
  # Garbage collection thresholds: (700, 10, 10)
  # 700：表示当分配对象的个数达到 700 时，进行一次 0 代回收
  # 10：当进行 10 次 0 代回收以后触发一次 1 代回收
  # 10：当进行 10 次 1 代回收以后触发一次 2 代回收

  # 上面这个是默认的回收策略的阈值
  # 也可以自己设置回收策略的阈值
  gc.set_threshold(500, 5, 5)
  ```

- 由于自动垃圾收集高度重视空闲对象的数量，而不是它们的大小，因此当代码中释放占据大量内存的对象时，是运行手动垃圾收集的良好时机（ 注意不要频繁调用，会浪费性能 ）

  ```python
  del train_val

  gc.collect()
  ```

## 5. 其他建议

- 在整个数据集上训练之前，先在非常小的子数据集上训练进行过拟合，这样可以知道你的网络是否可以收敛
- 避免 LRN 池化，MAX 池化会更快
- GPU 上报错时尽量放在 CPU 上重跑，错误信息更友好
- 一般来说，越大的 batch-size 使用越大的学习率。原理很简单，越大的 batch-size 意味着我们学习的时候，收敛方向的 confidence 越大，我们前进的方向更加坚定，而小的 batch-size 则显得比较杂乱，毫无规律性，因为相比批次大的时候，批次小的情况下无法照顾到更多的情况，所以需要小的学习率来保证不至于出错
- 卷积层通常使用金字塔状特征提取，比如 [64, 128, 256, 128, 64]，而全连接层，和 RNN 层通常从大到小的形状，比如 [256, 128, 64]
- 如果保存下来的模型，在新的训练过程中，其他代码没动，但发现 r2-score 相比于上次训练下降了不少，这种通常是因为学习率的问题，因为在上一轮的训练中，学习率已经被训练成一个适合的值，而重新训练的时候，又从默认值开始调，因此应该记住上次训练后的学习率的值，并在下一轮训练前改动学习率
- 一般顺序是，全连接后先 BN 层，再激活函数，再 dropout 层

## 6. 迁移学习

- 迁移学习专注于存储已有问题的解决模型，并将其利用在其他不同但相关问题上。比如说，用来辨识汽车的模型也可以被用来提升识别卡车的能力
- 一般图片识别问题的模型可以用在其他图片识别上，但不适合用在音频识别问题上
- 迁移学习步骤
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/744fedee-b1ce-4d08-85fc-03af2eb239e1)
  - 假如现在想训练一个识别手写数字 0，1，2 ... 9 的模型，但是标记数据集非常小，可以先用有 100 万张猫，狗，车，人等一千个类别的图片去训练神经网络（ 也叫监督预训练 ）
  - 然后将最后的输出层进行替换，因为之前是 1000 中类别的输出，现在是 10 中类别的输出
  - 接下来对模型进行微调，有两种策略
    - 如果手写数字数据集非常小，则只训练输出层参数，隐藏层参数不变
    - 如果手写数字数据集一般小，则训练输出层和隐藏层的所有参数

## 7. 增量学习（ Incremental Learning ）

- 增量学习亦称为持续学习（ Continual Learning ）或终身学习（ Life-Long Learning ）
- 增量学习是一种动态的机器学习方法，它允许模型通过对新数据进行持续学习而不是重头训练整个模型。这种方法允许模型不断地学习新的知识，并在不断实际复杂多变的环境变化
- 增量学习的优点是可以随时训练新数据，不需要保留大量训练数据，因此存储和计算开销都比较小，同时还能够有效避免用户的隐私泄露问题，这在移动边缘计算的场景下是非常有价值有意义的
- 对于满足以下条件的学习方法可以定义为增量学习方法：
  - 可以学习新的信息中的有用信息
  - **不使用旧数据或者少使用原始数据**
  - 对已经学习的知识具有记忆功能
  - 在面对新数据中包含的新类别时，可以有效地进行处理
- 增量学习方法的种类有很多种划分方式，大体可以将其划分为以下三种范式：
  - 正则化（ regularization ）：**不使用旧数据**
  - 回放（ replay ）：**少使用旧数据**（ 选一部分作为旧数据的代表 ）
  - 参数隔离（ parameter isolation ）
  - 其中基于正则化和回放的增量学习范式受到的关注更多，也更接近增量学习的真实目标，参数隔离范式需要引入较多的参数和计算量，因此通常只能用于较简单的任务增量学习

#### 7.1 基于正则化的增量学习

- 基本思想就是：**不但在学习新的知识，而且一定程度上保护旧知识**
- 模型是通过旧数据训练得到，所以模型经过新数据后会得到损失，因此希望在损失中再加一些项，希望其可以反应旧数据在当前模型上的表现
- 举例
  - 以多任务学习为例，任务就是要学习一个图片分类器，比如之前学的都是猫的图片，现在给了狗的图片，也要学
  - 通常会这么做，前面那些几十层是用来提取图片通用特征的（ 猫狗都共享 ），后面那些一两层是任务特定的（ 猫，狗各自都有自己的层 ），有点像预训练模型
  - 这个时候我们有两种选择：
    - 根据新数据微调，那么会改变共享参数，这个时候旧数据的表现可能会变得很差
    - 特征抽取，保持共享参数不变，根据新数据只训练新任务的那几个特定层，但这样也不好，因为之前的共享参数未必学得适用于所有任务，所以单靠后面几层难以学习一个很好的分类器

#### 7.2 LwF（ Learning Without Forgetting ）

- LwF 算法是基于深度学习的增量学习的里程碑之作，这个有点像微调，但是思想上比微调更加丰富，它通过知识蒸馏防止灾难性遗忘
- 算法如下
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/8a850f1f-d398-44bf-9d89-114c9a13b9cb)
- 计算步骤为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ffe2a674-083c-4cb7-9615-f8b392fbb0b7)
- 相当于引入了新数据在旧模型上的结果 $Yo$，作为旧模型的回忆，希望新数据训练完毕之后，这个 $Yo$ 在新模型上也没有怎么变化，从而希望旧数据输入新模型得到的结果也没有怎么变化

#### 7.3 基于回放的增量学习

- 基于回放的增量学习的基本思想就是 "温故而知新"，**在训练新任务时，一部分具有代表性的旧数据会被保留并用于模型复习曾经学到的旧知识**，因此要保留旧任务的哪部分数据，以及如何利用旧数据与新数据一起训练模型，就是这类方法需要考虑的主要问题
- iCaRL 是最经典的基于回放的增量学习模型，iCaRL 假设越靠近旧数据均值的样本越有代表性
