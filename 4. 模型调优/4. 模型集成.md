## 1. 集成学习（ Ensemble Learning ）

- 如果模型不知道如何再优化，可以试试 ensemble，拼接多个现有模型
- 集成学习也称为模型融合，不同于传统的机器学习方法在训练集上构建一个模型，集成学习通过构建并融合多个模型来完成学习任务
- 集成学习的主要方法可归类为：
  - 投票 / 平均（ Voting / Averaging ）
  - 装袋（ Bootstrap aggregating，即 Bagging ）
  - 提升（ Boosting ）
  - 堆叠（ Stacking ）
  - 混合（ Blending ）

## 2. Voting / Averaging

- 核心思想
  - 直接对多个不同模型的预测结果进行加权投票（ 分类 ）或加权平均（ 回归 ）
  - 适用于 **异质基模型**（ 如组合 决策树、SVM、逻辑回归 等不同算法模型 ），不同模型之间需具备一定的 **多样性**（ 如不同偏差-方差特性 ），避免所有模型犯相同错误
- 优点：**简单高效**，适合快速提升模型 **鲁棒性**
- 集成建议
  - **建议选择 3 ～ 5 个高精度模型作为核心（ 赋予高权重 ），其他模型作为补充（ 低权重 ）**
  - 权重大小可以通过优化算法进行选择，而非仅凭经验选择
  - 模型数量并非 “越多越好”，需验证补充模型是否带来正向增益，避免引入噪声

## 3. Bagging（ Boostrap Aggregating ）

- 核心思想
  - 和 Voting / Averaging 大致相似，都是对多个基模型的预测结果进行 投票 或 平均
  - 不同在于，**Bagging 基模型是同质的**，不同模型之间的多样性不来自不同的模型结构，而是来自 **自助采样** 的 数据扰动
  - 标准 Bagging 仅通过样本自助采样，但是实际实现中可能还包括特征的采样
- 算法步骤
  - 自助采样：从训练集中有放回地随机抽取 n 个样本（ 允许重复 ），生成 M 个子训练集
  - 基模型训练：每个子训练集独立训练一个基模型（ 如决策树 ）
  - 结果聚合：根据基模型预测结果进行 投票 或 平均
- 典型代表有 随机森林 和 极端随机森林

## 4. Boosting

#### 4.1 基础概念

- 核心思想
  - 通过 **顺序训练** 多个 **同质弱学习器**，每个模型 **专注于修正前序模型的错误**，最终通过加权组合提升整体性能
- 其中分为两类
  - Adaboost：**每轮训练后调整样本权重，使后续模型更关注被误分类的样本**
  - GBDT：**每轮训练拟合前序模型的残差**

#### 4.2 Adaboost（ 自适应增强 ）

- 核心流程
  - 初始时赋予所有样本相同权重
  - 迭代训练弱学习器，每次迭代后 **增加误分类样本的权重**，使后续模型更关注这些样本
  - 最终通过 **加权组合** 所有弱学习器的预测结果
- 关键点
  - 样本权重更新：误分类样本的权重增加，正确分类样本的权重减少
  - 模型权重计算：误差率越低的弱学习器，权重系数越高
  - 贪心策略：每一步优化当前局部最优解，而非全局最优

#### 4.3 GBDT（ Gradient Boosting Decision Tree，梯度提升决策树 ）

- 是一种迭代的决策树算法，又叫 MART（ Multiple Additive Regression Tree ）
- 算法步骤
  - 通过迭代拟合前几轮 **预测值累加值 与 真实值 的负梯度**（ 比如当损失函数为均方误差 MSE 时，负梯度等于残差 ），逐步优化预测结果
  - 最终预测值为所有基模型的 **预测值的累加**
- 以通俗的例子解释
  - 假设预测人类年龄，初始预测 20 岁 → 残差 10 岁 → 第二棵树预测 6 岁 → 残差 4 岁 → 第三棵树预测 3 岁 → 残差 1 岁，依此类推
- 基学习器细节
  - GBDT 的基学习器限定了 **只能使用 CART 回归树模型**，因为 GBDT 需要拟合损失函数的负梯度（ 连续值 ），而分类树的输出无法直接适配这一需求。尽管 GBDT 通过调整输出转换可以用于分类任务，但其基学习器始终是回归树
  - GBDT 对基学习器的要求一般是足够简单，并且是低方差和高偏差的，因为训练的过程是通过降低偏差来不断提高最终精度

## 5. Stacking

#### 5.1 基础概念

- 核心思想：Stacking 是一种分层模型融合方法，通过两阶段训练结合多个基模型的预测能力
  - 第一层（ 基模型层 ）：
    - 通常训练多个异质基学习器（ 如 SVM、决策树、神经网络 等 ），但也可以使用同质
    - **交叉验证 是 核心**，需要通过交叉验证去有效的利用训练集去生成足够多的预测结果去让元模型去训练
  - 第二层（ 元模型层 ）：
    - 将基模型的输出作为输入特征，训练一个元模型（ 如 线性回归、逻辑回归 或 复杂模型 ）生成最终预测

#### 5.2 算法步骤

- 假设现在有 12500 条数据，现在拆分为 10000 条训练数据，2500 条验证数据 D1，3 个基模型，用 5 折交叉验证法
- 每个基模型进行预测
  - 进行交叉验证，每一折验证时，将 10000 条训练数据中的 8000 条作为当前折的训练集，剩下的 2000 作为验证集 D2
  - 每次验证时，都能分别得到 D1 和 D2 的预测结果，最终 5 次交叉验证之后，能得到 5 \* 2500 的 D1 预测结果和 5 \* 2000 的 D1 预测结果
  - 5 个 D1 预测结果可以水平通过平均拼接成 2500 条验证集对应的预测结果，而 5 个 D2 的预测结果可以垂直拼接成 10000 条训练数据完整的对应预测结果
- 训练元模型
  - 3 \* 10000 的预测结果作为训练集，3 \* 2500 的预测结果作为验证集，输入给元模型进行训练

![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/155e1ffc-52f1-4315-93af-abb8b595a72e)
![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/bba8927f-807e-45ec-bb26-96f8ff50122f)

## 6. Stacking 实战

- 接下来的举例以 rf，extra，xgb，hgb，ctb，lgb 六个模型的拼接为例，争取找到最佳的权重组合（ 需要添加交叉验证，但是防止代码复杂，这里省略 ）

```python
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor

# 省略 train_X，train_y，valid_X，valid_y 定义

# 训练六个基础模型
models = [
  RandomForestRegressor(random_state=42),
  ExtraTreesRegressor(random_state=42),
  XGBRegressor(random_state=42),
  HistGradientBoostingRegressor(random_state=42),
  CatBoostRegressor(random_state=42, verbose=0),
  LGBMRegressor(random_state=42)
]

for model in models:
  model.fit(train_X, train_y)

```

#### 6.1 通过优化算法

- 核心是通过多次尝试加权组合，通过梯度不断调节，最终找到最佳组合

```python
from scipy.optimize import minimize
from sklearn.metrics import mean_squared_error

# 定义目标函数，用于优化权重
def objective(weights):
    weighted_preds = np.sum([w * model.predict(valid_X) for w, model in zip(weights, models)], axis=0)
    mse = mean_squared_error(valid_y, weighted_preds)
    return mse

# 初始权重猜测
initial_weights = np.ones(len(models)) / len(models)

# 约束条件：权重之和为 1 且非负
constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
bounds = [(0, 1) for _ in range(len(models))]

# 使用优化算法求解最优权重
result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)
optimal_weights = result.x

print("最优权重:", optimal_weights)
```

#### 6.2 通过线性回归

- 核心是通过用六个模型的预测作为输入，训练各个模型的权重
- 这个方法相比于 通过优化算法，对 **资源要求不高**，不需要多次迭代尝试，但是仅支持单个模型权重和拼接后模型得分是线性关系的情况，所以 **当存在非线性关系时，可能预测不准**

```python
from sklearn.linear_model import LinearRegression

# 获取六个模型在验证集上的预测结果
val_preds = np.array([model.predict(valid_X) for model in models]).T

# 训练线性回归模型来学习权重
linear_model = LinearRegression(positive=True)
linear_model.fit(val_preds, valid_y)

# 获取权重
weights = linear_model.coef_
weights = weights / np.sum(weights)  # 归一化权重

print("最优权重:", weights)
```

#### 6.3 通过 MLP 神经网络

- 核心和线性回归相似，只不过把模型替换成了神经网络
- 这个方法相比于 通过线性回归，**支持存在非线性关系的情况**，但是复杂度较高

```python
from sklearn.neural_network import MLPRegressor

# 获取六个模型在验证集上的预测结果
val_preds = np.array([model.predict(valid_X) for model in models]).T

# 训练神经网络来学习权重
meta_model = MLPRegressor(random_state=42)
meta_model.fit(val_preds, y_train)

# 最终预测
# final_preds = meta_model.predict(test_preds)
```

## 7. Blending

- 核心思想和 Stacking 和 Stacking 类似，但是不使用交叉验证，而是 **直接将训练集划分两部分，一部分训练基模型，一部分生成元模型的特征**，这样计算效率更高
