## 1. 题目介绍

- 题目：[预测聚合物的基本特性](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025)
- 数据集：由 7973 条数据，1 个特征组成，包含
  - SMILES：将 聚合物结构 的分子结构（ 原子、键、环、分支 ）编码为文本
- 目标变量
  - Tg：玻璃化转变温度
  - FFV：自由体积分数，训练数据中只有
  - Tc：热导率
  - Density：聚合物密度
  - Rg：回转半径
- 评估：通过 wMAE（ 平均绝对误差 ）评估

## 2. 数据预处理（ 针对所有目标变量 ）

#### 2.1 引入外部数据集

- 训练数据集存在两个较大问题
  - 数据量较少，只有 7973 条
  - 目标变量缺失严重
    - Tg 只有 511 个非缺失值
    - FFV 只有 7030 个非缺失值
    - Tc 只有 737 个非缺失值
    - Density 只有 613 个非缺失值
    - Rg 只有 614 个非缺失值
- 因此，非常需要外部数据集来扩充训练数据并填补缺失值，具体步骤
  - 引入新的数据表格
  - 优先保留训练数据中的目标值，如果训练数据中缺失，才用引入数据填补
  - 将引入数据中独有的 SMILES 添加到训练集
- 最终结果
  - 数据量增长到 11327 条
  - 目标变量缺失
    - Tg 有 8244 个非缺失值
    - FFV 有 7892 个非缺失值
    - Tc 有 866 个非缺失值
    - Density 有 1247 个非缺失值
    - Rg 有 614 个非缺失值

#### 2.2 数据增强

- 由于相同的 SMILES 可以有不同的表示方法
- 因此将 SMILES 规范化后，进行数据增强，每个 SMILES 都用两种不同的方式表示，让模型学到不同的表示方法仍可以代表一种 SMILES

#### 2.2 文本特征转化学描述

- 原始特征只有 SMILES，显然不能直接作为模型输入，因为是 SMILES 是文本特征，高维稀疏向量，训练集中每一个 SMILES 的值都是不同的，模型不能直接通过 SMILES 学习到化学含义
- 对原始 SMILES 的基础信息进行计算，包括
  - 基础统计：分子复杂程度，各个原子的数量，分子支链结构 等
  - 原子比例：各个原子在分子中的比例
  - 官能团相关：双键数量，三键数量，羟基数量，羰基数量 等
  - 分子复杂性：分支比例，环比例 等
- 计算 mordred 描述符 和 rdkit 描述符
- 计算图特征，包括是否连接，最小深度 等
- 计算 Morgan 指纹
- 最终结果
  - 添加了 898 个特征

## 3. 数据预处理（ 针对某一个目标变量 ）

- 因为我们这次的核心思路是 根据不同的 目标变量 进行分别预测，所以针对不同的 目标变量 我们进行不同的数据预处理
- 首先基于每个目标变量，复制一份新的数据集，并只保留 当前目标变量有值 的样本

#### 3.1 特征组合

- ~~特征进行 n 元组合，但是由于数据量较小，就算直接只进行二元组合，特征数量也会过大，因此不能直接获取全部的 n 元组合，需要挑选部分有价值的组合【 没有找到有效方法 】~~
  - ~~使用遗传编程 gplearn 生成特征组合【 生成的特征组合重复过多，效果一般 】~~
  - ~~直接让 gpt 针对每个目标变量，根据特征的化学特性，分析最佳组合，给出指定数量的 n 元最佳组合【 给的组合大多会导致劣化，效果不佳 】~~

#### 3.2 特征过滤

- 需要注意，因为不同目标变量对应的数据量大小不同，因此需要特征过滤的程度也不同
- 删除指定不重要的特征
  - ~~让 gpt 筛选出 从化学分析上来看 最没有影响的 n 个特征，然后将其删除（ gpt 不够智能，不要让其一次性生成太多，可能会误删除重要特征，可以每次让 gpt 删除 3 个，并给出删除理由 ）~~
  - 手动选择对目标变量影响很小的特征
- 删除包含信息较少的特征
  - 过滤 存在空值 的特征，有空值就删除
  - 过滤 最大类别占比过大 的特征（ 即类似常量特征 ），大于 0.9 就删除
  - 过滤 信息熵较低 的特征（ 只针对离散特征 ），小于 0.3 就删除
- 删除与目标变量不相关的特征
  - ~~基于 皮尔逊相关系数【 只能捕捉两个连续特征间的线性关系，同时还对异常值敏感，效果一般 】~~
  - 基于 斯皮尔曼秩相关系数，与目标变量相关小于阈值即删除
    ```python
    target_corr_threshold = {
      'Tg': 0.06,
      'FFV': 0.06,
      'Tc': 0.08,
      'Density': 0.08,
      'Rg': 0.08,
    }
    ```
  - ~~基于 互信息【 效果一般 】~~
  - 基于决策树过滤，最终使用的 ExtraTrees 进行过滤，过滤最不相关的 n 个
    ```python
    feature_filter_threshold = {
      'Tg': 30,
      'FFV': 30,
      'Tc': 60,
      'Density': 60,
      'Rg': 60,
    }
    ```
- 删除与其他特征高度相关的特征
  - ~~基于 皮尔逊相关系数【 只能捕捉两个连续特征间的线性关系，同时还对异常值敏感，效果一般 】~~
  - 基于 斯皮尔曼秩相关系数，特征之间相关性大于阈值即删除
    ```python
    feature_corr_threshold = {
      'Tg': 0.92,
      'FFV': 0.95,
      'Tc': 0.8,
      'Density': 0.8,
      'Rg': 0.8,
    }
    ```
  - ~~基于 互信息【 效果一般，同时较慢 】~~
- 需要注意，最好先进行 “删除与目标变量不相关的特征” 再 “删除与其他特征高度相关的特征”，防止两个相似特征时，考虑删除某一个时，将与目标变量较相关的特征删除

#### 3.3 特征编码

- 对分类特征进行处理
  - 使用目标编码，获取特征与目标变量的关系
    - ~~并保留原始特征【 正常是保留会对模型更有优化，但是因为这次数据量过少，保留的话，会让特征过多 】~~
    - ~~不保留原始特征，直接替换【 会失去原本特征的值大小分布 】~~
    - 部分保留原始特征，部分直接替换（ 小于等于 `replace` 的时候 直接替换，大于 `replace` 小于等于 `reserve` 的时候 保留原始特征 ）
      ```
      target_encoding_config = {
        'Tg': {
          'replace': 3,
          'reserve': 8,
        },
        'FFV': {
          'replace': 3,
          'reserve': 8,
        },
        'Tc': {
          'replace': 3,
          'reserve': 8,
        },
        'Density': {
          'replace': 3,
          'reserve': 8,
        },
        'Rg': {
          'replace': 3,
          'reserve': 8,
        },
      }
      ```
  - ~~使用独热编码，消除分类特征的虚假序关系（ 值类别大于 2 种，小于等于 5 种 ）【 决策树模型不适合，会导致劣化 】~~

## 4. 模型训练

- 拼接的模型包括
  - ~~Ridge~~
  - ~~Lasso~~
  - ~~SVR~~
  - ~~LinearSVR~~
  - RandomForestRegressor
  - ExtraTreesRegressor
  - GradientBoostingRegressor
  - HistGradientBoostingRegressor
  - XGBRegressor
  - CatBoostRegressor
  - LGBMRegressor
  - MLPRegressor
  - TabPFNRegressor
  - ~~KNeighborsRegressor~~
  - ~~RadiusNeighborsRegressor~~
  - ~~autogluon~~
- 训练细节
  - 训练的时候，用 5Fold 通过早停找到最佳迭代次数，并记录这 5 次的迭代次数
  - 最终模型预测的时候，分别用 5 个不同的 seed，用全部数据进行训练，迭代次数修改为 最大迭代次数 \* 1.25
  - 取 最大迭代次数 而不是 平均迭代次数 的原因是，需要考虑最难拟合的部分数据
