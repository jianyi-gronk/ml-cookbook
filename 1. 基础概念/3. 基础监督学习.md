## 1. 线性回归

- 用于解决监督学习的回归问题

#### 1.1 普通线性回归模型

- 模型与数据拟合成一条直线
- 本质就是求解方程，w，b 是模型的参数（ 也可以叫系数，权重，即可以调整的模型变量 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0d648649-4a46-4145-8238-6271ef8f91b1)

#### 1.2 代价函数

- 用于计算误差，多除以 2 是为了方便求导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/93a8cbf7-ac7e-4ea0-b90b-a5b7eee9d57e)

#### 1.3 梯度下降

- 梯度下降适用于拥有两个或两个以上参数的一般函数，可以用来尝试最小化任何函数
- 核心步骤（ 以线性回归的代价方程为例 ）
  1. 从一个初始 w，b 开始
  2. 不断的修改 w，b，去减少 J(w, b)
  3. 知道 J 到达最小值（ 局部最小值，不是全局最小值，应此可能有多个最小值 ）
- 梯度下降公式
  - 正确的，α 为学习率，通常是 0 到 1 之间的较小正数（ 例 0.01 ），控制修改 w，b 的幅度，剩下的部分为求偏导
    ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/9e146a07-8cb5-4782-b409-9480dab5eb90)
  - 错误的，不能用修改后的 w 去计算
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a5179d47-d49e-4d76-a5f7-5624fc17a32a)
- 梯度下降公式中求导公式推导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/612b5384-2789-4186-9fc2-11609406f7e8)
- 如图
  ![image.png](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/332495c4-ccfb-4848-84bf-368dfbf9b83e)
- 梯度下降方法分为 Batch gradient descent 和 Mini-batch gradient descent
  - Batch gradient descent 代表每次迭代训练所有样本
  - Mini-batch gradient descent 代表每次迭代训练部分样本，通常样本数设置为 2 的幂次方，通常设置 2，4，8，16，32，64，128，256，512（ 设置成 2 的幂次方，更有利于 GPU 加速，并且很少设置大于 512 ）
- 在线性回归中，有且只有一个最小值，成碗状图
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/109b13d7-41fd-4f96-b6e2-f05fec3953c8)
- 当随着迭代次数增加，代价函数结果趋于稳定，则梯度下降收敛，可通过看曲线图或者判断代价函数结果是否小于阈值
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/896033c0-f761-4feb-8298-cb3a6e7fc598)

#### 1.4 学习率

- 当随着迭代次数增加，代价函数结果变化异常，需判断是否因为代码异常或者学习率过大，可以通过将学习率设置成非常小的值，如果还不是下降趋势，那么就是代码问题
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a9b2b4e4-1e76-420d-9bd0-4e05b0dfca8e)
- 学习率的选择，可以从低到高尝试，0.001，0.01，0.1

#### 1.5 多元线性回归

- 多维线性回归公式
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/07eb0531-04bb-4234-b56a-98811d1aa418)
- 常用符号
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/5d8b40a0-9472-4ebc-80d2-a97b2ea8b559)
- 公式还可以写成（ 向量点积 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/937dcba2-bd18-45fc-b1d5-86d42b0479ab)
  - 转换成代码
    ```
    w = np.array([1.0, 2.5, -3.3])
    x = np.array([10, 20, 30])
    b = 4
    f = np.dot(w, x) + b;
    ```
  - 并且这种向量化计算，性能更好，无论是否使用 GPU。如果不使用向量计算，则是一个一个计算后累加，如果使用向量计算，计算机会在一步中并行计算每对 (w, x) 乘积，并将每对计算结果通过专门的硬件进行高效累加
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e7bc0c60-c536-4570-9999-08c89ab7d7a9)
- 梯度下降
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/fc2c992a-0227-46ee-a8ab-f760d3ef50dd)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c693468b-0d16-48af-810d-3d6160ac97bb)

## 2. 多项式回归

- 如果数据不是直线，而是曲线，那么可能多项式回归的曲线更能符合数据集
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/2f52c6b7-1064-44f6-bba4-626aac6cbc97)
- 这个时候通常特征缩放非常重要，因为 x 的范围是 1-1000 的话，x 的三次方范围就是 1-1 亿

## 3. 逻辑回归

- 虽然名字里带有回归，但是是为了解决分类问题
- 对于分类问题，线性回归通常表现都不好
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e2163523-8350-44bb-a233-d547baae8124)
- 常用函数 Sigmoid 函数，也叫 Logistic 函数，取值范围为 (0,1)，它可以将一个实数映射到 (0,1) 的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好，优点是平滑，易于求导
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/923d6f1c-158a-4da6-a265-e6f9f18656e2)

#### 3.1 逻辑回归公式

- 完整公式，一般大于 0.5 就认为是 1（ 即 z 大于 0 ），小于 0.5 就认为是 0（ 即 z 小于 0 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/7b9f73ad-67d8-40d1-a105-571b5b955cb1)
- 也可以写成，意思是 w，b 是影响 P(y = 1)（ y 等于 1 的概率 ） 的参数，x 是输入的特征
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/cddc9709-f2e3-4251-8d9a-262348433ff2)

#### 3.2 决策边界

- z = 0 且划分两个集合，分别对应两个分类的线，被称为决策边界
- 比如，决策边界右边是 y = 1 的集合，左边是 y = 0 的集合
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/1e7c9b42-0002-4ff2-9133-c74082785c65)
- 也有可能是曲线
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/37bb2ac6-d0f8-47e8-8dec-6a186b2cf923)
- 也有可能是更复杂的不规则曲线
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/10ae28bb-a268-4c49-b4a4-6c83be9e8d73)

#### 3.3 代价函数

- 不能和线性回归使用相同的代价函数，得到的会是非凸函数，有很多局部最小值影响
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/31cae5eb-ca20-4348-bbc8-1ee4f7115e80)
- 因此需要将计算误差的公式进行替换，根据真实标签的值不同，Cost 公式也不同
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/47b30024-306f-436a-81d0-754394b7b83a)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3de79591-7591-4737-bae9-cface535b4c8)
- 最终函数的曲线为（ 注意 f 的取值在 0 - 1 区间，且为 -log ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4283f93d-63d5-4a7f-ac8a-4d391d6590b3)
- 真实标签为 1 时，f = 1 时，接近真实标签，那么损失很小，f 接近 0 时，离真实标签相差大，那么损失大
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/7ee51183-4510-4be8-8146-baba5bf70f39)
- 真实标签为 0 时同理
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/f1f5be35-f517-4a13-9e08-dbe9720734a0)
- 简化误差函数（ 即缩略成一行，方便计算 ）
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/fb7efbb6-d706-4ee2-adf9-e12dce085742)
- 最终逻辑回归的代价函数为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ae3ec934-554f-450e-9339-5bdec2d2959a)

#### 3.4 梯度下降

- 可以发现和线性回归的梯度下降公式相同
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e0bab2ca-415b-49d2-956c-d4da184affa9)
- 但是需注意，f 代表的公式不同
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/bf5d2618-94b8-43e9-bdaa-43717fa28bfa)

#### 3.5 过拟合 和 欠拟合

- 过拟合虽然看似完全符合训练集，但非常不适合应用到其他新数据，欠拟合则连训练机也无法很好的符合
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/796cba97-eed9-4d65-8204-b00628caab3d)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/5c6c901c-0ae9-4f3f-a7bc-c9fc2dd5512d)
- 解决过拟合方案
  1. 使用更多的训练数据
  2. 选择最合适的特征子集，当特征特别多，但训练数据少的情况时，通常会导致过拟合
  3. 正则化（ 最合适的方案，和第 2 点相似，只不过一个 w 设置为 0，一个 w 设置为接近 0 ）

## 4. 正则化

- 正则化指的是通过限制模型的复杂度，降低模型对输入的敏感性，避免过拟合
- 比如当特征值特别大时，不想让特征过于影响预测值，所以让参数的权重 w 接近 0
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/83440e15-2895-498d-8a3e-14877f4ca5b0)
- 更一般的场景是，比如存在 100 个特征，但是不知道哪些特征是最重要的，以及哪些特征是要限制的，这个时候通常可以正则化所有特征，这通常会让拟合更平滑，更简单，且不容易过拟合
- 通过在代价函数后面添加新的项，n 是指**需要限制**的 w1，w2 ... wn 的个数。而通常不会因为 b 过大去限制，这一般是没作用的，所以最后添加的项可有可无。λ 是用户定义的值
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/764efe35-59d5-4127-82d5-2c731998b679)
- 因此最终代价函数是由两个部分组成，前部分用于拟合数据，后部分用于限制 wj 为较小数
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/e566d2c7-0164-4b62-bf38-d8dba00d6966)
- 如果 λ 为 0，则说明正则化不重要，这项不存在，会过拟合；如果 λ 过大，则会导致 w1，w2 ... wn 都趋于 0，变成 f(x) = b，会欠拟合

#### 4.1 用于线性回归

- wj 需要多减去一项，去控制 wj 大小；bj 的计算公式不变，因为正则化对 bj 没影响
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b87733c2-527c-4376-8fe5-05c2bb1dfa46)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/716015d8-6891-4bc0-819b-55c2d26ae63b)

#### 4.2 用于逻辑回归

![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/6b897df9-a2d0-46d2-9a53-a645b76f04e4)

## 5. Softmax 回归算法

- Softmax 回归算法是逻辑回归的泛化，是多类分类问题的二进制分类算法

#### 5.1 Softmax 输出公式

- 当 N = 2 的时候，最终结果和逻辑回归的结果基本相同，因此叫逻辑回归的泛化
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/93a92d0a-2df0-4e0d-a999-c638139551ef)
- 比如当 N = 4 的时候，输出公式为
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d6eb8b72-f896-438c-a140-66fbf0e94ac1)

#### 5.2 代价函数

- 如果 y 是 j，那么损失是 aj 的副对数，如果 aj 接近 1，即结果是 j 的概率接近 1，那么损失接近 0，如果 aj 接近 0，即结果是 j 的概率接近 0，那么损失值非常大
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/5246c6cc-4827-4af3-946b-ff8a56c7e293)

## 6. 梯度下降优化 —— Adam

- Adam 算法（ adaptive moment estimation ）的核心思路是，如果 wj 持续向一个方向移动，那么就增加学习率 α，如果 wj 持续向另一个方向移动的话，就减小学习率 α
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/4afcfcdc-2310-42f1-aafb-40003502cc6b)
- Adam 算法还有一个特点，就是他对不同的 w1，w2 ... wn 和 b 都有着不同的学习率 α
- 代码改动

  ```
  mobile.compile(loss=SparseCategoricalCrossEntropy(from_logits=True))

  改成

  mobile.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=SparseCategoricalCrossEntropy(from_logits=True))
  ```
