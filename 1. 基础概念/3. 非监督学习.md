## 1. 聚类算法

- 聚类算法让大量数据点自动查找彼此相关或相似的数据点

#### 1.1 K-means 算法

- 是最常用的聚类算法
- 假设有 30 个未标记的数据，需要分成两类
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/3821c91d-4f95-4cae-a9ad-fe322f6c1e67)
  - K-means 算法首先进行随机猜测，猜两个集群的中心在哪
  - K-means 算法然后会反复进行两件事情
    1. 遍历所有点，判断离两个质心中哪个更近，将点分配给最近的集群质心
    2. 查看所有属于该质心的点，并取它们的平均值，然后将集群质心移动到平均位置
  - 如果随机猜测时，某质点并没有分配到任何数据点，有两种解决方案
    - 消除没有被分配到点的簇质点
    - 如果需要保证有 K 类，则重新初始化该集群质心
- 代价函数
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/620b9abc-ac3b-495a-8fd5-f815ce141c13)
  - 所有点到自己所属簇质点的距离的平方的平均值
  - 通过判断该代价函数结果是否停止下降，来判断 K-means 是否收敛，从而判断是否需要停止

#### 1.2 初始化簇质点

- 完全随机选点
  - 不太合理，可能与所有数据的距离都很远
- 从数据点中随机选择
  - 可能会出现局部最优解
    ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/5a97a00e-e813-4176-bb0b-1c2c99169e03)
- 通过多次从数据点中随机选择，找到代价函数 J 最小的随机选法，通常是选择 50-1000 次即可

#### 1.3 选择聚类数量

- K 值的选择，通常是模凌两可的，比如下图 K 可以是 2，也可以是 4
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/0ce1e43a-2398-4f27-b8db-0737873ebb20)
- 不能通过选取最小的 J 值，去选择对应 K，因为随着 K 的增大，J 会一直变小
- 可以参考肘法，找到曲线的弯曲点，类似于手的肘部，但是对于平滑的曲线就很难选择
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/b6dbba1b-c163-4a92-9b52-1926975ed153)
- 更推荐的方法是基于真实需求去制定，比如 K = 3 更适合衣服尺码的大中小型号的聚类，又或者说 K = 5 在之后下游代码中效果更好

## 2. 异常检测

- 异常检测算法查看未标记的数据集，从而学会检测危险信号
- 比如检测飞机发动机是否异常，x1 为发动机热量，x2 为发动机振动强度
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/9623e7f1-b194-4f1c-bb29-04c709a1413c)
- 一般通过密度评估
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/d6ee1ac4-f636-4b46-a497-06958e1c03e9)
  - 为 x 的概率建立模型，即算法将尝试找出具有高概率的特征 x1 和 x2 的值是什么，以及在数据集中出现的可能性较小的值是什么
  - 通常椭圆的内层（ 密度高的地方 ）概率较高，越外层概率越低
  - 如果数据点的概率 P 小于阀值，那么就说明它是异常状态，如果大于，则说明它是正常状态

#### 2.1 高斯分布（ 或叫正态分布 ）

- 用来描述随机变量的分布情况，是一种连续概率分布，若随机变量 x 服从一个数学期望为 μ、方差为 σ^2 的正态分布，记为 N(μ，σ^2)
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/9c2b8f35-d720-437a-b2a6-cb8da8de947e)
- 高斯分布可以作用于单一特征

#### 2.2 异常检测算法

- x 概率为将不同特征的概率进行相乘
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/6da82dea-0569-47cb-ab79-39422343dfdf)
- 整体算法步骤
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/acd6cf4f-3693-4f5e-b79d-c7f82cc572ef)

#### 2.3 评估异常检测算法

- 如何设计训练集，验证集，测试集
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/a5ed4917-5c01-40c9-a6bc-489374a2d179)
  - 假如有 10000 个正面例子，20 个反面例子，训练集可以有 6000 个正面，验证集有 2000 个正面，10 个反面，测试集有 2000 个正面，10 个反面
  - 假如反面例子非常少，只有 2 个，可以验证集有 4000 个正面，2 个反面，没有测试集
- 计算模型得分
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/ce092497-c841-4e6d-b899-ac3f749d6976)

#### 2.4 何时使用异常检测

- 当异常的样例非常少的时候，只有 0-20 个例子是非常常见的，这个时候适合异常检测。如果正常样例和异常样例都非常多的时候，更适合使用监督学习
- 如果有非常多的异常样例情况，比如可能以后飞机以一种全新的异常情况，完全和当前异常样例不同，此时更适合异常检测，因为异常检测是从正常样例中建模，所有和正常样例差异大的都是异常。如果有足够的异常样例让模型了解全部情况，则监督学习更适用

#### 2.5 如何选择特征

- 在监督学习中，可能存在一些与问题无关的额外特征对模型的影响并不会非常巨大，但是在异常检测中，可能特征的影响更大
- 最好保证异常检测的特征都是符合高斯分布的，如果不是高斯分布的特征，可以通过转换让它符合
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/c3f29b75-0300-4a4b-9fba-7841a5f684e1)
- 通过分析特征，可以增加更有影响力的特征
  ![image](https://github.com/jianyi-gronk/jianyi-gronk/assets/95062803/07a108d5-0f5d-44a6-9eae-8d31909a237a)

## 3. 生成对抗网络（ Generative Adversarial Network，GAN ）

- GAN 是最近很热门的一种无监督算法，它能生成出非常逼真的照片，图像甚至视频，常见的实际应用有生成图像数据集，生成人脸照片，漫画人物等，图像到图像的转换，文字到图像的转换，照片修复等等
- GAN 由 2 个重要的部分构成：
  - 生成器（ Generator ）：通过机器生成数据（ 大部分情况下是图像 ），目的是 “骗过” 判别器
  - 判别器（ Discriminator ）：判断这张图像是真实的还是机器生成的，目的是找出生成器做的 “假数据”

#### 3.1 算法过程：

- 第一阶段：固定 判别器，训练 生成器
  - 我们使用一个还可以的 判别器，让一个 生成器 不断生成 “假数据”，然后给这个 判别器 去判断
  - 一开始 生成器 还很弱，所以很容易被揪出来
  - 但是随着不断的训练，生成器 性能不断提升，最终骗过了 判别器
  - 到了这个时候，判别器 基本属于瞎猜的状态，判断是否为假数据的概率为 50%
- 第二阶段：固定 生成器，训练 判别器
  - 当通过了第一阶段，继续训练 生成器 就没有意义了。这个时候我们固定 生成器，然后开始训练 判别器
  - 判别器 通过不断训练，提高了自己的鉴别能力，最终他可以准确的判断出所有的假图片
  - 到了这个时候，生成器 已经无法骗过 判别器
- 循环阶段一和阶段二
  - 通过不断的循环，生成器 和 判别器 的能力都越来越强
  - 最终得到了一个效果非常好的 生成器，就可以用它来生成想要的图片

#### 3.2 GAN 的优缺点

- 优点
  - 能更好建模数据分布（ 图像更锐利、清晰 ）
  - 理论上，GAN 能训练任何一种生成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是高斯的
  - 无需利用马尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘手的概率的难题
- 缺陷
  - 难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易 D 收敛，G 发散。D/G 的训练需要精心的设计
  - 模式缺失（ Mode Collapse ）问题。即 GAN 的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习
